Using gpu device 3: Tesla M2090
/home/rizar/dist/fully-neural-lvsr/libs/Theano/theano/gof/vm.py:833: UserWarning: CVM does not support memory profile, using Stack VM.
  'CVM does not support memory profile, using Stack VM.')
2015-06-29 11:40:46,525: bokeh: DEBUG: You are not inside an IPython/Jupyter instance.
2015-06-29 11:40:46,762: root: INFO: Config:
{'cmd_args': {'beam_size': 10,
              'config_changes': [],
              'config_path': '/home/rizar/dist/fully-neural-lvsr/lvsr/configs/wsj_jan_baseline.yaml',
              'fast_start': True,
              'mode': 'train',
              'num_batches': 10,
              'old_labels': False,
              'params': None,
              'part': 'valid',
              'report': None,
              'save_path': 'tmp.zip'},
 'data': {'add_eos': True,
          'batch_size': 10,
          'dataset': 'WSJnew',
          'labels_source': 'characters',
          'max_length': None,
          'normalization': None,
          'preprocess_features': False,
          'preprocess_text': False,
          'recordings_source': 'fbank_dd',
          'sort_k_batches': None},
 'initialization': [['/recognizer', 'weights_init', 'IsotropicGaussian(0.1)'],
                    ['/recognizer', 'biases_init', 'Constant(0.0)'],
                    ['/recognizer', 'rec_weights_init', 'Orthogonal()'],
                    ['/recognizer', 'rec_weights_init', 'IsotropicGaussian(0.1)']],
 'net': {'attention_type': 'content_and_conv',
         'bottom_activation': 'relu',
         'conv_n': 100,
         'dec_transition': 'GatedRecurrent',
         'dim_dec': 250,
         'dims_bidir': [250, 250, 250],
         'dims_bottom': [250],
         'enc_transition': 'GatedRecurrent',
         'post_merge_dims': [250],
         'prior': {'initial_begin': 0, 'initial_end': 80, 'max_speed': 4.4, 'min_speed': 2.4},
         'subsample': [1, 1, 2],
         'use_states_for_readout': True},
 'parent': '$LVSR/lvsr/configs/wsj_good_fbank.yaml',
 'regularization': {'dropout': False, 'noise': None},
 'training': {'decay_rate': 0.95,
              'epsilon': '1e-8',
              'gradient_threshold': 100.0,
              'momentum': 0.0,
              'rules': ['momentum', 'adadelta'],
              'scale': 0.1}}
2015-06-29 11:40:47,090: lvsr.main: INFO: Initialization schemes for all bricks.
Works well only in my branch with __repr__ added to all them,
there is an issue #463 in Blocks to do that properly.
2015-06-29 11:40:47,101: lvsr.main: INFO: {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
 'bottom': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
            'linear_0': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                         'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
            'rectifier': {},
            'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
 'encoder': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
             'bidir0': {'backward': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                     'fork': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                              'fork_gate_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                                   'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                              'fork_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                              'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                              'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                     'gatedrecurrent': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                        'logistic': {},
                                                        'tanh': {},
                                                        'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                     'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                        'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                        'forward': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                    'fork': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                             'fork_gate_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                                  'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                             'fork_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                             'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                             'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                    'gatedrecurrent': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                       'logistic': {},
                                                       'tanh': {},
                                                       'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                    'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                        'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
             'bidir1': {'backward': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                     'fork': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                              'fork_gate_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                                   'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                              'fork_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                              'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                              'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                     'gatedrecurrent': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                        'logistic': {},
                                                        'tanh': {},
                                                        'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                     'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                        'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                        'forward': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                    'fork': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                             'fork_gate_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                                  'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                             'fork_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                             'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                             'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                    'gatedrecurrent': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                       'logistic': {},
                                                       'tanh': {},
                                                       'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                    'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                        'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
             'bidir2': {'backward': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                     'fork': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                              'fork_gate_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                                   'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                              'fork_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                              'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                              'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                     'gatedrecurrent': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                        'logistic': {},
                                                        'tanh': {},
                                                        'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                     'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                        'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                        'forward': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                    'fork': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                             'fork_gate_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                                  'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                             'fork_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                             'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                             'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                    'gatedrecurrent': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                       'logistic': {},
                                                       'tanh': {},
                                                       'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                                    'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                        'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
             'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
 'generator': {'att_trans': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                             'conv_att': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                          'conv1d': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                     'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                          'energy_comp': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                          'linear': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                                     'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                                          'tanh': {},
                                                          'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                          'handler': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                      'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                          'preprocess': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                         'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                          'state_trans': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                          'transform_states': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                                               'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                                          'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                          'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                             'distribute': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                            'fork_gate_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                                 'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                            'fork_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                            'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                            'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                             'transition': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                            'logistic': {},
                                            'tanh': {},
                                            'weights_init': <blocks.initialization.IsotropicGaussian object at 0xc861990>},
                             'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
               'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
               'fork': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                        'fork_gate_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                             'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                        'fork_inputs': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                        'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                        'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
               'readout': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                           'emitter': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                       'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                           'lookupfeedback': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                              'lookuptable': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                              'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                              'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                           'merge': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                     'transform_states': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                          'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                     'transform_weighted_averages': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                                     'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                     'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                           'post_merge': {'bias': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                   'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                          'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                          'mlp': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                  'identity': {},
                                                  'linear_0': {'biases_init': <blocks.initialization.Constant object at 0xe565f10>,
                                                               'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                                  'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                                          'rectifier': {},
                                          'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
                           'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
               'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>},
 'top': {},
 'weights_init': <blocks.initialization.IsotropicGaussian object at 0xe565f90>}
2015-06-29 11:40:47,900: lvsr.main: INFO: Cost graph is built
2015-06-29 11:40:48,009: lvsr.main: INFO: Parameters:
[('/recognizer/generator/readout/post_merge/mlp/linear_0.b', (32,)),
 ('/recognizer/generator/readout/post_merge/mlp/linear_0.W', (250, 32)),
 ('/recognizer/generator/readout/post_merge/bias.b', (250,)),
 ('/recognizer/generator/readout/merge/transform_weighted_averages.W', (500, 250)),
 ('/recognizer/generator/att_trans/conv_att/preprocess.b', (250,)),
 ('/recognizer/generator/att_trans/conv_att/preprocess.W', (500, 250)),
 ('/recognizer/encoder/bidir2/backward/gatedrecurrent.state_to_state', (250, 250)),
 ('/recognizer/encoder/bidir2/backward/gatedrecurrent.state_to_gates', (250, 500)),
 ('/recognizer/encoder/bidir2/backward/gatedrecurrent.initial_state', (250,)),
 ('/recognizer/encoder/bidir2/backward/fork/fork_gate_inputs.b', (500,)),
 ('/recognizer/encoder/bidir2/backward/fork/fork_gate_inputs.W', (500, 500)),
 ('/recognizer/encoder/bidir1/backward/gatedrecurrent.state_to_state', (250, 250)),
 ('/recognizer/encoder/bidir1/backward/gatedrecurrent.state_to_gates', (250, 500)),
 ('/recognizer/encoder/bidir1/backward/gatedrecurrent.initial_state', (250,)),
 ('/recognizer/encoder/bidir1/backward/fork/fork_gate_inputs.b', (500,)),
 ('/recognizer/encoder/bidir1/backward/fork/fork_gate_inputs.W', (500, 500)),
 ('/recognizer/encoder/bidir0/backward/gatedrecurrent.state_to_state', (250, 250)),
 ('/recognizer/encoder/bidir0/backward/gatedrecurrent.state_to_gates', (250, 500)),
 ('/recognizer/encoder/bidir0/backward/gatedrecurrent.initial_state', (250,)),
 ('/recognizer/encoder/bidir0/backward/fork/fork_gate_inputs.b', (500,)),
 ('/recognizer/encoder/bidir0/backward/fork/fork_gate_inputs.W', (250, 500)),
 ('/recognizer/bottom/linear_0.b', (250,)),
 ('/recognizer/bottom/linear_0.W', (123, 250)),
 ('/recognizer/encoder/bidir0/backward/fork/fork_inputs.b', (250,)),
 ('/recognizer/encoder/bidir0/backward/fork/fork_inputs.W', (250, 250)),
 ('/recognizer/encoder/bidir0/forward/gatedrecurrent.state_to_state', (250, 250)),
 ('/recognizer/encoder/bidir0/forward/gatedrecurrent.state_to_gates', (250, 500)),
 ('/recognizer/encoder/bidir0/forward/gatedrecurrent.initial_state', (250,)),
 ('/recognizer/encoder/bidir0/forward/fork/fork_gate_inputs.b', (500,)),
 ('/recognizer/encoder/bidir0/forward/fork/fork_gate_inputs.W', (250, 500)),
 ('/recognizer/encoder/bidir0/forward/fork/fork_inputs.b', (250,)),
 ('/recognizer/encoder/bidir0/forward/fork/fork_inputs.W', (250, 250)),
 ('/recognizer/encoder/bidir1/backward/fork/fork_inputs.b', (250,)),
 ('/recognizer/encoder/bidir1/backward/fork/fork_inputs.W', (500, 250)),
 ('/recognizer/encoder/bidir1/forward/gatedrecurrent.state_to_state', (250, 250)),
 ('/recognizer/encoder/bidir1/forward/gatedrecurrent.state_to_gates', (250, 500)),
 ('/recognizer/encoder/bidir1/forward/gatedrecurrent.initial_state', (250,)),
 ('/recognizer/encoder/bidir1/forward/fork/fork_gate_inputs.b', (500,)),
 ('/recognizer/encoder/bidir1/forward/fork/fork_gate_inputs.W', (500, 500)),
 ('/recognizer/encoder/bidir1/forward/fork/fork_inputs.b', (250,)),
 ('/recognizer/encoder/bidir1/forward/fork/fork_inputs.W', (500, 250)),
 ('/recognizer/encoder/bidir2/backward/fork/fork_inputs.b', (250,)),
 ('/recognizer/encoder/bidir2/backward/fork/fork_inputs.W', (500, 250)),
 ('/recognizer/encoder/bidir2/forward/gatedrecurrent.state_to_state', (250, 250)),
 ('/recognizer/encoder/bidir2/forward/gatedrecurrent.state_to_gates', (250, 500)),
 ('/recognizer/encoder/bidir2/forward/gatedrecurrent.initial_state', (250,)),
 ('/recognizer/encoder/bidir2/forward/fork/fork_gate_inputs.b', (500,)),
 ('/recognizer/encoder/bidir2/forward/fork/fork_gate_inputs.W', (500, 500)),
 ('/recognizer/encoder/bidir2/forward/fork/fork_inputs.b', (250,)),
 ('/recognizer/encoder/bidir2/forward/fork/fork_inputs.W', (500, 250)),
 ('/recognizer/generator/att_trans/distribute/fork_inputs.W', (500, 250)),
 ('/recognizer/generator/att_trans/transition.state_to_state', (250, 250)),
 ('/recognizer/generator/att_trans/distribute/fork_gate_inputs.W', (500, 500)),
 ('/recognizer/generator/att_trans/conv_att/energy_comp/linear.W', (250, 1)),
 ('/recognizer/generator/att_trans/conv_att/handler.W', (1, 250)),
 ('/recognizer/generator/att_trans/conv_att/conv1d.filters', (1, 201)),
 ('/recognizer/generator/att_trans/conv_att/state_trans/transform_states.W', (250, 250)),
 ('/recognizer/generator/att_trans/transition.state_to_gates', (250, 500)),
 ('/recognizer/generator/att_trans/transition.initial_state', (250,)),
 ('/recognizer/generator/fork/fork_gate_inputs.b', (500,)),
 ('/recognizer/generator/fork/fork_gate_inputs.W', (250, 500)),
 ('/recognizer/generator/readout/lookupfeedback/lookuptable.W', (33, 250)),
 ('/recognizer/generator/fork/fork_inputs.b', (250,)),
 ('/recognizer/generator/fork/fork_inputs.W', (250, 250)),
 ('/recognizer/generator/readout/merge/transform_states.W', (250, 250))]
2015-06-29 11:40:48,010: lvsr.main: INFO: Using scaling and momentum for training
2015-06-29 11:40:48,010: lvsr.main: INFO: Using AdaDelta for training
2015-06-29 11:40:48,042: blocks.algorithms: INFO: Taking the cost gradient
2015-06-29 11:40:52,728: blocks.algorithms: INFO: The cost gradient computation graph is built
2015-06-29 11:40:53,683: lvsr.main: INFO: Initialize extensions
2015-06-29 11:40:53,838: root: DEBUG: variable to evaluate: sequence_log_likelihood
2015-06-29 11:40:53,841: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for sequence_log_likelihood
2015-06-29 11:40:53,844: root: DEBUG: variable to evaluate: total_gradient_norm
2015-06-29 11:40:53,847: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for total_gradient_norm
2015-06-29 11:40:53,850: root: DEBUG: variable to evaluate: total_step_norm
2015-06-29 11:40:53,854: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for total_step_norm
2015-06-29 11:40:53,857: root: DEBUG: variable to evaluate: gradient_norm_threshold
2015-06-29 11:40:53,857: root: DEBUG: Using TakeLast aggregation scheme for gradient_norm_threshold since it does not depend on the data
2015-06-29 11:40:53,857: root: DEBUG: variable to evaluate: max_recording_length
2015-06-29 11:40:53,857: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for max_recording_length
2015-06-29 11:40:53,860: root: DEBUG: variable to evaluate: max_attended_length
2015-06-29 11:40:53,860: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for max_attended_length
2015-06-29 11:40:53,862: root: DEBUG: variable to evaluate: max_attended_mask_length
2015-06-29 11:40:53,862: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for max_attended_mask_length
2015-06-29 11:40:53,865: root: DEBUG: Compiling initialization and readout functions
2015-06-29 11:40:54,028: root: DEBUG: Initialization and readout functions compiled
2015-06-29 11:40:54,273: root: DEBUG: variable to evaluate: sequence_log_likelihood
2015-06-29 11:40:54,276: root: DEBUG: variable to evaluate: weights_penalty_per_recording
2015-06-29 11:40:54,279: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for weights_penalty_per_recording
2015-06-29 11:40:54,282: root: DEBUG: variable to evaluate: weights_entropy_per_label
2015-06-29 11:40:54,282: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for weights_entropy_per_label
2015-06-29 11:40:54,285: root: DEBUG: variable to evaluate: min_energy
2015-06-29 11:40:54,285: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for min_energy
2015-06-29 11:40:54,288: root: DEBUG: variable to evaluate: max_energy
2015-06-29 11:40:54,288: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for max_energy
2015-06-29 11:40:54,291: root: DEBUG: variable to evaluate: mean_attended
2015-06-29 11:40:54,291: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for mean_attended
2015-06-29 11:40:54,294: root: DEBUG: variable to evaluate: mean_bottom_output
2015-06-29 11:40:54,294: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for mean_bottom_output
2015-06-29 11:40:54,297: root: DEBUG: variable to evaluate: batch_size
2015-06-29 11:40:54,297: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for batch_size
2015-06-29 11:40:54,300: root: DEBUG: variable to evaluate: max_num_phonemes
2015-06-29 11:40:54,300: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for max_num_phonemes
2015-06-29 11:40:54,302: root: DEBUG: variable to evaluate: mask_density
2015-06-29 11:40:54,303: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for mask_density
2015-06-29 11:40:54,305: root: DEBUG: variable to evaluate: total_step_norm
2015-06-29 11:40:54,309: root: DEBUG: variable to evaluate: total_gradient_norm
2015-06-29 11:40:54,312: root: DEBUG: variable to evaluate: gradient_norm_threshold
2015-06-29 11:40:54,312: root: DEBUG: variable to evaluate: /recognizer/generator/readout/post_merge/mlp/linear_0.b_stats
2015-06-29 11:40:54,317: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/readout/post_merge/mlp/linear_0.b_stats
2015-06-29 11:40:54,320: root: DEBUG: variable to evaluate: /recognizer/generator/readout/post_merge/mlp/linear_0.W_stats
2015-06-29 11:40:54,320: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/readout/post_merge/mlp/linear_0.W_stats
2015-06-29 11:40:54,323: root: DEBUG: variable to evaluate: /recognizer/generator/readout/post_merge/bias.b_stats
2015-06-29 11:40:54,323: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/readout/post_merge/bias.b_stats
2015-06-29 11:40:54,326: root: DEBUG: variable to evaluate: /recognizer/generator/readout/merge/transform_weighted_averages.W_stats
2015-06-29 11:40:54,326: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/readout/merge/transform_weighted_averages.W_stats
2015-06-29 11:40:54,329: root: DEBUG: variable to evaluate: /recognizer/generator/att_trans/conv_att/preprocess.b_stats
2015-06-29 11:40:54,329: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/att_trans/conv_att/preprocess.b_stats
2015-06-29 11:40:54,332: root: DEBUG: variable to evaluate: /recognizer/generator/att_trans/conv_att/preprocess.W_stats
2015-06-29 11:40:54,332: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/att_trans/conv_att/preprocess.W_stats
2015-06-29 11:40:54,335: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/backward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,335: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/backward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,337: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/backward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,338: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/backward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,340: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/backward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,341: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/backward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,344: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/backward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,344: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/backward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,346: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/backward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,347: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/backward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,349: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/backward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,350: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/backward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,352: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/backward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,352: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/backward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,355: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/backward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,355: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/backward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,358: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/backward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,358: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/backward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,361: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/backward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,361: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/backward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,364: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/backward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,364: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/backward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,367: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/backward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,367: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/backward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,370: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/backward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,370: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/backward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,373: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/backward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,373: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/backward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,377: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/backward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,378: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/backward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,380: root: DEBUG: variable to evaluate: /recognizer/bottom/linear_0.b_stats
2015-06-29 11:40:54,380: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/bottom/linear_0.b_stats
2015-06-29 11:40:54,383: root: DEBUG: variable to evaluate: /recognizer/bottom/linear_0.W_stats
2015-06-29 11:40:54,383: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/bottom/linear_0.W_stats
2015-06-29 11:40:54,386: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/backward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,387: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/backward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,389: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/backward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,390: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/backward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,392: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/forward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,393: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/forward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,395: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/forward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,396: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/forward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,398: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/forward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,399: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/forward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,401: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/forward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,401: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/forward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,404: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/forward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,404: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/forward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,407: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/forward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,407: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/forward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,410: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir0/forward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,410: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir0/forward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,413: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/backward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,413: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/backward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,416: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/backward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,416: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/backward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,419: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/forward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,419: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/forward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,422: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/forward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,422: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/forward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,425: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/forward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,425: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/forward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,428: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/forward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,428: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/forward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,431: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/forward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,431: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/forward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,434: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/forward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,434: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/forward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,437: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir1/forward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,437: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir1/forward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,439: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/backward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,440: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/backward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,444: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/backward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,444: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/backward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,447: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/forward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,447: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/forward/gatedrecurrent.state_to_state_stats
2015-06-29 11:40:54,450: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/forward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,450: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/forward/gatedrecurrent.state_to_gates_stats
2015-06-29 11:40:54,452: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/forward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,453: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/forward/gatedrecurrent.initial_state_stats
2015-06-29 11:40:54,455: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/forward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,456: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/forward/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,458: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/forward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,459: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/forward/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,461: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/forward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,461: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/forward/fork/fork_inputs.b_stats
2015-06-29 11:40:54,464: root: DEBUG: variable to evaluate: /recognizer/encoder/bidir2/forward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,464: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/encoder/bidir2/forward/fork/fork_inputs.W_stats
2015-06-29 11:40:54,467: root: DEBUG: variable to evaluate: /recognizer/generator/att_trans/distribute/fork_inputs.W_stats
2015-06-29 11:40:54,467: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/att_trans/distribute/fork_inputs.W_stats
2015-06-29 11:40:54,470: root: DEBUG: variable to evaluate: /recognizer/generator/att_trans/transition.state_to_state_stats
2015-06-29 11:40:54,470: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/att_trans/transition.state_to_state_stats
2015-06-29 11:40:54,473: root: DEBUG: variable to evaluate: /recognizer/generator/att_trans/distribute/fork_gate_inputs.W_stats
2015-06-29 11:40:54,473: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/att_trans/distribute/fork_gate_inputs.W_stats
2015-06-29 11:40:54,476: root: DEBUG: variable to evaluate: /recognizer/generator/att_trans/conv_att/energy_comp/linear.W_stats
2015-06-29 11:40:54,476: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/att_trans/conv_att/energy_comp/linear.W_stats
2015-06-29 11:40:54,479: root: DEBUG: variable to evaluate: /recognizer/generator/att_trans/conv_att/handler.W_stats
2015-06-29 11:40:54,479: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/att_trans/conv_att/handler.W_stats
2015-06-29 11:40:54,482: root: DEBUG: variable to evaluate: /recognizer/generator/att_trans/conv_att/conv1d.filters_stats
2015-06-29 11:40:54,482: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/att_trans/conv_att/conv1d.filters_stats
2015-06-29 11:40:54,485: root: DEBUG: variable to evaluate: /recognizer/generator/att_trans/conv_att/state_trans/transform_states.W_stats
2015-06-29 11:40:54,485: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/att_trans/conv_att/state_trans/transform_states.W_stats
2015-06-29 11:40:54,488: root: DEBUG: variable to evaluate: /recognizer/generator/att_trans/transition.state_to_gates_stats
2015-06-29 11:40:54,488: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/att_trans/transition.state_to_gates_stats
2015-06-29 11:40:54,491: root: DEBUG: variable to evaluate: /recognizer/generator/att_trans/transition.initial_state_stats
2015-06-29 11:40:54,491: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/att_trans/transition.initial_state_stats
2015-06-29 11:40:54,494: root: DEBUG: variable to evaluate: /recognizer/generator/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,494: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/fork/fork_gate_inputs.b_stats
2015-06-29 11:40:54,497: root: DEBUG: variable to evaluate: /recognizer/generator/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,497: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/fork/fork_gate_inputs.W_stats
2015-06-29 11:40:54,500: root: DEBUG: variable to evaluate: /recognizer/generator/readout/lookupfeedback/lookuptable.W_stats
2015-06-29 11:40:54,500: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/readout/lookupfeedback/lookuptable.W_stats
2015-06-29 11:40:54,503: root: DEBUG: variable to evaluate: /recognizer/generator/fork/fork_inputs.b_stats
2015-06-29 11:40:54,503: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/fork/fork_inputs.b_stats
2015-06-29 11:40:54,507: root: DEBUG: variable to evaluate: /recognizer/generator/fork/fork_inputs.W_stats
2015-06-29 11:40:54,507: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/fork/fork_inputs.W_stats
2015-06-29 11:40:54,510: root: DEBUG: variable to evaluate: /recognizer/generator/readout/merge/transform_states.W_stats
2015-06-29 11:40:54,510: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for /recognizer/generator/readout/merge/transform_states.W_stats
2015-06-29 11:40:54,513: root: DEBUG: Compiling initialization and readout functions
2015-06-29 11:40:57,605: root: DEBUG: Initialization and readout functions compiled
2015-06-29 11:40:57,650: root: DEBUG: variable to evaluate: sequence_log_likelihood
2015-06-29 11:40:57,653: root: DEBUG: variable to evaluate: weights_entropy_per_label
2015-06-29 11:40:57,655: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for weights_entropy_per_label
2015-06-29 11:40:57,658: root: DEBUG: variable to evaluate: weights_penalty_per_recording
2015-06-29 11:40:57,658: root: DEBUG: Using the default  (average over minibatches) aggregation scheme for weights_penalty_per_recording
2015-06-29 11:40:57,661: root: DEBUG: Compiling initialization and readout functions
2015-06-29 11:40:57,734: root: DEBUG: Initialization and readout functions compiled
/home/rizar/dist/fully-neural-lvsr/libs/Theano/theano/scan_module/scan_perform_ext.py:134: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
2015-06-29 11:41:28,455: root: DEBUG: Compiling initialization and readout functions
2015-06-29 11:41:28,474: root: DEBUG: Initialization and readout functions compiled
2015-06-29 11:41:28,502: requests.packages.urllib3.connectionpool: INFO: Starting new HTTP connection (1): localhost
2015-06-29 11:41:28,507: requests.packages.urllib3.connectionpool: DEBUG: "GET /bokeh/userinfo/ HTTP/1.1" 200 15288
2015-06-29 11:41:28,541: requests.packages.urllib3.connectionpool: DEBUG: "GET /bokeh/getdocapikey/e756af41-d85e-4288-a59d-435876bd0ec9 HTTP/1.1" 200 54
2015-06-29 11:41:28,542: bokeh.session: INFO: got read write apikey
2015-06-29 11:41:34,155: requests.packages.urllib3.connectionpool: DEBUG: "POST /bokeh/bb/e756af41-d85e-4288-a59d-435876bd0ec9/gc HTTP/1.1" 200 25
2015-06-29 11:41:34,412: requests.packages.urllib3.connectionpool: DEBUG: "GET /bokeh/bb/e756af41-d85e-4288-a59d-435876bd0ec9/ HTTP/1.1" 200 42159
2015-06-29 11:41:34,480: blocks.main_loop: WARNING: different costs for model and algorithm
2015-06-29 11:41:34,481: blocks.main_loop: INFO: Entered the main loop
2015-06-29 11:41:34,531: blocks.algorithms: INFO: Initializing the training algorithm
2015-06-29 11:48:06,076: blocks.algorithms: INFO: The training algorithm is initialized
2015-06-29 11:48:06,586: requests.packages.urllib3.connectionpool: DEBUG: "POST /bokeh/bb/e756af41-d85e-4288-a59d-435876bd0ec9/bulkupsert HTTP/1.1" 200 32686
Epoch 0, step 0 |#                                      | Elapsed Time: 0:00:00Epoch 0, step 1 |#                                      | Elapsed Time: 0:00:00Epoch 0, step 2 | #                                     | Elapsed Time: 0:00:22Epoch 0, step 3 |  #                                    | Elapsed Time: 0:00:45Epoch 0, step 4 |   #                                   | Elapsed Time: 0:01:06Epoch 0, step 5 |    #                                  | Elapsed Time: 0:01:31Using saved session configuration for http://localhost:5006/
To override, pass 'load_from_config=False' to Session
Computation graph statistics:
	number of scan nodes: 8

-------------------------------------------------------------------------------
BEFORE FIRST EPOCH
-------------------------------------------------------------------------------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 0
	 received_first_batch: False
	 resumed_from: None
	 training_started: True
Log records from the iteration 0:
	 time_initialization: 391.54585886


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 1
	 received_first_batch: True
	 resumed_from: None
	 training_started: True
Log records from the iteration 1:
	 gradient_norm_threshold: 100.0
	 length_filter_switched: True
	 max_attended_length: 544.0
	 max_attended_mask_length: 544.0
	 max_recording_length: 1088.0
	 sequence_log_likelihood: 458.524902344
	 time_read_data_this_batch: 0.0225479602814
	 time_read_data_total: 0.0225479602814
	 time_train_this_batch: 22.4027807713
	 time_train_total: 22.4027807713
	 total_gradient_norm: 1865.03137207
	 total_step_norm: 0.65468609333


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 2
	 received_first_batch: True
	 resumed_from: None
	 training_started: True
Log records from the iteration 2:
	 gradient_norm_threshold: 99.8028411865
	 max_attended_length: 570.0
	 max_attended_mask_length: 570.0
	 max_recording_length: 1139.0
	 sequence_log_likelihood: 327.218688965
	 time_read_data_this_batch: 0.0190370082855
	 time_read_data_total: 0.0415849685669
	 time_train_this_batch: 22.6119511127
	 time_train_total: 45.014731884
	 total_gradient_norm: 1262.01306152
	 total_step_norm: 0.662943184376


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 3
	 received_first_batch: True
	 resumed_from: None
	 training_started: True
Log records from the iteration 3:
	 gradient_norm_threshold: 99.6065444946
	 max_attended_length: 528.0
	 max_attended_mask_length: 528.0
	 max_recording_length: 1055.0
	 sequence_log_likelihood: 465.02645874
	 time_read_data_this_batch: 0.0207087993622
	 time_read_data_total: 0.0622937679291
	 time_train_this_batch: 21.3865990639
	 time_train_total: 66.4013309479
	 total_gradient_norm: 1509.73217773
	 total_step_norm: 0.639310181141


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 4
	 received_first_batch: True
	 resumed_from: None
	 training_started: True
Log records from the iteration 4:
	 gradient_norm_threshold: 99.4110412598
	 max_attended_length: 710.0
	 max_attended_mask_length: 710.0
	 max_recording_length: 1419.0
	 sequence_log_likelihood: 547.888671875
	 time_read_data_this_batch: 0.0339140892029
	 time_read_data_total: 0.096207857132
	 time_train_this_batch: 25.4802691936
	 time_train_total: 91.8816001415
	 total_gradient_norm: 1557.3190918
	 total_step_norm: 0.676996052265


-------------------------------------------------------------------------------
-----------------------------------------------------------------------Epoch 0, step 6 |     #                                 | Elapsed Time: 0:01:55Epoch 0, step 7 |      #                                | Elapsed Time: 0:02:17Epoch 0, step 8 |       #                               | Elapsed Time: 0:02:47Epoch 0, step 9 |        #                              | Elapsed Time: 0:03:12Epoch 0, step 10 |         #                            | Elapsed Time: 0:03:392015-06-29 11:52:17,884: requests.packages.urllib3.connectionpool: DEBUG: "POST /bokeh/bb/e756af41-d85e-4288-a59d-435876bd0ec9/bulkupsert HTTP/1.1" 200 18095
--------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 5
	 received_first_batch: True
	 resumed_from: None
	 training_started: True
Log records from the iteration 5:
	 gradient_norm_threshold: 99.216293335
	 max_attended_length: 581.0
	 max_attended_mask_length: 581.0
	 max_recording_length: 1161.0
	 sequence_log_likelihood: 454.866607666
	 time_read_data_this_batch: 0.0224139690399
	 time_read_data_total: 0.118621826172
	 time_train_this_batch: 23.8805041313
	 time_train_total: 115.762104273
	 total_gradient_norm: 1767.60375977
	 total_step_norm: 0.668272852898


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 6
	 received_first_batch: True
	 resumed_from: None
	 training_started: True
Log records from the iteration 6:
	 gradient_norm_threshold: 99.0223617554
	 max_attended_length: 525.0
	 max_attended_mask_length: 525.0
	 max_recording_length: 1050.0
	 sequence_log_likelihood: 344.341461182
	 time_read_data_this_batch: 0.020336151123
	 time_read_data_total: 0.138957977295
	 time_train_this_batch: 21.4913640022
	 time_train_total: 137.253468275
	 total_gradient_norm: 668.450073242
	 total_step_norm: 0.693110823631


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 7
	 received_first_batch: True
	 resumed_from: None
	 training_started: True
Log records from the iteration 7:
	 gradient_norm_threshold: 98.8286819458
	 max_attended_length: 724.0
	 max_attended_mask_length: 724.0
	 max_recording_length: 1448.0
	 sequence_log_likelihood: 393.481628418
	 time_read_data_this_batch: 0.0234129428864
	 time_read_data_total: 0.162370920181
	 time_train_this_batch: 29.6192369461
	 time_train_total: 166.872705221
	 total_gradient_norm: 797.986633301
	 total_step_norm: 0.660746097565


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 8
	 received_first_batch: True
	 resumed_from: None
	 training_started: True
Log records from the iteration 8:
	 gradient_norm_threshold: 98.6357421875
	 max_attended_length: 613.0
	 max_attended_mask_length: 613.0
	 max_recording_length: 1225.0
	 sequence_log_likelihood: 439.755615234
	 time_read_data_this_batch: 0.0202929973602
	 time_read_data_total: 0.182663917542
	 time_train_this_batch: 25.7773041725
	 time_train_total: 192.650009394
	 total_gradient_norm: 813.594726562
	 total_step_norm: 0.615109443665


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 9
	 received_first_batch: True
	 resumed_from: None
	 training_started: True
Log records from the iteration 9:
	 gradient_norm_threshold: 98.4434967041
	 max_attended_length: 701.0
	 max_attended_mask_length: 701.0
	 max_recording_length: 1402.0
	 sequence_log_likelihood: 348.970031738
	 time_read_data_this_batch: 0.0229480266571
	 time_read_data_total: 0.205611944199
	 time_train_this_batch: 26.7772061825
	 time_train_total: 219.427215576
	 total_gradient_norm: 591.850830078
	 total_step_norm: 0.633984744549


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 10
	 received_first_batch: True
	 resumed_from: None
	 training_started: True
Log records from the iteration 10:
	 average_/recognizer/bottom/linear_0.W_stats: [  9.87963056e-02   1.58714834e+00   4.04041959e-04   3.32732609e-04]
	 average_/recognizer/bottom/linear_0.b_stats: [  7.16617513e-04   3.29886797e+00   4.23609486e-04   1.58945287e-04]
	 average_/recognizer/encoder/bidir0/backward/fork/fork_gate_inputs.W_stats: [ 0.10015734  0.17004602  0.00030847  0.00220164]
	 average_/recognizer/encoder/bidir0/backward/fork/fork_gate_inputs.b_stats: [ 0.00063334  0.26624792  0.00035788  0.00166376]
	 average_/recognizer/encoder/bidir0/backward/fork/fork_inputs.W_stats: [  1.00201958e-01   1.26345230e+00   4.07590717e-04   3.97723752e-04]
	 average_/recognizer/encoder/bidir0/backward/fork/fork_inputs.b_stats: [  6.99518385e-04   2.66040656e+00   4.26509385e-04   1.95658378e-04]
	 average_/recognizer/encoder/bidir0/backward/gatedrecurrent.initial_state_stats: [ 0.00045526  0.05453042  0.0001767   0.00551876]
	 average_/recognizer/encoder/bidir0/backward/gatedrecurrent.state_to_gates_stats: [ 0.10010799  0.13937994  0.00030217  0.00271753]
	 average_/recognizer/encoder/bidir0/backward/gatedrecurrent.state_to_state_stats: [  9.98947273e-02   6.10140164e-01   3.91208190e-04   7.98241905e-04]
	 average_/recognizer/encoder/bidir0/forward/fork/fork_gate_inputs.W_stats: [ 0.09957625  0.18927767  0.0003117   0.0019548 ]
	 average_/recognizer/encoder/bidir0/forward/fork/fork_gate_inputs.b_stats: [ 0.0006052   0.27754369  0.00035931  0.00159199]
	 average_/recognizer/encoder/bidir0/forward/fork/fork_inputs.W_stats: [  9.96452446e-02   1.30010881e+00   4.10708508e-04   3.79780788e-04]
	 average_/recognizer/encoder/bidir0/forward/fork/fork_inputs.b_stats: [  6.75619548e-04   2.74292577e+00   4.27124275e-04   1.86804402e-04]
	 average_/recognizer/encoder/bidir0/forward/gatedrecurrent.initial_state_stats: [  1.60508458e-04   1.04048581e-02   7.20706318e-05   8.22683098e-03]
	 average_/recognizer/encoder/bidir0/forward/gatedrecurrent.state_to_gates_stats: [ 0.09992362  0.14696877  0.00030473  0.00257063]
	 average_/recognizer/encoder/bidir0/forward/gatedrecurrent.state_to_state_stats: [  1.00222920e-01   6.15754762e-01   3.91739285e-04   7.83385089e-04]
	 average_/recognizer/encoder/bidir1/backward/fork/fork_gate_inputs.W_stats: [ 0.10014759  0.10413263  0.00027319  0.00316884]
	 average_/recognizer/encoder/bidir1/backward/fork/fork_gate_inputs.b_stats: [ 0.0005749   0.19878957  0.0003348   0.00204631]
	 average_/recognizer/encoder/bidir1/backward/fork/fork_inputs.W_stats: [  1.00199812e-01   8.06206705e-01   3.95496231e-04   5.87115998e-04]
	 average_/recognizer/encoder/bidir1/backward/fork/fork_inputs.b_stats: [  6.28432098e-04   1.85805402e+00   4.14234786e-04   2.64075847e-04]
	 average_/recognizer/encoder/bidir1/backward/gatedrecurrent.initial_state_stats: [ 0.00040408  0.07604756  0.00017217  0.00370306]
	 average_/recognizer/encoder/bidir1/backward/gatedrecurrent.state_to_gates_stats: [ 0.10030181  0.11438521  0.00028197  0.00300878]
	 average_/recognizer/encoder/bidir1/backward/gatedrecurrent.state_to_state_stats: [  9.97602135e-02   5.21025026e-01   3.78008559e-04   8.74028268e-04]
	 average_/recognizer/encoder/bidir1/forward/fork/fork_gate_inputs.W_stats: [ 0.10019892  0.10780604  0.00027663  0.00317284]
	 average_/recognizer/encoder/bidir1/forward/fork/fork_gate_inputs.b_stats: [ 0.0006      0.21189247  0.00033539  0.00195516]
	 average_/recognizer/encoder/bidir1/forward/fork/fork_inputs.W_stats: [  9.95732521e-02   8.79620625e-01   3.95888227e-04   5.49430316e-04]
	 average_/recognizer/encoder/bidir1/forward/fork/fork_inputs.b_stats: [  6.57443330e-04   2.02369350e+00   4.15619570e-04   2.47904852e-04]
	 average_/recognizer/encoder/bidir1/forward/gatedrecurrent.initial_state_stats: [  1.85947465e-04   1.88010207e-02   8.26841668e-05   5.96084483e-03]
	 average_/recognizer/encoder/bidir1/forward/gatedrecurrent.state_to_gates_stats: [ 0.09968547  0.11974477  0.00028747  0.0029856 ]
	 average_/recognizer/encoder/bidir1/forward/gatedrecurrent.state_to_state_stats: [  9.99971558e-02   5.50266733e-01   3.84207222e-04   8.56264264e-04]
	 average_/recognizer/encoder/bidir2/backward/fork/fork_gate_inputs.W_stats: [ 0.10012536  0.10009502  0.00026619  0.0032277 ]
	 average_/recognizer/encoder/bidir2/backward/fork/fork_gate_inputs.b_stats: [ 0.00054194  0.17842331  0.00032059  0.00213755]
	 average_/recognizer/encoder/bidir2/backward/fork/fork_inputs.W_stats: [  9.99536544e-02   8.54802588e-01   4.08165372e-04   5.60944332e-04]
	 average_/recognizer/encoder/bidir2/backward/fork/fork_inputs.b_stats: [  6.75720103e-04   1.72183579e+00   4.34562722e-04   2.93980354e-04]
	 average_/recognizer/encoder/bidir2/backward/gatedrecurrent.initial_state_stats: [ 0.00038502  0.06125809  0.00016392  0.00447663]
	 average_/recognizer/encoder/bidir2/backward/gatedrecurrent.state_to_gates_stats: [ 0.09983892  0.10575235  0.00027348  0.00309437]
	 average_/recognizer/encoder/bidir2/backward/gatedrecurrent.state_to_state_stats: [  1.00154520e-01   5.19240271e-01   3.89754164e-04   8.73907836e-04]
	 average_/recognizer/encoder/bidir2/forward/fork/fork_gate_inputs.W_stats: [ 0.09997719  0.09619392  0.00026165  0.0032207 ]
	 average_/recognizer/encoder/bidir2/forward/fork/fork_gate_inputs.b_stats: [ 0.00052965  0.17027513  0.00031241  0.00215647]
	 average_/recognizer/encoder/bidir2/forward/fork/fork_inputs.W_stats: [  9.99095799e-02   8.46072953e-01   4.10180712e-04   5.76936029e-04]
	 average_/recognizer/encoder/bidir2/forward/fork/fork_inputs.b_stats: [  6.62862783e-04   1.69922947e+00   4.31524409e-04   3.02390165e-04]
	 average_/recognizer/encoder/bidir2/forward/gatedrecurrent.initial_state_stats: [  1.33743318e-04   1.01025375e-02   5.71327841e-05   6.70635025e-03]
	 average_/recognizer/encoder/bidir2/forward/gatedrecurrent.state_to_gates_stats: [ 0.10009844  0.10082851  0.00026744  0.00312211]
	 average_/recognizer/encoder/bidir2/forward/gatedrecurrent.state_to_state_stats: [  1.00383533e-01   5.00813147e-01   3.89649937e-04   9.16719472e-04]
	 average_/recognizer/generator/att_trans/conv_att/conv1d.filters_stats: [  1.03385845e-01   1.71608098e-03   1.57559413e-05   9.63074742e-03]
	 average_/recognizer/generator/att_trans/conv_att/energy_comp/linear.W_stats: [  9.46863318e-02   3.71041956e+00   5.28481944e-04   1.77161704e-04]
	 average_/recognizer/generator/att_trans/conv_att/handler.W_stats: [  9.43099508e-02   1.76320323e-03   1.55522843e-05   9.64520522e-03]
	 average_/recognizer/generator/att_trans/conv_att/preprocess.W_stats: [ 0.09970464  0.18743728  0.00034742  0.00225261]
	 average_/recognizer/generator/att_trans/conv_att/preprocess.b_stats: [ 0.00057009  0.16119971  0.00033696  0.00251228]
	 average_/recognizer/generator/att_trans/conv_att/state_trans/transform_states.W_stats: [ 0.09993146  0.11504627  0.0002861   0.00296832]
	 average_/recognizer/generator/att_trans/distribute/fork_gate_inputs.W_stats: [ 0.10004991  0.1153786   0.00024764  0.00252861]
	 average_/recognizer/generator/att_trans/distribute/fork_inputs.W_stats: [  1.00389519e-01   7.75855152e-01   3.88536312e-04   5.95256218e-04]
	 average_/recognizer/generator/att_trans/transition.initial_state_stats: [ 0.00113674  0.23661354  0.00038632  0.001939  ]
	 average_/recognizer/generator/att_trans/transition.state_to_gates_stats: [ 0.09996259  0.14924286  0.00026746  0.00207988]
	 average_/recognizer/generator/att_trans/transition.state_to_state_stats: [  9.98305710e-02   5.96250620e-01   3.63500056e-04   7.17162938e-04]
	 average_/recognizer/generator/fork/fork_gate_inputs.W_stats: [  9.95401097e-02   7.25810892e-03   5.55114445e-05   8.74636658e-03]
	 average_/recognizer/generator/fork/fork_gate_inputs.b_stats: [ 0.00056188  0.21685773  0.00030946  0.00168483]
	 average_/recognizer/generator/fork/fork_inputs.W_stats: [ 0.10026672  0.04583523  0.00019746  0.00500861]
	 average_/recognizer/generator/fork/fork_inputs.b_stats: [  6.25522554e-04   1.47812988e+00   4.09666885e-04   3.31335337e-04]
	 average_/recognizer/generator/readout/lookupfeedback/lookuptable.W_stats: [ 0.09911473  0.12405798  0.00027519  0.00254402]
	 average_/recognizer/generator/readout/merge/transform_states.W_stats: [  9.99569016e-02   1.28329349e+00   3.81013227e-04   3.27678837e-04]
	 average_/recognizer/generator/readout/merge/transform_weighted_averages.W_stats: [  1.00204990e-01   9.72616962e-01   3.69553009e-04   4.23945035e-04]
	 average_/recognizer/generator/readout/post_merge/bias.b_stats: [  5.84864115e-04   1.84800115e+00   3.98165826e-04   2.41200220e-04]
	 average_/recognizer/generator/readout/post_merge/mlp/linear_0.W_stats: [  9.98084557e-02   5.76659061e+00   3.48503424e-04   6.87044663e-05]
	 average_/recognizer/generator/readout/post_merge/mlp/linear_0.b_stats: [  1.13995327e-03   5.27414539e+00   4.97567658e-04   1.07450998e-04]
	 average_batch_size: 10.0
	 average_gradient_norm_threshold: 98.2516937256
	 average_mask_density: 0.63605260849
	 average_max_energy: 5.09787559509
	 average_max_num_phonemes: 167.199996948
	 average_mean_attended: 0.547704577446
	 average_mean_bottom_output: 0.267571926117
	 average_min_energy: -5.72096586227
	 average_sequence_log_likelihood: 431.364547729
	 average_total_gradient_norm: 1191.6706543
	 average_total_step_norm: 0.653477430344
	 average_weights_entropy_per_label: -4.70119428635
	 average_weights_penalty_per_recording: 5.21301722527
	 gradient_norm_threshold: 98.2516937256
	 max_attended_length: 831.0
	 max_attended_mask_length: 831.0
	 max_recording_length: 1662.0
	 sequence_log_likelihood: 533.571411133
	 time_read_data_this_batch: 0.0275950431824
	 time_read_data_total: 0.233206987381
	 time_train_this_batch: 31.1435320377
	 time_train_total: 250.570747614
	 total_gradient_norm: 1083.12548828
	 total_step_norm: 0.629614830017
	 training_finish_requested: True


-------------------------------------------------------------------------------
TRAINING HAS BEEN FINISHED:
-------------------------------------------------------------------------------
Training status:
	 batch_interrupt_received: False
	 epoch_interrupt_received: False
	 epoch_started: True
	 epochs_done: 0
	 iterations_done: 10
	 received_first_batch: True
	 resumed_from: None
	 training_started: True
Log records from the iteration 10:
	 average_/recognizer/bottom/linear_0.W_stats: [  9.87963056e-02   1.58714834e+00   4.04041959e-04   3.32732609e-04]
	 average_/recognizer/bottom/linear_0.b_stats: [  7.16617513e-04   3.29886797e+00   4.23609486e-04   1.58945287e-04]
	 average_/recognizer/encoder/bidir0/backward/fork/fork_gate_inputs.W_stats: [ 0.10015734  0.17004602  0.00030847  0.00220164]
	 average_/recognizer/encoder/bidir0/backward/fork/fork_gate_inputs.b_stats: [ 0.00063334  0.26624792  0.00035788  0.00166376]
	 average_/recognizer/encoder/bidir0/backward/fork/fork_inputs.W_stats: [  1.00201958e-01   1.26345230e+00   4.07590717e-04   3.97723752e-04]
	 average_/recognizer/encoder/bidir0/backward/fork/fork_inputs.b_stats: [  6.99518385e-04   2.66040656e+00   4.26509385e-04   1.95658378e-04]
	 average_/recognizer/encoder/bidir0/backward/gatedrecurrent.initial_state_stats: [ 0.00045526  0.05453042  0.0001767   0.00551876]
	 average_/recognizer/encoder/bidir0/backward/gatedrecurrent.state_to_gates_stats: [ 0.10010799  0.13937994  0.00030217  0.00271753]
	 average_/recognizer/encoder/bidir0/backward/gatedrecurrent.state_to_state_stats: [  9.98947273e-02   6.10140164e-01   3.91208190e-04   7.98241905e-04]
	 average_/recognizer/encoder/bidir0/forward/fork/fork_gate_inputs.W_stats: [ 0.09957625  0.18927767  0.0003117   0.0019548 ]
	 average_/recognizer/encoder/bidir0/forward/fork/fork_gate_inputs.b_stats: [ 0.0006052   0.27754369  0.00035931  0.00159199]
	 average_/recognizer/encoder/bidir0/forward/fork/fork_inputs.W_stats: [  9.96452446e-02   1.30010881e+00   4.10708508e-04   3.79780788e-04]
	 average_/recognizer/encoder/bidir0/forward/fork/fork_inputs.b_stats: [  6.75619548e-04   2.74292577e+00   4.27124275e-04   1.86804402e-04]
	 average_/recognizer/encoder/bidir0/forward/gatedrecurrent.initial_state_stats: [  1.60508458e-04   1.04048581e-02   7.20706318e-05   8.22683098e-03]
	 average_/recognizer/encoder/bidir0/forward/gatedrecurrent.state_to_gates_stats: [ 0.09992362  0.14696877  0.00030473  0.00257063]
	 average_/recognizer/encoder/bidir0/forward/gatedrecurrent.state_to_state_stats: [  1.00222920e-01   6.15754762e-01   3.91739285e-04   7.83385089e-04]
	 average_/recognizer/encoder/bidir1/backward/fork/fork_gate_inputs.W_stats: [ 0.10014759  0.10413263  0.00027319  0.00316884]
	 average_/recognizer/encoder/bidir1/backward/fork/fork_gate_inputs.b_stats: [ 0.0005749   0.19878957  0.0003348   0.00204631]
	 average_/recognizer/encoder/bidir1/backward/fork/fork_inputs.W_stats: [  1.00199812e-01   8.06206705e-01   3.95496231e-04   5.87115998e-04]
	 average_/recognizer/encoder/bidir1/backward/fork/fork_inputs.b_stats: [  6.28432098e-04   1.85805402e+00   4.14234786e-04   2.64075847e-04]
	 average_/recognizer/encoder/bidir1/backward/gatedrecurrent.initial_state_stats: [ 0.00040408  0.07604756  0.00017217  0.00370306]
	 average_/recognizer/encoder/bidir1/backward/gatedrecurrent.state_to_gates_stats: [ 0.10030181  0.11438521  0.00028197  0.00300878]
	 average_/recognizer/encoder/bidir1/backward/gatedrecurrent.state_to_state_stats: [  9.97602135e-02   5.21025026e-01   3.78008559e-04   8.74028268e-04]
	 average_/recognizer/encoder/bidir1/forward/fork/fork_gate_inputs.W_stats: [ 0.10019892  0.10780604  0.00027663  0.00317284]
	 average_/recognizer/encoder/bidir1/forward/fork/fork_gate_inputs.b_stats: [ 0.0006      0.21189247  0.00033539  0.00195516]
	 average_/recognizer/encoder/bidir1/forward/fork/fork_inputs.W_stats: [  9.95732521e-02   8.79620625e-01   3.95888227e-04   5.49430316e-04]
	 average_/recognizer/encoder/bidir1/forward/fork/fork_inputs.b_stats: [  6.57443330e-04   2.02369350e+00   4.15619570e-04   2.47904852e-04]
	 average_/recognizer/encoder/bidir1/forward/gatedrecurrent.initial_state_stats: [  1.85947465e-04   1.88010207e-02   8.26841668e-05   5.96084483e-03]
	 average_/recognizer/encoder/bidir1/forward/gatedrecurrent.state_to_gates_stats: [ 0.09968547  0.11974477  0.00028747  0.0029856 ]
	 average_/recognizer/encoder/bidir1/forward/gatedrecurrent.state_to_state_stats: [  9.99971558e-02   5.50266733e-01   3.84207222e-04   8.56264264e-04]
	 average_/recognizer/encoder/bidir2/backward/fork/fork_gate_inputs.W_stats: [ 0.10012536  0.10009502  0.00026619  0.0032277 ]
	 average_/recognizer/encoder/bidir2/backward/fork/fork_gate_inputs.b_stats: [ 0.00054194  0.17842331  0.00032059  0.00213755]
	 average_/recognizer/encoder/bidir2/backward/fork/fork_inputs.W_stats: [  9.99536544e-02   8.54802588e-01   4.08165372e-04   5.60944332e-04]
	 average_/recognizer/encoder/bidir2/backward/fork/fork_inputs.b_stats: [  6.75720103e-04   1.72183579e+00   4.34562722e-04   2.93980354e-04]
	 average_/recognizer/encoder/bidir2/backward/gatedrecurrent.initial_state_stats: [ 0.00038502  0.06125809  0.00016392  0.00447663]
	 average_/recognizer/encoder/bidir2/backward/gatedrecurrent.state_to_gates_stats: [ 0.09983892  0.10575235  0.00027348  0.00309437]
	 average_/recognizer/encoder/bidir2/backward/gatedrecurrent.state_to_state_stats: [  1.00154520e-01   5.19240271e-01   3.89754164e-04   8.73907836e-04]
	 average_/recognizer/encoder/bidir2/forward/fork/fork_gate_inputs.W_stats: [ 0.09997719  0.09619392  0.00026165  0.0032207 ]
	 average_/recognizer/encoder/bidir2/forward/fork/fork_gate_inputs.b_stats: [ 0.00052965  0.17027513  0.00031241  0.00215647]
	 average_/recognizer/encoder/bidir2/forward/fork/fork_inputs.W_stats: [  9.99095799e-02   8.46072953e-01   4.10180712e-04   5.76936029e-04]
	 average_/recognizer/encoder/bidir2/forward/fork/fork_inputs.b_stats: [  6.62862783e-04   1.69922947e+00   4.31524409e-04   3.02390165e-04]
	 average_/recognizer/encoder/bidir2/forward/gatedrecurrent.initial_state_stats: [  1.33743318e-04   1.01025375e-02   5.71327841e-05   6.70635025e-03]
	 average_/recognizer/encoder/bidir2/forward/gatedrecurrent.state_to_gates_stats: [ 0.10009844  0.10082851  0.00026744  0.00312211]
	 averageFunction profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/monitoring/evaluators.py:166
  Time in 11 calls to Function.__call__: 1.892376e-02s
  Time in Function.fn.__call__: 1.532245e-02s (80.969%)
  Time in thunks: 9.180784e-03s (48.515%)
  Total compile time: 1.069870e-01s
    Number of Apply nodes: 19
    Theano Optimizer time: 3.292608e-02s
       Theano validate time: 8.025169e-04s
    Theano Linker time (includes C, CUDA code generation/compiling): 5.309010e-02s
       Import time 5.381107e-04s

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  100.0%   100.0%       0.009s       4.39e-05s     C      209      19   theano.compile.ops.DeepCopyOp
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  100.0%   100.0%       0.009s       4.39e-05s     C      209       19   DeepCopyOp
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  19.8%    19.8%       0.002s       1.66e-04s     11     0                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=() 
   6.1%    25.9%       0.001s       5.09e-05s     11     4                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=() 
   5.9%    31.9%       0.001s       4.93e-05s     11     2                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=() 
   5.9%    37.7%       0.001s       4.91e-05s     11     1                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=() 
   5.7%    43.5%       0.001s       4.80e-05s     11    14                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   5.7%    49.2%       0.001s       4.73e-05s     11     6                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=() 
   5.6%    54.8%       0.001s       4.69e-05s     11     3                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=() 
   5.4%    60.2%       0.000s       4.51e-05s     11     5                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=() 
   5.4%    65.6%       0.000s       4.49e-05s     11    12                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   5.3%    70.9%       0.000s       4.45e-05s     11    13                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   5.3%    76.2%       0.000s       4.42e-05s     11     7                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=() 
   5.2%    81.4%       0.000s       4.38e-05s     11     8                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=() 
   5.2%    86.6%       0.000s       4.33e-05s     11     9                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   5.1%    91.7%       0.000s       4.23e-05s     11    11                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   5.1%    96.7%       0.000s       4.23e-05s     11    10                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   1.0%    97.8%       0.000s       8.56e-06s     11    15                     DeepCopyOp(TensorConstant{0})
    input 0: dtype=int64, shape=(), strides=c 
    output 0: dtype=int64, shape=(), strides=c 
   0.9%    98.7%       0.000s       7.76e-06s     11    17                     DeepCopyOp(TensorConstant{0})
    input 0: dtype=int64, shape=(), strides=c 
    output 0: dtype=int64, shape=(), strides=c 
   0.7%    99.4%       0.000s       5.55e-06s     11    16                     DeepCopyOp(TensorConstant{0})
    input 0: dtype=int64, shape=(), strides=c 
    output 0: dtype=int64, shape=(), strides=c 
   0.6%   100.0%       0.000s       5.29e-06s     11    18                     DeepCopyOp(TensorConstant{0.0})
    input 0: dtype=float64, shape=(), strides=c 
    output 0: dtype=float64, shape=(), strides=c 
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 0KB (0KB)
    CPU: 0KB (0KB)
    GPU: 0KB (0KB)
---
    Max if linker=cvm(default): 0KB (0KB)
    CPU: 0KB (0KB)
    GPU: 0KB (0KB)
---
    Memory saved if views are used: 0KB (0KB)
    Memory saved if inplace ops are used: 0KB (0KB)
    Memory saved if gc is enabled: 0KB (0KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

   ... (remaining 19 Apply account for   92B/92B ((100.00%)) of the Apply with dense outputs sizes)

    All Apply nodes have output sizes that take less than 1024B.
    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/monitoring/evaluators.py:176
  Time in 10 calls to Function.__call__: 8.964777e-03s
  Time in Function.fn.__call__: 8.498192e-03s (94.795%)
  Time in thunks: 3.453255e-03s (38.520%)
  Total compile time: 5.236197e-02s
    Number of Apply nodes: 13
    Theano Optimizer time: 2.439809e-02s
       Theano validate time: 3.480911e-05s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.176190e-02s
       Import time 8.389950e-04s

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  52.0%    52.0%       0.002s       2.56e-05s     C       70       7   theano.sandbox.cuda.basic_ops.HostFromGpu
  41.7%    93.6%       0.001s       7.19e-05s     C       20       2   theano.sandbox.cuda.basic_ops.GpuElemwise
   6.4%   100.0%       0.000s       5.48e-06s     C       40       4   theano.tensor.elemwise.Elemwise
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  52.0%    52.0%       0.002s       2.56e-05s     C       70        7   HostFromGpu
  41.7%    93.6%       0.001s       7.19e-05s     C       20        2   GpuElemwise{true_div,no_inplace}
   6.4%   100.0%       0.000s       5.48e-06s     C       40        4   Elemwise{true_div,no_inplace}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  28.5%    28.5%       0.001s       9.83e-05s     10     4                     GpuElemwise{true_div,no_inplace}(shared_total_step_norm, shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=() 
  14.6%    43.0%       0.001s       5.03e-05s     10     0                     HostFromGpu(shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
  13.2%    56.2%       0.000s       4.56e-05s     10     5                     GpuElemwise{true_div,no_inplace}(shared_total_gradient_norm, shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=() 
   6.5%    62.7%       0.000s       2.24e-05s     10     1                     HostFromGpu(shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   6.4%    69.1%       0.000s       2.19e-05s     10     3                     HostFromGpu(shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   6.3%    75.4%       0.000s       2.17e-05s     10    10                     HostFromGpu(GpuElemwise{true_div,no_inplace}.0)
    input 0: dtype=float32, shape=(), strides=() 
    output 0: dtype=float32, shape=(), strides=c 
   6.2%    81.6%       0.000s       2.14e-05s     10    11                     HostFromGpu(GpuElemwise{true_div,no_inplace}.0)
    input 0: dtype=float32, shape=(), strides=() 
    output 0: dtype=float32, shape=(), strides=c 
   6.2%    87.7%       0.000s       2.12e-05s     10     6                     HostFromGpu(shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   5.9%    93.6%       0.000s       2.05e-05s     10     2                     HostFromGpu(shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   1.9%    95.6%       0.000s       6.60e-06s     10     7                     Elemwise{true_div,no_inplace}(shared_max_attended_mask_length, HostFromGpu.0)
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float64, shape=(), strides=c 
   1.6%    97.2%       0.000s       5.60e-06s     10    12                     Elemwise{true_div,no_inplace}(shared_sequence_log_likelihood, HostFromGpu.0)
    input 0: dtype=float64, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float64, shape=(), strides=c 
   1.4%    98.6%       0.000s       4.91e-06s     10     8                     Elemwise{true_div,no_inplace}(shared_max_attended_length, HostFromGpu.0)
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float64, shape=(), strides=c 
   1.4%   100.0%       0.000s       4.82e-06s     10     9                     Elemwise{true_div,no_inplace}(shared_max_recording_length, HostFromGpu.0)
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float64, shape=(), strides=c 
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 0KB (0KB)
    CPU: 0KB (0KB)
    GPU: 0KB (0KB)
---
    Max if linker=cvm(default): 0KB (0KB)
    CPU: 0KB (0KB)
    GPU: 0KB (0KB)
---
    Memory saved if views are used: 0KB (0KB)
    Memory saved if inplace ops are used: 0KB (0KB)
    Memory saved if gc is enabled: 0KB (0KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

   ... (remaining 13 Apply account for   68B/68B ((100.00%)) of the Apply with dense outputs sizes)

    All Apply nodes have output sizes that take less than 1024B.
    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/monitoring/evaluators.py:166
  Time in 2 calls to Function.__call__: 9.063292e-02s
  Time in Function.fn.__call__: 8.394098e-02s (92.616%)
  Time in thunks: 1.579475e-02s (17.427%)
  Total compile time: 2.204037e+00s
    Number of Apply nodes: 297
    Theano Optimizer time: 1.415451e+00s
       Theano validate time: 3.391862e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 2.620409e-01s
       Import time 5.309582e-04s

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  91.4%    91.4%       0.014s       4.32e-05s     C      334     167   theano.compile.ops.DeepCopyOp
   4.4%    95.9%       0.001s       5.39e-06s     C      130      65   theano.tensor.basic.Alloc
   4.1%   100.0%       0.001s       5.02e-06s     C      130      65   theano.compile.ops.Shape_i
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  91.4%    91.4%       0.014s       4.32e-05s     C      334      167   DeepCopyOp
   4.4%    95.9%       0.001s       5.39e-06s     C      130       65   Alloc
   4.1%   100.0%       0.001s       5.02e-06s     C      130       65   Shape_i{0}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
   0.9%     0.9%       0.000s       6.76e-05s      2     0                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.8%     1.7%       0.000s       6.44e-05s      2    58                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.8%     2.5%       0.000s       6.29e-05s      2   118                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.7%     3.2%       0.000s       5.46e-05s      2     3                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.7%     3.8%       0.000s       5.45e-05s      2     1                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.7%     4.5%       0.000s       5.39e-05s      2   159                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.7%     5.2%       0.000s       5.35e-05s      2    54                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.7%     5.9%       0.000s       5.26e-05s      2    90                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.7%     6.5%       0.000s       5.20e-05s      2    84                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.7%     7.2%       0.000s       5.16e-05s      2    29                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.6%     7.8%       0.000s       5.10e-05s      2    39                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.6%     8.5%       0.000s       4.96e-05s      2    86                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.6%     9.1%       0.000s       4.89e-05s      2    57                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.6%     9.7%       0.000s       4.84e-05s      2    17                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.6%    10.3%       0.000s       4.80e-05s      2   114                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.6%    10.9%       0.000s       4.80e-05s      2    63                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.6%    11.5%       0.000s       4.74e-05s      2   135                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.6%    12.1%       0.000s       4.70e-05s      2     2                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.6%    12.7%       0.000s       4.66e-05s      2    88                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.6%    13.3%       0.000s       4.60e-05s      2    80                     DeepCopyOp(CudaNdarrayConstant{0.0})
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   ... (remaining 277 Apply instances account for 86.72%(0.01s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 3KB (3KB)
    CPU: 3KB (3KB)
    GPU: 1KB (1KB)
---
    Max if linker=cvm(default): 3KB (3KB)
    CPU: 2KB (2KB)
    GPU: 1KB (1KB)
---
    Memory saved if views are used: 0KB (0KB)
    Memory saved if inplace ops are used: 0KB (0KB)
    Memory saved if gc is enabled: 0KB (0KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

   ... (remaining 297 Apply account for 3284B/3284B ((100.00%)) of the Apply with dense outputs sizes)

    All Apply nodes have output sizes that take less than 1024B.
    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/monitoring/evaluators.py:176
  Time in 1 calls to Function.__call__: 1.951504e-02s
  Time in Function.fn.__call__: 1.923108e-02s (98.545%)
  Time in thunks: 2.548456e-03s (13.059%)
  Total compile time: 8.275430e-01s
    Number of Apply nodes: 220
    Theano Optimizer time: 3.083150e-01s
       Theano validate time: 1.117468e-03s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.847658e-01s
       Import time 5.369186e-04s

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  61.3%    61.3%       0.002s       2.00e-05s     C       78      78   theano.sandbox.cuda.basic_ops.HostFromGpu
  13.7%    75.0%       0.000s       5.07e-06s     C       69      69   theano.tensor.elemwise.Elemwise
  13.4%    88.4%       0.000s       4.28e-05s     C        8       8   theano.sandbox.cuda.basic_ops.GpuElemwise
  11.6%   100.0%       0.000s       4.54e-06s     C       65      65   theano.sandbox.cuda.basic_ops.GpuDimShuffle
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  61.3%    61.3%       0.002s       2.00e-05s     C       78       78   HostFromGpu
  13.7%    75.0%       0.000s       5.07e-06s     C       69       69   Elemwise{true_div,no_inplace}
  13.4%    88.4%       0.000s       4.28e-05s     C        8        8   GpuElemwise{true_div,no_inplace}
  11.6%   100.0%       0.000s       4.54e-06s     C       65       65   GpuDimShuffle{x}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
   2.0%     2.0%       0.000s       5.20e-05s      1    66                     GpuElemwise{true_div,no_inplace}(shared_total_gradient_norm, shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   1.8%     3.8%       0.000s       4.48e-05s      1    71                     GpuElemwise{true_div,no_inplace}(shared_mean_bottom_output, shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   1.6%     5.4%       0.000s       4.20e-05s      1    67                     GpuElemwise{true_div,no_inplace}(shared_total_step_norm, shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   1.6%     7.1%       0.000s       4.10e-05s      1    74                     GpuElemwise{true_div,no_inplace}(shared_min_energy, shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   1.6%     8.7%       0.000s       4.10e-05s      1    73                     GpuElemwise{true_div,no_inplace}(shared_max_energy, shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   1.6%    10.3%       0.000s       4.10e-05s      1    72                     GpuElemwise{true_div,no_inplace}(shared_mean_attended, shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   1.6%    11.9%       0.000s       4.08e-05s      1    68                     GpuElemwise{true_div,no_inplace}(shared_mask_density, shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   1.6%    13.4%       0.000s       4.01e-05s      1    75                     GpuElemwise{true_div,no_inplace}(shared_weights_entropy_per_label, shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   1.1%    14.5%       0.000s       2.72e-05s      1    78                     HostFromGpu(GpuDimShuffle{x}.0)
    input 0: dtype=float32, shape=(1,), strides=c 
    output 0: dtype=float32, shape=(1,), strides=c 
   0.9%    15.4%       0.000s       2.22e-05s      1   127                     HostFromGpu(GpuDimShuffle{x}.0)
    input 0: dtype=float32, shape=(1,), strides=c 
    output 0: dtype=float32, shape=(1,), strides=c 
   0.9%    16.3%       0.000s       2.22e-05s      1    69                     HostFromGpu(shared_None)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.9%    17.1%       0.000s       2.19e-05s      1   150                     HostFromGpu(GpuElemwise{true_div,no_inplace}.0)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.9%    18.0%       0.000s       2.19e-05s      1   149                     HostFromGpu(GpuElemwise{true_div,no_inplace}.0)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.9%    18.8%       0.000s       2.19e-05s      1   143                     HostFromGpu(GpuElemwise{true_div,no_inplace}.0)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.9%    19.7%       0.000s       2.19e-05s      1   140                     HostFromGpu(GpuDimShuffle{x}.0)
    input 0: dtype=float32, shape=(1,), strides=c 
    output 0: dtype=float32, shape=(1,), strides=c 
   0.9%    20.6%       0.000s       2.19e-05s      1    79                     HostFromGpu(GpuDimShuffle{x}.0)
    input 0: dtype=float32, shape=(1,), strides=c 
    output 0: dtype=float32, shape=(1,), strides=c 
   0.8%    21.4%       0.000s       2.12e-05s      1   107                     HostFromGpu(GpuDimShuffle{x}.0)
    input 0: dtype=float32, shape=(1,), strides=c 
    output 0: dtype=float32, shape=(1,), strides=c 
   0.8%    22.2%       0.000s       2.10e-05s      1   148                     HostFromGpu(GpuElemwise{true_div,no_inplace}.0)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.8%    23.0%       0.000s       2.10e-05s      1   137                     HostFromGpu(GpuDimShuffle{x}.0)
    input 0: dtype=float32, shape=(1,), strides=c 
    output 0: dtype=float32, shape=(1,), strides=c 
   0.8%    23.9%       0.000s       2.10e-05s      1   136                     HostFromGpu(GpuDimShuffle{x}.0)
    input 0: dtype=float32, shape=(1,), strides=c 
    output 0: dtype=float32, shape=(1,), strides=c 
   ... (remaining 200 Apply instances account for 76.14%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 2KB (2KB)
    CPU: 2KB (2KB)
    GPU: 0KB (0KB)
---
    Max if linker=cvm(default): 2KB (2KB)
    CPU: 2KB (2KB)
    GPU: 0KB (0KB)
---
    Memory saved if views are used: 0KB (0KB)
    Memory saved if inplace ops are used: 0KB (0KB)
    Memory saved if gc is enabled: 0KB (0KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

   ... (remaining 220 Apply account for 2716B/2716B ((100.00%)) of the Apply with dense outputs sizes)

    All Apply nodes have output sizes that take less than 1024B.
    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/monitoring/evaluators.py:166
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 4.124093e-02s
    Number of Apply nodes: 9
    Theano Optimizer time: 1.702690e-02s
       Theano validate time: 3.590584e-04s
    Theano Linker time (includes C, CUDA code generation/compiling): 7.205009e-03s
       Import time 0.000000e+00s

Time in all call to theano.grad() 5.968459e+00s
Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/monitoring/evaluators.py:176
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 3.047895e-02s
    Number of Apply nodes: 6
    Theano Optimizer time: 1.377320e-02s
       Theano validate time: 1.621246e-05s
    Theano Linker time (includes C, CUDA code generation/compiling): 4.308939e-03s
       Import time 0.000000e+00s

Time in all call to theano.grad() 5.968459e+00s
Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/monitoring/evaluators.py:275
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 1.634387e+01s
    Number of Apply nodes: 822
    Theano Optimizer time: 1.383880e+01s
       Theano validate time: 7.661479e-01s
    Theano Linker time (includes C, CUDA code generation/compiling): 2.314159e+00s
       Import time 3.688002e-02s

Time in all call to theano.grad() 5.968459e+00s
Time in all call to theano.grad() 5.968459e+00s
Time in all call to theano.grad() 5.968459e+00s
Time in all call to theano.grad() 5.968459e+00s
Time in all call to theano.grad() 5.968459e+00s
Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/search.py:95
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 9.308657e+00s
    Number of Apply nodes: 505
    Theano Optimizer time: 8.184754e+00s
       Theano validate time: 1.383002e-01s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.057746e+00s
       Import time 5.426645e-03s

Time in all call to theano.grad() 5.968459e+00s
Time in all call to theano.grad() 5.968459e+00s
Time in all call to theano.grad() 5.968459e+00s
Time in all call to theano.grad() 5.968459e+00s
Time in all call to theano.grad() 5.968459e+00s
Time in all call to theano.grad() 5.968459e+00s
Time in all call to theano.grad() 5.968459e+00s
Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/search.py:103
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 8.511400e-02s
    Number of Apply nodes: 14
    Theano Optimizer time: 5.487108e-02s
       Theano validate time: 1.065016e-03s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.143384e-02s
       Import time 1.388550e-03s

Time in all call to theano.grad() 5.968459e+00s
Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/search.py:114
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 1.705330e+00s
    Number of Apply nodes: 129
    Theano Optimizer time: 1.522025e+00s
       Theano validate time: 3.090596e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.450849e-01s
       Import time 9.166241e-03s

Time in all call to theano.grad() 5.968459e+00s
Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/search.py:126
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 9.863050e-01s
    Number of Apply nodes: 120
    Theano Optimizer time: 8.207822e-01s
       Theano validate time: 2.932429e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.328220e-01s
       Import time 2.795935e-03s

Time in all call to theano.grad() 5.968459e+00s
Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/monitoring/evaluators.py:176
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 1.857901e-02s
    Number of Apply nodes: 0
    Theano Optimizer time: 5.228996e-03s
       Theano validate time: 0.000000e+00s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.060963e-04s
       Import time 0.000000e+00s

Time in all call to theano.grad() 5.968459e+00s
Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/monitoring/evaluators.py:275
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 2.130604e-02s
    Number of Apply nodes: 2
    Theano Optimizer time: 7.462978e-03s
       Theano validate time: 0.000000e+00s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.019001e-03s
       Import time 0.000000e+00s

Time in all call to theano.grad() 5.968459e+00s
Function profiling
==================
  Message: /home/rizar/dist/fully-neural-lvsr/blocks/blocks/algorithms/__init__.py:232
  Time in 10 calls to Function.__call__: 2.504264e+02s
  Time in Function.fn.__call__: 2.503187e+02s (99.957%)
  Time in thunks: 2.137404e+02s (85.351%)
  Total compile time: 3.910738e+02s
    Number of Apply nodes: 5364
    Theano Optimizer time: 3.263339e+02s
       Theano validate time: 6.187315e+01s
    Theano Linker time (includes C, CUDA code generation/compiling): 2.262656e+01s
       Import time 9.240985e-02s

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  96.6%    96.6%     206.482s       2.58e+00s     Py      80       8   theano.scan_module.scan_op.Scan
   1.0%    97.6%       2.183s       3.90e-03s     C      560      56   theano.sandbox.cuda.blas.GpuDot22
   0.5%    98.1%       0.986s       4.27e-04s     C     2310     231   theano.sandbox.cuda.basic_ops.GpuCAReduce
   0.3%    98.4%       0.656s       4.10e-03s     C      160      16   theano.sandbox.cuda.blas.GpuGemm
   0.3%    98.6%       0.538s       5.38e-05s     C    10000    1000   theano.sandbox.cuda.basic_ops.GpuElemwise
   0.2%    98.8%       0.436s       5.13e-04s     C      850      85   theano.sandbox.cuda.basic_ops.GpuFromHost
   0.2%    99.0%       0.370s       2.55e-05s     C    14480    1448   theano.tensor.elemwise.Elemwise
   0.2%    99.2%       0.352s       1.92e-04s     Py    1830     183   theano.sandbox.cuda.basic_ops.GpuFlatten
   0.2%    99.3%       0.323s       4.68e-04s     C      690      69   theano.sandbox.cuda.basic_ops.GpuIncSubtensor
   0.1%    99.5%       0.304s       6.39e-05s     C     4760     476   theano.sandbox.cuda.basic_ops.HostFromGpu
   0.1%    99.6%       0.215s       2.87e-04s     C      750      75   theano.sandbox.cuda.basic_ops.GpuAlloc
   0.1%    99.7%       0.175s       1.75e-02s     C       10       1   theano.sandbox.cuda.basic_ops.GpuAdvancedIncSubtensor1
   0.1%    99.7%       0.140s       1.55e-03s     C       90       9   theano.tensor.basic.Alloc
   0.1%    99.8%       0.108s       2.70e-03s     Py      40       4   theano.sandbox.cuda.basic_ops.GpuSplit
   0.0%    99.8%       0.087s       1.45e-03s     C       60       6   theano.sandbox.cuda.basic_ops.GpuJoin
   0.0%    99.8%       0.061s       4.59e-05s     C     1320     132   theano.tensor.elemwise.Sum
   0.0%    99.9%       0.058s       5.85e-03s     C       10       1   theano.tensor.basic.Join
   0.0%    99.9%       0.056s       3.07e-05s     Py    1840     184   theano.sandbox.cuda.basic_ops.GpuReshape
   0.0%    99.9%       0.044s       5.16e-05s     C      850      85   theano.compile.ops.DeepCopyOp
   0.0%    99.9%       0.038s       1.52e-05s     Py    2490     166   theano.ifelse.IfElse
   ... (remaining 19 Classes account for   0.06%(0.13s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  45.7%    45.7%      97.775s       3.26e+00s     Py      30        3   forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}
  29.1%    74.8%      62.122s       6.21e+00s     Py      10        1   forall_inplace,gpu,grad_of_att_trans_do_apply_scan}
  16.9%    91.7%      36.035s       1.20e+00s     Py      30        3   forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}
   4.9%    96.6%      10.551s       1.06e+00s     Py      10        1   forall_inplace,gpu,att_trans_do_apply_scan}
   1.0%    97.6%       2.183s       3.90e-03s     C      560       56   GpuDot22
   0.3%    97.9%       0.656s       4.10e-03s     C      160       16   GpuGemm{inplace}
   0.2%    98.1%       0.436s       5.13e-04s     C      850       85   GpuFromHost
   0.2%    98.3%       0.370s       2.18e-03s     C      170       17   GpuCAReduce{add}{1,1,0}
   0.2%    98.5%       0.341s       2.28e-03s     Py     150       15   GpuFlatten{2}
   0.1%    98.6%       0.304s       6.39e-05s     C     4760      476   HostFromGpu
   0.1%    98.7%       0.263s       3.42e-04s     C      770       77   GpuCAReduce{pre=sqr,red=add}{1}
   0.1%    98.8%       0.226s       1.88e-04s     C     1200      120   GpuCAReduce{pre=sqr,red=add}{1,1}
   0.1%    98.9%       0.202s       3.06e-04s     C      660       66   GpuAlloc{memset_0=True}
   0.1%    99.0%       0.175s       1.75e-02s     C       10        1   GpuAdvancedIncSubtensor1{inplace,inc}
   0.1%    99.1%       0.147s       2.27e-04s     C      650       65   Elemwise{isinf,no_inplace}
   0.1%    99.2%       0.140s       1.55e-03s     C       90        9   Alloc
   0.1%    99.2%       0.133s       2.04e-04s     C      650       65   Elemwise{isnan,no_inplace}
   0.1%    99.3%       0.114s       5.69e-03s     C       20        2   GpuCAReduce{add}{1,1,1}
   0.0%    99.3%       0.093s       4.44e-04s     C      210       21   GpuIncSubtensor{InplaceInc;int64::}
   0.0%    99.4%       0.087s       1.45e-03s     C       60        6   GpuJoin
   ... (remaining 228 Ops account for   0.65%(1.39s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  29.1%    29.1%      62.122s       6.21e+00s     10   2504                     forall_inplace,gpu,grad_of_att_trans_do_apply_scan}(Shape_i{0}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, Subtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,2,1}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSub
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(183, 10, 831), strides=(8310, 831, 1) 
    input 2: dtype=float32, shape=(183, 10, 500), strides=(5000, 500, 1) 
    input 3: dtype=float32, shape=(183, 10), strides=(10, 1) 
    input 4: dtype=float32, shape=(183, 10, 500), strides=(-5000, 500, 1) 
    input 5: dtype=float32, shape=(183, 10, 250), strides=(-2500, 250, 1) 
    input 6: dtype=float32, shape=(183, 10, 250), strides=(-2500, 250, 1) 
    input 7: dtype=int64, shape=(183, 10), strides=(-80, 8) 
    input 8: dtype=float32, shape=(183, 10, 831), strides=(-8310, 831, 1) 
    input 9: dtype=float32, shape=(183, 10, 500), strides=(-5000, 500, 1) 
    input 10: dtype=float32, shape=(183, 250, 10), strides=(-2500, 1, 250) 
    input 11: dtype=float32, shape=(183, 10, 1), strides=(-10, 1, 0) 
    input 12: dtype=float32, shape=(184, 10, 250), strides=(-2500, 250, 1) 
    input 13: dtype=float32, shape=(184, 10, 500), strides=(-5000, 500, 1) 
    input 14: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    input 15: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    input 16: dtype=float32, shape=(184, 10), strides=(10, 1) 
    input 17: dtype=float32, shape=(2, 250, 250), strides=(62500, 250, 1) 
    input 18: dtype=float32, shape=(2, 1, 201), strides=(201, 0, 1) 
    input 19: dtype=float32, shape=(2, 1, 250), strides=(250, 0, 1) 
    input 20: dtype=float32, shape=(2, 250, 1), strides=(250, 1, 0) 
    input 21: dtype=float32, shape=(2, 831, 10, 500), strides=(4155000, 5000, 500, 1) 
    input 22: dtype=float32, shape=(2, 831, 10, 250), strides=(2077500, 2500, 250, 1) 
    input 23: dtype=int64, shape=(), strides=c 
    input 24: dtype=int64, shape=(), strides=c 
    input 25: dtype=int64, shape=(), strides=c 
    input 26: dtype=float32, shape=(250, 500), strides=c 
    input 27: dtype=float32, shape=(250, 250), strides=c 
    input 28: dtype=float32, shape=(500, 500), strides=c 
    input 29: dtype=float32, shape=(250, 250), strides=c 
    input 30: dtype=float32, shape=(500, 250), strides=c 
    input 31: dtype=float32, shape=(500, 500), strides=(1, 500) 
    input 32: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 33: dtype=float32, shape=(250, 500), strides=(1, 250) 
    input 34: dtype=float32, shape=(250, 250), strides=(1, 250) 
    input 35: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
    input 36: dtype=float32, shape=(250, 250), strides=(1, 250) 
    input 37: dtype=int64, shape=(2,), strides=c 
    input 38: dtype=int64, shape=(2,), strides=c 
    input 39: dtype=float32, shape=(831, 10, 500), strides=(10000, 500, 1) 
    input 40: dtype=int64, shape=(1,), strides=c 
    input 41: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, -1) 
    input 42: dtype=float32, shape=(831, 10, 250), strides=(2500, 250, 1) 
    input 43: dtype=int64, shape=(1,), strides=c 
    input 44: dtype=float32, shape=(831, 10), strides=(20, 1) 
    input 45: dtype=float32, shape=(831, 10, 500), strides=(5000, 500, 1) 
    input 46: dtype=float32, shape=(831, 10, 250), strides=(2500, 250, 1) 
    input 47: dtype=int64, shape=(), strides=c 
    input 48: dtype=int64, shape=(), strides=c 
    input 49: dtype=float32, shape=(1, 250), strides=(0, 1) 
    input 50: dtype=float32, shape=(250, 1), strides=(1, 0) 
    input 51: dtype=float32, shape=(250, 1), strides=(1, 0) 
    input 52: dtype=float32, shape=(1, 250), strides=(0, 1) 
    output 0: dtype=float32, shape=(184, 10, 250), strides=(-2500, 250, 1) 
    output 1: dtype=float32, shape=(184, 10, 500), strides=(-5000, 500, 1) 
    output 2: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    output 3: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    output 4: dtype=float32, shape=(184, 10), strides=(10, 1) 
    output 5: dtype=float32, shape=(2, 250, 250), strides=(62500, 250, 1) 
    output 6: dtype=float32, shape=(2, 1, 201), strides=(201, 0, 1) 
    output 7: dtype=float32, shape=(2, 1, 250), strides=(250, 0, 1) 
    output 8: dtype=float32, shape=(2, 250, 1), strides=(250, 1, 0) 
    output 9: dtype=float32, shape=(2, 831, 10, 500), strides=(4155000, 5000, 500, 1) 
    output 10: dtype=float32, shape=(2, 831, 10, 250), strides=(2077500, 2500, 250, 1) 
    output 11: dtype=float32, shape=(183, 10, 250), strides=(2500, 250, 1) 
    output 12: dtype=float32, shape=(183, 10, 500), strides=(5000, 500, 1) 
    output 13: dtype=float32, shape=(183, 250, 10), strides=(2500, 10, 1) 
  15.4%    44.4%      32.875s       3.29e+00s     10   2612                     forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 4: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 5: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 6: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 7: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 9: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 10: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 11: dtype=int64, shape=(), strides=c 
    input 12: dtype=int64, shape=(), strides=c 
    input 13: dtype=int64, shape=(), strides=c 
    input 14: dtype=int64, shape=(), strides=c 
    input 15: dtype=int64, shape=(), strides=c 
    input 16: dtype=int64, shape=(), strides=c 
    input 17: dtype=float32, shape=(250, 500), strides=c 
    input 18: dtype=float32, shape=(250, 250), strides=c 
    input 19: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 20: dtype=float32, shape=(250, 250), strides=(1, 250) 
    input 21: dtype=float32, shape=(250, 500), strides=c 
    input 22: dtype=float32, shape=(250, 250), strides=c 
    input 23: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 24: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 3: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 4: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
    output 5: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 6: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 7: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
  15.3%    59.7%      32.689s       3.27e+00s     10   2744                     forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 4: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 5: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 6: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 7: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 9: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 10: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 11: dtype=int64, shape=(), strides=c 
    input 12: dtype=int64, shape=(), strides=c 
    input 13: dtype=int64, shape=(), strides=c 
    input 14: dtype=int64, shape=(), strides=c 
    input 15: dtype=int64, shape=(), strides=c 
    input 16: dtype=int64, shape=(), strides=c 
    input 17: dtype=float32, shape=(250, 500), strides=c 
    input 18: dtype=float32, shape=(250, 250), strides=c 
    input 19: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 20: dtype=float32, shape=(250, 250), strides=(1, 250) 
    input 21: dtype=float32, shape=(250, 500), strides=c 
    input 22: dtype=float32, shape=(250, 250), strides=c 
    input 23: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 24: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 3: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 4: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
    output 5: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 6: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 7: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
  15.1%    74.8%      32.211s       3.22e+00s     10   2875                     forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 4: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 5: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 6: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 7: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 9: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 10: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 11: dtype=int64, shape=(), strides=c 
    input 12: dtype=int64, shape=(), strides=c 
    input 13: dtype=int64, shape=(), strides=c 
    input 14: dtype=int64, shape=(), strides=c 
    input 15: dtype=int64, shape=(), strides=c 
    input 16: dtype=int64, shape=(), strides=c 
    input 17: dtype=float32, shape=(250, 500), strides=c 
    input 18: dtype=float32, shape=(250, 250), strides=c 
    input 19: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 20: dtype=float32, shape=(250, 250), strides=(1, 250) 
    input 21: dtype=float32, shape=(250, 500), strides=c 
    input 22: dtype=float32, shape=(250, 250), strides=c 
    input 23: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 24: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 3: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 4: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
    output 5: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 6: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 7: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
   5.6%    80.4%      12.054s       1.21e+00s     10   2124                     forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 4: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 5: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 6: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 7: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 9: dtype=float32, shape=(250, 500), strides=c 
    input 10: dtype=float32, shape=(250, 250), strides=c 
    input 11: dtype=float32, shape=(250, 500), strides=c 
    input 12: dtype=float32, shape=(250, 250), strides=c 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
   5.6%    86.1%      12.039s       1.20e+00s     10   1803                     forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 4: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 5: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 6: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 7: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 9: dtype=float32, shape=(250, 500), strides=c 
    input 10: dtype=float32, shape=(250, 250), strides=c 
    input 11: dtype=float32, shape=(250, 500), strides=c 
    input 12: dtype=float32, shape=(250, 250), strides=c 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
   5.6%    91.7%      11.941s       1.19e+00s     10   2307                     forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 4: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 5: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 6: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 7: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 9: dtype=float32, shape=(250, 500), strides=c 
    input 10: dtype=float32, shape=(250, 250), strides=c 
    input 11: dtype=float32, shape=(250, 500), strides=c 
    input 12: dtype=float32, shape=(250, 250), strides=c 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
   4.9%    96.6%      10.551s       1.06e+00s     10   2352                     forall_inplace,gpu,att_trans_do_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuAlloc{memset_0=True}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, Alloc.0, state_to_gates, W, W, state_to_state, W, GpuDimShuffle{0,x,x,1}.0, GpuSubtensor{::int64}.0, MakeVector.0, GpuElemwise{add,no_inplace}.0, MakeVector.0, GpuSubtensor{::int
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(183, 10, 500), strides=(5000, 500, 1) 
    input 2: dtype=float32, shape=(183, 10, 250), strides=(2500, 250, 1) 
    input 3: dtype=float32, shape=(183, 10, 1), strides=(10, 1, 0) 
    input 4: dtype=float32, shape=(184, 10, 250), strides=(2500, 250, 1) 
    input 5: dtype=float32, shape=(183, 10, 500), strides=(5000, 500, 1) 
    input 6: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    input 7: dtype=int64, shape=(184, 10), strides=c 
    input 8: dtype=float32, shape=(250, 500), strides=c 
    input 9: dtype=float32, shape=(250, 250), strides=c 
    input 10: dtype=float32, shape=(500, 500), strides=c 
    input 11: dtype=float32, shape=(250, 250), strides=c 
    input 12: dtype=float32, shape=(500, 250), strides=c 
    input 13: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
    input 14: dtype=float32, shape=(831, 10, 500), strides=(10000, 500, 1) 
    input 15: dtype=int64, shape=(1,), strides=c 
    input 16: dtype=float32, shape=(831, 10, 250), strides=(2500, 250, 1) 
    input 17: dtype=int64, shape=(1,), strides=c 
    input 18: dtype=float32, shape=(831, 10), strides=(20, 1) 
    input 19: dtype=int64, shape=(), strides=c 
    input 20: dtype=int64, shape=(), strides=c 
    input 21: dtype=float32, shape=(1, 250), strides=(0, 1) 
    input 22: dtype=float32, shape=(250, 1), strides=(1, 0) 
    output 0: dtype=float32, shape=(184, 10, 250), strides=(2500, 250, 1) 
    output 1: dtype=float32, shape=(183, 10, 500), strides=(5000, 500, 1) 
    output 2: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    output 3: dtype=int64, shape=(184, 10), strides=c 
   0.1%    96.7%       0.175s       1.75e-02s     10   2452                     GpuAdvancedIncSubtensor1{inplace,inc}(GpuAlloc{memset_0=True}.0, GpuReshape{1}.0, Elemwise{Composite{((i0 * i1) + i2)}}[(0, 1)].0)
    input 0: dtype=float32, shape=(58560,), strides=(1,) 
    input 1: dtype=float32, shape=(1830,), strides=(1,) 
    input 2: dtype=int64, shape=(1830,), strides=c 
    output 0: dtype=float32, shape=(58560,), strides=(1,) 
   0.0%    96.7%       0.101s       1.01e-02s     10   1442                     GpuFromHost(Alloc.0)
    input 0: dtype=float32, shape=(8310000,), strides=c 
    output 0: dtype=float32, shape=(8310000,), strides=(1,) 
   0.0%    96.8%       0.095s       9.52e-03s     10   1807                     GpuFromHost(Alloc.0)
    input 0: dtype=float32, shape=(8310000,), strides=c 
    output 0: dtype=float32, shape=(8310000,), strides=(1,) 
   0.0%    96.8%       0.082s       8.18e-03s     10   1977                     GpuDot22(GpuReshape{2}.0, GpuReshape{2}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    96.9%       0.081s       8.07e-03s     10   2173                     GpuDot22(GpuReshape{2}.0, GpuReshape{2}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    96.9%       0.081s       8.06e-03s     10   2175                     GpuDot22(GpuReshape{2}.0, GpuReshape{2}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    96.9%       0.081s       8.05e-03s     10   2802                     GpuDot22(GpuReshape{2}.0, GpuDimShuffle{1,0}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(1, 500) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    97.0%       0.080s       8.05e-03s     10   2682                     GpuDot22(GpuReshape{2}.0, GpuDimShuffle{1,0}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(1, 500) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    97.0%       0.080s       8.04e-03s     10   1979                     GpuDot22(GpuReshape{2}.0, GpuReshape{2}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    97.0%       0.080s       8.02e-03s     10   2671                     GpuDot22(GpuReshape{2}.0, GpuDimShuffle{1,0}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(1, 500) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    97.1%       0.080s       8.01e-03s     10   2672                     GpuDot22(GpuDimShuffle{1,0}.0, GpuReshape{2}.0)
    input 0: dtype=float32, shape=(500, 16620), strides=(1, 500) 
    input 1: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(500, 500), strides=(500, 1) 
   0.0%    97.1%       0.080s       8.01e-03s     10   2813                     GpuDot22(GpuReshape{2}.0, GpuDimShuffle{1,0}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(1, 500) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   ... (remaining 5344 Apply instances account for 2.88%(6.16s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 3036900KB (3036900KB)
    CPU: 162926KB (162926KB)
    GPU: 2873974KB (2873974KB)
---
    Max if linker=cvm(default): 910339KB (1331587KB)
    CPU: 32462KB (38479KB)
    GPU: 877877KB (1293108KB)
---
    Memory saved if views are used: 4291558KB (4291557KB)
    Memory saved if inplace ops are used: 1605717KB (1605717KB)
    Memory saved if gc is enabled: 2126561KB (1705312KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

     166220000B  [(1663, 10, 250), (1663, 10, 250), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10)] i i c c c c c c forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0)
     166220000B  [(1663, 10, 250), (1663, 10, 250), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10)] i i c c c c c c forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0)
     166220000B  [(1663, 10, 250), (1663, 10, 250), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10)] i i c c c c c c forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0)
      75445288B  [(184, 10, 250), (184, 10, 500), (184, 10, 831), (184, 10, 831), (184, 10), (2, 250, 250), (2, 1, 201), (2, 1, 250), (2, 250, 1), (2, 831, 10, 500), (2, 831, 10, 250), (183, 10, 250), (183, 10, 500), (183, 250, 10)] i i i i i i i i i i i c c c forall_inplace,gpu,grad_of_att_trans_do_apply_scan}(Shape_i{0}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, Subtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,2,1}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, GpuFromHost.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, state_to_gates, W, W, state_to_state, W, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{0,x,x,1}.0, GpuDimShuffle{1,0}.0, MakeVector.0, MakeVector.0, GpuSubtensor{::int64}.0, MakeVector.0, GpuSubtensor{::, ::, ::int64, ::int64}.0, GpuElemwise{add,no_inplace}.0, MakeVector.0, GpuSubtensor{::int64}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, max_attended_length, Elemwise{Composite{(((i0 + i1) - Switch(LT(i2, i1), i2, i1)) // i3)}}.0, GpuReshape{2}.0, GpuReshape{2}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0)
      33260000B  [(1663, 10, 250), (1663, 10, 250)] i i forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
      33260000B  [(1663, 10, 250), (1663, 10, 250)] i i forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
      33260000B  [(1663, 10, 250), (1663, 10, 250)] i i forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
      33240000B  [(1662, 10, 500)] v GpuReshape{3}(GpuDot22.0, MakeVector.0)
      33240000B  [(16620, 500)] c GpuDot22(GpuReshape{2}.0, GpuDimShuffle{1,0}.0)
      33240000B  [(1662, 10, 500)] v GpuSubtensor{::int64}(forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}.3, Constant{-1})
      33240000B  [(1662, 10, 500)] v GpuReshape{3}(GpuDot22.0, MakeVector.0)
      33240000B  [(1662, 10, 500)] c GpuJoin(TensorConstant{2}, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int64}.0)
      33240000B  [(1662, 10, 500)] v GpuReshape{3}(GpuDot22.0, MakeVector.0)
      33240000B  [(1662, 10, 500)] i GpuElemwise{Composite{((i0 + i1) + (i2 + i3))}}[(0, 0)](GpuSubtensor{:int64:}.0, GpuSubtensor{:int64:}.0, GpuSubtensor{:int64:}.0, GpuSubtensor{:int64:}.0)
      33240000B  [(1662, 10, 500)] i GpuIncSubtensor{InplaceInc;::int64}(GpuAlloc{memset_0=True}.0, GpuElemwise{Composite{((i0 + i1) + (i2 + i3))}}[(0, 0)].0, Constant{1})
      33240000B  [(16620, 500)] c GpuDot22(GpuReshape{2}.0, GpuReshape{2}.0)
      33240000B  [(1662, 10, 500)] c GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[[ 0.]]]}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0)
      33240000B  [(2, 831, 10, 500)] c GpuAlloc{memset_0=True}(CudaNdarrayConstant{0.0}, Elemwise{Composite{(Switch(LT(maximum(i0, i1), i2), Switch(LT((maximum(i0, i1) + i1 + i3), i2), i2, (maximum(i0, i1) + i1 + i3)), Switch(LT(maximum(i0, i1), i4), maximum(i0, i1), i4)) - i2)}}.0, max_attended_length, Shape_i{0}.0, Elemwise{add,no_inplace}.0)
      33240000B  [(1662, 10, 500)] v GpuSubtensor{int64:int64:int64}(GpuElemwise{add,no_inplace}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})
      33240000B  [(1662, 10, 500)] v GpuReshape{3}(GpuDot22.0, MakeVector.0)
   ... (remaining 5344 Apply account for 8042589641B/9148594929B ((87.91%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( gatedrecurrent_apply_scan&gatedrecurrent_apply_scan )
==================
  Message: None
  Time in 10 calls of the op (for a total of 12649 steps) 1.193918e+01s

  Total time spent in calling the VM 1.183959e+01s (99.166%)
  Total overhead (computing slices..) 9.959435e-02s (0.834%)

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  71.1%    71.1%       5.619s       1.11e-04s     C    50596       4   theano.sandbox.cuda.blas.GpuGemm
  26.3%    97.4%       2.077s       2.05e-05s     C   101192       8   theano.sandbox.cuda.basic_ops.GpuElemwise
   2.6%   100.0%       0.208s       4.12e-06s     C    50596       4   theano.sandbox.cuda.basic_ops.GpuSubtensor
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  71.1%    71.1%       5.619s       1.11e-04s     C     50596        4   GpuGemm{no_inplace}
   7.3%    78.4%       0.579s       2.29e-05s     C     25298        2   GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}
   6.3%    84.7%       0.500s       1.98e-05s     C     25298        2   GpuElemwise{ScalarSigmoid}[(0, 0)]
   6.3%    91.1%       0.500s       1.98e-05s     C     25298        2   GpuElemwise{sub,no_inplace}
   6.3%    97.4%       0.498s       1.97e-05s     C     25298        2   GpuElemwise{mul,no_inplace}
   1.4%    98.8%       0.112s       4.41e-06s     C     25298        2   GpuSubtensor{::, :int64:}
   1.2%   100.0%       0.097s       3.83e-06s     C     25298        2   GpuSubtensor{::, int64::}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  18.2%    18.2%       1.438s       1.14e-04s   12649     1                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
  18.2%    36.4%       1.437s       1.14e-04s   12649     3                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
  17.5%    53.9%       1.382s       1.09e-04s   12649    12                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace1[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
  17.2%    71.1%       1.362s       1.08e-04s   12649    13                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.7%    74.8%       0.291s       2.30e-05s   12649    14                     GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]1[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
    input 0: dtype=float32, shape=(10, 1), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(1, 1), strides=c 
    input 5: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.6%    78.4%       0.288s       2.28e-05s   12649    15                     GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]0[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
    input 0: dtype=float32, shape=(10, 1), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(1, 1), strides=c 
    input 5: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.3%    81.7%       0.261s       2.06e-05s   12649     0                     GpuElemwise{sub,no_inplace}(CudaNdarrayConstant{[[ 1.]]}, <CudaNdarrayType(float32, col)>)
    input 0: dtype=float32, shape=(1, 1), strides=c 
    input 1: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 1), strides=c 
   3.2%    84.9%       0.253s       2.00e-05s   12649     5                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   3.2%    88.1%       0.249s       1.97e-05s   12649    11                     GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]0[cuda], GpuSubtensor{::, int64::}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.1%    91.2%       0.248s       1.96e-05s   12649    10                     GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]1[cuda], GpuSubtensor{::, int64::}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.1%    94.3%       0.247s       1.95e-05s   12649     4                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   3.0%    97.4%       0.239s       1.89e-05s   12649     2                     GpuElemwise{sub,no_inplace}(CudaNdarrayConstant{[[ 1.]]}, <CudaNdarrayType(float32, col)>)
    input 0: dtype=float32, shape=(1, 1), strides=c 
    input 1: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 1), strides=c 
   0.7%    98.1%       0.056s       4.42e-06s   12649     6                     GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   0.7%    98.8%       0.056s       4.40e-06s   12649     8                     GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   0.6%    99.4%       0.050s       3.96e-06s   12649     7                     GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   0.6%   100.0%       0.047s       3.70e-06s   12649     9                     GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 98KB (98KB)
    CPU: 0KB (0KB)
    GPU: 98KB (98KB)
---
    Max if linker=cvm(default): 49KB (68KB)
    CPU: 0KB (0KB)
    GPU: 49KB (68KB)
---
    Memory saved if views are used: 39KB (39KB)
    Memory saved if inplace ops are used: 39KB (39KB)
    Memory saved if gc is enabled: 48KB (29KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]0[cuda], GpuSubtensor{::, int64::}.0)
         10000B  [(10, 250)] c GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace1[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy1[cuda], TensorConstant{1.0})
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]1[cuda], GpuSubtensor{::, int64::}.0)
         10000B  [(10, 250)] v GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] v GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] v GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] c GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]1[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
         10000B  [(10, 250)] c GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
         10000B  [(10, 250)] c GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]0[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
         10000B  [(10, 250)] v GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
   ... (remaining 2 Apply account for   80B/180080B ((0.04%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( gatedrecurrent_apply_scan&gatedrecurrent_apply_scan )
==================
  Message: None
  Time in 10 calls of the op (for a total of 12649 steps) 1.201616e+01s

  Total time spent in calling the VM 1.191935e+01s (99.194%)
  Total overhead (computing slices..) 9.681368e-02s (0.806%)

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  71.0%    71.0%       5.599s       1.11e-04s     C    50596       4   theano.sandbox.cuda.blas.GpuGemm
  26.3%    97.3%       2.072s       2.05e-05s     C   101192       8   theano.sandbox.cuda.basic_ops.GpuElemwise
   2.7%   100.0%       0.210s       4.16e-06s     C    50596       4   theano.sandbox.cuda.basic_ops.GpuSubtensor
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  71.0%    71.0%       5.599s       1.11e-04s     C     50596        4   GpuGemm{no_inplace}
   7.3%    78.4%       0.576s       2.28e-05s     C     25298        2   GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}
   6.4%    84.7%       0.504s       1.99e-05s     C     25298        2   GpuElemwise{sub,no_inplace}
   6.3%    91.1%       0.497s       1.96e-05s     C     25298        2   GpuElemwise{mul,no_inplace}
   6.3%    97.3%       0.495s       1.96e-05s     C     25298        2   GpuElemwise{ScalarSigmoid}[(0, 0)]
   1.4%    98.8%       0.113s       4.46e-06s     C     25298        2   GpuSubtensor{::, :int64:}
   1.2%   100.0%       0.098s       3.86e-06s     C     25298        2   GpuSubtensor{::, int64::}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  18.2%    18.2%       1.438s       1.14e-04s   12649     1                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
  18.2%    36.4%       1.433s       1.13e-04s   12649     3                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
  17.3%    53.8%       1.366s       1.08e-04s   12649    12                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace1[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
  17.3%    71.0%       1.362s       1.08e-04s   12649    13                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.7%    74.7%       0.288s       2.28e-05s   12649    14                     GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]1[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
    input 0: dtype=float32, shape=(10, 1), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(1, 1), strides=c 
    input 5: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.7%    78.4%       0.288s       2.28e-05s   12649    15                     GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]0[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
    input 0: dtype=float32, shape=(10, 1), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(1, 1), strides=c 
    input 5: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.3%    81.7%       0.262s       2.07e-05s   12649     0                     GpuElemwise{sub,no_inplace}(CudaNdarrayConstant{[[ 1.]]}, <CudaNdarrayType(float32, col)>)
    input 0: dtype=float32, shape=(1, 1), strides=c 
    input 1: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 1), strides=c 
   3.2%    84.8%       0.249s       1.97e-05s   12649    10                     GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]1[cuda], GpuSubtensor{::, int64::}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.1%    88.0%       0.248s       1.96e-05s   12649     4                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   3.1%    91.1%       0.248s       1.96e-05s   12649    11                     GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]0[cuda], GpuSubtensor{::, int64::}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.1%    94.3%       0.247s       1.95e-05s   12649     5                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   3.1%    97.3%       0.242s       1.91e-05s   12649     2                     GpuElemwise{sub,no_inplace}(CudaNdarrayConstant{[[ 1.]]}, <CudaNdarrayType(float32, col)>)
    input 0: dtype=float32, shape=(1, 1), strides=c 
    input 1: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 1), strides=c 
   0.7%    98.1%       0.057s       4.50e-06s   12649     6                     GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   0.7%    98.8%       0.056s       4.42e-06s   12649     8                     GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   0.6%    99.4%       0.050s       3.98e-06s   12649     7                     GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   0.6%   100.0%       0.047s       3.73e-06s   12649     9                     GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 98KB (98KB)
    CPU: 0KB (0KB)
    GPU: 98KB (98KB)
---
    Max if linker=cvm(default): 49KB (68KB)
    CPU: 0KB (0KB)
    GPU: 49KB (68KB)
---
    Memory saved if views are used: 39KB (39KB)
    Memory saved if inplace ops are used: 39KB (39KB)
    Memory saved if gc is enabled: 48KB (29KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
         10000B  [(10, 250)] v GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] v GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] c GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]1[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
         10000B  [(10, 250)] v GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]1[cuda], GpuSubtensor{::, int64::}.0)
         10000B  [(10, 250)] c GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]0[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
         10000B  [(10, 250)] c GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace1[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy1[cuda], TensorConstant{1.0})
         10000B  [(10, 250)] v GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] c GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]0[cuda], GpuSubtensor{::, int64::}.0)
   ... (remaining 2 Apply account for   80B/180080B ((0.04%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( gatedrecurrent_apply_scan&gatedrecurrent_apply_scan )
==================
  Message: None
  Time in 10 calls of the op (for a total of 12654 steps) 1.190756e+01s

  Total time spent in calling the VM 1.181238e+01s (99.201%)
  Total overhead (computing slices..) 9.517932e-02s (0.799%)

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  71.1%    71.1%       5.600s       1.11e-04s     C    50616       4   theano.sandbox.cuda.blas.GpuGemm
  26.2%    97.4%       2.066s       2.04e-05s     C   101232       8   theano.sandbox.cuda.basic_ops.GpuElemwise
   2.6%   100.0%       0.207s       4.08e-06s     C    50616       4   theano.sandbox.cuda.basic_ops.GpuSubtensor
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  71.1%    71.1%       5.600s       1.11e-04s     C     50616        4   GpuGemm{no_inplace}
   7.3%    78.5%       0.577s       2.28e-05s     C     25308        2   GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}
   6.4%    84.8%       0.500s       1.98e-05s     C     25308        2   GpuElemwise{mul,no_inplace}
   6.3%    91.1%       0.494s       1.95e-05s     C     25308        2   GpuElemwise{sub,no_inplace}
   6.3%    97.4%       0.494s       1.95e-05s     C     25308        2   GpuElemwise{ScalarSigmoid}[(0, 0)]
   1.4%    98.8%       0.110s       4.37e-06s     C     25308        2   GpuSubtensor{::, :int64:}
   1.2%   100.0%       0.096s       3.80e-06s     C     25308        2   GpuSubtensor{::, int64::}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  18.3%    18.3%       1.439s       1.14e-04s   12654     1                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
  18.2%    36.4%       1.431s       1.13e-04s   12654     3                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
  17.4%    53.8%       1.366s       1.08e-04s   12654    12                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace1[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
  17.3%    71.1%       1.364s       1.08e-04s   12654    13                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.7%    74.8%       0.289s       2.28e-05s   12654    14                     GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]1[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
    input 0: dtype=float32, shape=(10, 1), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(1, 1), strides=c 
    input 5: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.7%    78.5%       0.289s       2.28e-05s   12654    15                     GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]0[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
    input 0: dtype=float32, shape=(10, 1), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(1, 1), strides=c 
    input 5: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.3%    81.8%       0.258s       2.04e-05s   12654     0                     GpuElemwise{sub,no_inplace}(CudaNdarrayConstant{[[ 1.]]}, <CudaNdarrayType(float32, col)>)
    input 0: dtype=float32, shape=(1, 1), strides=c 
    input 1: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 1), strides=c 
   3.2%    84.9%       0.250s       1.98e-05s   12654    10                     GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]1[cuda], GpuSubtensor{::, int64::}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.2%    88.1%       0.250s       1.97e-05s   12654    11                     GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]0[cuda], GpuSubtensor{::, int64::}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   3.1%    91.2%       0.247s       1.96e-05s   12654     4                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   3.1%    94.4%       0.247s       1.95e-05s   12654     5                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   3.0%    97.4%       0.236s       1.86e-05s   12654     2                     GpuElemwise{sub,no_inplace}(CudaNdarrayConstant{[[ 1.]]}, <CudaNdarrayType(float32, col)>)
    input 0: dtype=float32, shape=(1, 1), strides=c 
    input 1: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 1), strides=c 
   0.7%    98.1%       0.056s       4.40e-06s   12654     6                     GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   0.7%    98.8%       0.055s       4.34e-06s   12654     8                     GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   0.6%    99.4%       0.048s       3.83e-06s   12654     7                     GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   0.6%   100.0%       0.048s       3.78e-06s   12654     9                     GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 98KB (98KB)
    CPU: 0KB (0KB)
    GPU: 98KB (98KB)
---
    Max if linker=cvm(default): 49KB (68KB)
    CPU: 0KB (0KB)
    GPU: 49KB (68KB)
---
    Memory saved if views are used: 39KB (39KB)
    Memory saved if inplace ops are used: 39KB (39KB)
    Memory saved if gc is enabled: 48KB (29KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_initial_states_states[t-1]1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         10000B  [(10, 250)] v GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] c GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]0[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
         10000B  [(10, 250)] v GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] c GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace1[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy1[cuda], TensorConstant{1.0})
         10000B  [(10, 250)] v GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] c GpuElemwise{Composite{((i0 * ((tanh(i1) * i2) + (i3 * (i4 - i2)))) + (i5 * i3))},no_inplace}(<CudaNdarrayType(float32, col)>, GpuGemm{no_inplace}.0, GpuSubtensor{::, :int64:}.0, gatedrecurrent_initial_states_states[t-1]1[cuda], CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{sub,no_inplace}.0)
         10000B  [(10, 250)] c GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]0[cuda], GpuSubtensor{::, int64::}.0)
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(gatedrecurrent_initial_states_states[t-1]1[cuda], GpuSubtensor{::, int64::}.0)
         10000B  [(10, 250)] v GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
   ... (remaining 2 Apply account for   80B/180080B ((0.04%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( att_trans_do_apply_scan )
==================
  Message: None
  Time in 10 calls of the op (for a total of 1672 steps) 1.053332e+01s

  Total time spent in calling the VM 1.024462e+01s (97.259%)
  Total overhead (computing slices..) 2.887046e-01s (2.741%)

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  26.8%    26.8%       2.089s       1.14e-04s     C    18392      11   theano.sandbox.cuda.basic_ops.GpuElemwise
  16.8%    43.7%       1.311s       1.57e-04s     C     8360       5   theano.sandbox.cuda.blas.GpuDot22
  15.0%    58.7%       1.169s       6.99e-04s     C     1672       1   theano.sandbox.cuda.blas.GpuCorrMM_gradInputs
  13.5%    72.2%       1.050s       1.26e-04s     Py    8360       5   theano.sandbox.cuda.basic_ops.GpuReshape
   9.0%    81.2%       0.702s       2.10e-04s     C     3344       2   theano.sandbox.cuda.blas.GpuGemm
   5.8%    87.0%       0.451s       2.45e-05s     C    18392      11   theano.sandbox.cuda.basic_ops.GpuDimShuffle
   5.2%    92.2%       0.405s       8.08e-05s     C     5016       3   theano.sandbox.cuda.basic_ops.GpuCAReduce
   1.6%    93.8%       0.125s       3.74e-05s     C     3344       2   theano.sandbox.cuda.basic_ops.GpuContiguous
   1.3%    95.2%       0.103s       6.13e-05s     C     1672       1   theano.sandbox.cuda.basic_ops.GpuFromHost
   1.0%    96.2%       0.081s       3.73e-06s     C    21736      13   theano.tensor.elemwise.Elemwise
   0.7%    96.9%       0.054s       4.65e-06s     C    11704       7   theano.sandbox.cuda.basic_ops.GpuSubtensor
   0.7%    97.6%       0.053s       3.19e-05s     C     1672       1   theano.sandbox.cuda.basic_ops.HostFromGpu
   0.6%    98.2%       0.047s       2.82e-06s     C    16720      10   theano.compile.ops.Shape_i
   0.5%    98.7%       0.037s       2.24e-05s     C     1672       1   theano.sandbox.cuda.basic_ops.GpuAlloc
   0.5%    99.1%       0.036s       2.14e-05s     C     1672       1   theano.sandbox.cuda.basic_ops.GpuIncSubtensor
   0.3%    99.4%       0.024s       2.84e-06s     C     8360       5   theano.tensor.opt.MakeVector
   0.2%    99.6%       0.013s       7.82e-06s     C     1672       1   theano.tensor.basic.Join
   0.1%    99.7%       0.010s       3.02e-06s     C     3344       2   theano.tensor.basic.ScalarFromTensor
   0.1%    99.8%       0.008s       4.58e-06s     C     1672       1   theano.tensor.elemwise.All
   0.1%    99.9%       0.008s       4.50e-06s     C     1672       1   theano.tensor.subtensor.Subtensor
   ... (remaining 1 Classes account for   0.08%(0.01s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  16.8%    16.8%       1.311s       1.57e-04s     C     8360        5   GpuDot22
  15.0%    31.9%       1.169s       6.99e-04s     C     1672        1   GpuCorrMM_gradInputs{valid, (1, 1)}
  13.3%    45.2%       1.039s       3.11e-04s     C     3344        2   GpuElemwise{mul,no_inplace}
  12.6%    57.8%       0.977s       1.95e-04s     Py    5016        3   GpuReshape{2}
   9.0%    66.8%       0.702s       2.10e-04s     C     3344        2   GpuGemm{inplace}
   6.4%    73.2%       0.502s       3.00e-04s     C     1672        1   GpuElemwise{Tanh}[(0, 0)]
   4.3%    77.6%       0.338s       2.02e-04s     C     1672        1   GpuDimShuffle{1,0,2}
   2.6%    80.2%       0.201s       1.20e-04s     C     1672        1   GpuElemwise{Composite{((i0 + i1) + i2)}}[(0, 2)]
   2.3%    82.4%       0.178s       1.06e-04s     C     1672        1   GpuCAReduce{add}{1,0,0}
   2.0%    84.4%       0.153s       9.18e-05s     C     1672        1   GpuCAReduce{maximum}{1,0}
   1.9%    86.3%       0.145s       4.32e-05s     C     3344        2   GpuElemwise{sub,no_inplace}
   1.6%    87.9%       0.125s       3.74e-05s     C     3344        2   GpuContiguous
   1.3%    89.2%       0.103s       6.13e-05s     C     1672        1   GpuFromHost
   0.9%    90.1%       0.074s       4.41e-05s     C     1672        1   GpuCAReduce{add}{1,0}
   0.9%    91.1%       0.073s       2.17e-05s     Py    3344        2   GpuReshape{3}
   0.7%    91.8%       0.053s       3.19e-05s     C     1672        1   HostFromGpu
   0.6%    92.4%       0.048s       2.89e-05s     C     1672        1   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 1)]
   0.6%    93.0%       0.046s       2.73e-05s     C     1672        1   GpuElemwise{Composite{((i0 * ((tanh((i1 + i2)) * i3) + (i4 * (i5 - i3)))) + (i6 * i4))},no_inplace}
   0.5%    93.5%       0.039s       2.32e-05s     C     1672        1   GpuElemwise{TrueDiv}[(0, 0)]
   0.5%    94.0%       0.037s       2.24e-05s     C     1672        1   GpuAlloc{memset_0=True}
   ... (remaining 38 Ops account for   6.05%(0.47s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  15.0%    15.0%       1.169s       6.99e-04s   1672    37     44.8        0.0 GpuCorrMM_gradInputs{valid, (1, 1)}(GpuContiguous.0, GpuContiguous.0)
    input 0: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
    input 1: dtype=float32, shape=(1, 1, 10, 395), strides=(0, 0, 395, 1) 
    output 0: dtype=float32, shape=(1, 1, 10, 595), strides=(0, 0, 595, 1) 
  12.8%    27.8%       0.998s       5.97e-04s   1672    75                     GpuElemwise{mul,no_inplace}(GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:}.0)
    input 0: dtype=float32, shape=(395, 10, 1), strides=(10, 1, 0) 
    input 1: dtype=float32, shape=(395, 10, 500), strides=(10000, 500, 1) 
    output 0: dtype=float32, shape=(395, 10, 500), strides=(5000, 500, 1) 
   9.3%    37.2%       0.725s       4.33e-04s   1672    61                     GpuReshape{2}(GpuElemwise{Composite{((i0 + i1) + i2)}}[(0, 2)].0, MakeVector.0)
    input 0: dtype=float32, shape=(395, 10, 250), strides=(250, 98750, 1) 
    input 1: dtype=int64, shape=(2,), strides=c 
    output 0: dtype=float32, shape=(3950, 250), strides=(250, 1) 
   6.4%    43.6%       0.502s       3.00e-04s   1672    62                     GpuElemwise{Tanh}[(0, 0)](GpuReshape{2}.0)
    input 0: dtype=float32, shape=(3950, 250), strides=(250, 1) 
    output 0: dtype=float32, shape=(3950, 250), strides=(250, 1) 
   5.9%    49.5%       0.463s       2.77e-04s   1672    57                     GpuDot22(GpuReshape{2}.0, <CudaNdarrayType(float32, matrix)>)
    input 0: dtype=float32, shape=(3950, 1), strides=(1, 0) 
    input 1: dtype=float32, shape=(1, 250), strides=c 
    output 0: dtype=float32, shape=(3950, 250), strides=(250, 1) 
   5.3%    54.8%       0.412s       2.46e-04s   1672    77                     GpuGemm{inplace}(GpuDot22.0, TensorConstant{1.0}, GpuCAReduce{add}{1,0,0}.0, W_copy[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 500), strides=(500, 1) 
    input 3: dtype=float32, shape=(500, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
   4.3%    59.2%       0.338s       2.02e-04s   1672    59                     GpuDimShuffle{1,0,2}(GpuReshape{3}.0)
    input 0: dtype=float32, shape=(10, 395, 250), strides=(98750, 250, 1) 
    output 0: dtype=float32, shape=(395, 10, 250), strides=(250, 98750, 1) 
   4.2%    63.3%       0.325s       1.94e-04s   1672    63                     GpuDot22(GpuElemwise{Tanh}[(0, 0)].0, <CudaNdarrayType(float32, matrix)>)
    input 0: dtype=float32, shape=(3950, 250), strides=(250, 1) 
    input 1: dtype=float32, shape=(250, 1), strides=c 
    output 0: dtype=float32, shape=(3950, 1), strides=(1, 0) 
   3.7%    67.1%       0.290s       1.74e-04s   1672    83                     GpuGemm{inplace}(GpuDot22.0, TensorConstant{1.0}, GpuCAReduce{add}{1,0,0}.0, W_copy[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 500), strides=(500, 1) 
    input 3: dtype=float32, shape=(500, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   2.8%    69.9%       0.218s       1.31e-04s   1672    56                     GpuReshape{2}(GpuDimShuffle{0,2,1}.0, MakeVector.0)
    input 0: dtype=float32, shape=(10, 395, 1), strides=(595, 1, 0) 
    input 1: dtype=int64, shape=(2,), strides=c 
    output 0: dtype=float32, shape=(3950, 1), strides=(1, 0) 
   2.6%    72.5%       0.201s       1.20e-04s   1672    60                     GpuElemwise{Composite{((i0 + i1) + i2)}}[(0, 2)](GpuDimShuffle{0,1,2}.0, GpuDimShuffle{x,0,1}.0, GpuDimShuffle{1,0,2}.0)
    input 0: dtype=float32, shape=(395, 10, 250), strides=(2500, 250, 1) 
    input 1: dtype=float32, shape=(1, 10, 250), strides=(0, 250, 1) 
    input 2: dtype=float32, shape=(395, 10, 250), strides=(250, 98750, 1) 
    output 0: dtype=float32, shape=(395, 10, 250), strides=(250, 98750, 1) 
   2.3%    74.8%       0.180s       1.08e-04s   1672    10                     GpuDot22(att_trans_initial_states_states[t-1][cuda], W_copy[cuda])
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(250, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   2.3%    77.1%       0.178s       1.06e-04s   1672    76                     GpuCAReduce{add}{1,0,0}(GpuElemwise{mul,no_inplace}.0)
    input 0: dtype=float32, shape=(395, 10, 500), strides=(5000, 500, 1) 
    output 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
   2.2%    79.3%       0.174s       1.04e-04s   1672    12                     GpuDot22(att_trans_initial_states_states[t-1][cuda], state_to_gates_copy[cuda])
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(250, 500), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
   2.2%    81.5%       0.170s       1.01e-04s   1672    82                     GpuDot22(GpuElemwise{mul,no_inplace}.0, state_to_state_copy[cuda])
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 1: dtype=float32, shape=(250, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   2.0%    83.5%       0.153s       9.18e-05s   1672    65                     GpuCAReduce{maximum}{1,0}(GpuReshape{2}.0)
    input 0: dtype=float32, shape=(395, 10), strides=(10, 1) 
    output 0: dtype=float32, shape=(10,), strides=(1,) 
   1.6%    85.0%       0.121s       7.22e-05s   1672    34                     GpuContiguous(GpuDimShuffle{x,x,0,1}.0)
    input 0: dtype=float32, shape=(1, 1, 10, 395), strides=(0, 0, 831, 1) 
    output 0: dtype=float32, shape=(1, 1, 10, 395), strides=(0, 0, 395, 1) 
   1.4%    86.4%       0.109s       6.55e-05s   1672    31                     GpuElemwise{sub,no_inplace}(CudaNdarrayConstant{[[ 1.]]}, GpuSubtensor{int64:int64:}.0)
    input 0: dtype=float32, shape=(1, 1), strides=c 
    input 1: dtype=float32, shape=(395, 10), strides=(20, 1) 
    output 0: dtype=float32, shape=(395, 10), strides=(10, 1) 
   1.3%    87.7%       0.103s       6.13e-05s   1672    51                     GpuFromHost(Elemwise{Cast{float32}}.0)
    input 0: dtype=float32, shape=(1, 10), strides=c 
    output 0: dtype=float32, shape=(1, 10), strides=(0, 1) 
   0.9%    88.7%       0.074s       4.41e-05s   1672    68                     GpuCAReduce{add}{1,0}(GpuElemwise{Composite{(exp((i0 - i1)) * i2)}}[(0, 0)].0)
    input 0: dtype=float32, shape=(395, 10), strides=(10, 1) 
    output 0: dtype=float32, shape=(10,), strides=(1,) 
   ... (remaining 65 Apply instances account for 11.33%(0.88s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 11757KB (11757KB)
    CPU: 20KB (20KB)
    GPU: 11737KB (11737KB)
---
    Max if linker=cvm(default): 7786KB (7806KB)
    CPU: 19KB (19KB)
    GPU: 7767KB (7786KB)
---
    Memory saved if views are used: 27251KB (27251KB)
    Memory saved if inplace ops are used: 7827KB (7827KB)
    Memory saved if gc is enabled: 3970KB (3950KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

       7900000B  [(395, 10, 500)] v GpuSubtensor{int64:int64:}(conv_att_take_glimpses_attended_replace[cuda], ScalarFromTensor.0, ScalarFromTensor.0)
       7900000B  [(395, 10, 500)] c GpuElemwise{mul,no_inplace}(GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:}.0)
       3950000B  [(3950, 250)] v GpuReshape{2}(GpuElemwise{Composite{((i0 + i1) + i2)}}[(0, 2)].0, MakeVector.0)
       3950000B  [(395, 10, 250)] v GpuDimShuffle{0,1,2}(GpuSubtensor{int64:int64:}.0)
       3950000B  [(395, 10, 250)] v GpuDimShuffle{1,0,2}(GpuReshape{3}.0)
       3950000B  [(3950, 250)] c GpuDot22(GpuReshape{2}.0, <CudaNdarrayType(float32, matrix)>)
       3950000B  [(10, 395, 250)] v GpuReshape{3}(GpuDot22.0, Join.0)
       3950000B  [(395, 10, 250)] i GpuElemwise{Composite{((i0 + i1) + i2)}}[(0, 2)](GpuDimShuffle{0,1,2}.0, GpuDimShuffle{x,0,1}.0, GpuDimShuffle{1,0,2}.0)
       3950000B  [(3950, 250)] i GpuElemwise{Tanh}[(0, 0)](GpuReshape{2}.0)
       3950000B  [(395, 10, 250)] v GpuSubtensor{int64:int64:}(conv_att_take_glimpses_preprocessed_attended_replace[cuda], ScalarFromTensor.0, ScalarFromTensor.0)
         33240B  [(831, 10)] c GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[ 0.]]}, Shape_i{1}.0, Shape_i{0}.0)
         33240B  [(10, 831)] v GpuDimShuffle{1,0}(GpuIncSubtensor{InplaceSet;int64:int64:}.0)
         33240B  [(831, 10)] i GpuIncSubtensor{InplaceSet;int64:int64:}(GpuAlloc{memset_0=True}.0, GpuElemwise{TrueDiv}[(0, 0)].0, ScalarFromTensor.0, ScalarFromTensor.0)
         23800B  [(10, 1, 595)] v GpuReshape{3}(GpuDimShuffle{2,1,3,0}.0, MakeVector.0)
         23800B  [(1, 1, 10, 595)] c GpuCorrMM_gradInputs{valid, (1, 1)}(GpuContiguous.0, GpuContiguous.0)
         23800B  [(10, 1, 595, 1)] v GpuDimShuffle{2,1,3,0}(GpuCorrMM_gradInputs{valid, (1, 1)}.0)
         20000B  [(10, 500)] c GpuCAReduce{add}{1,0,0}(GpuElemwise{mul,no_inplace}.0)
         20000B  [(10, 500)] i GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 1)](distribute_apply_gate_inputs_replace[cuda], GpuGemm{inplace}.0)
         20000B  [(10, 500)] c GpuDot22(att_trans_initial_states_states[t-1][cuda], state_to_gates_copy[cuda])
         20000B  [(10, 500)] i GpuGemm{inplace}(GpuDot22.0, TensorConstant{1.0}, GpuCAReduce{add}{1,0,0}.0, W_copy[cuda], TensorConstant{1.0})
   ... (remaining 65 Apply account for 307460B/47958580B ((0.64%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( grad_of_att_trans_do_apply_scan )
==================
  Message: None
  Time in 10 calls of the op (for a total of 1672 steps) 6.205167e+01s

  Total time spent in calling the VM 5.955017e+01s (95.969%)
  Total overhead (computing slices..) 2.501498e+00s (4.031%)

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  26.1%    26.1%      12.823s       9.59e-04s     C    13376       8   theano.sandbox.cuda.blas.GpuCorrMM
  18.1%    44.2%       8.905s       1.04e-04s     C    85272      51   theano.sandbox.cuda.basic_ops.GpuElemwise
  17.1%    61.2%       8.408s       1.86e-04s     C    45144      27   theano.sandbox.cuda.blas.GpuDot22
  14.8%    76.1%       7.287s       1.32e-04s     Py   55176      33   theano.sandbox.cuda.basic_ops.GpuReshape
   6.7%    82.8%       3.309s       1.24e-04s     C    26752      16   theano.sandbox.cuda.basic_ops.GpuIncSubtensor
   6.5%    89.3%       3.195s       3.13e-05s     C   101992      61   theano.sandbox.cuda.basic_ops.GpuDimShuffle
   3.3%    92.5%       1.611s       6.88e-05s     C    23408      14   theano.sandbox.cuda.basic_ops.GpuCAReduce
   2.6%    95.2%       1.301s       7.78e-04s     C     1672       1   theano.sandbox.cuda.blas.GpuCorrMM_gradInputs
   2.4%    97.5%       1.160s       8.67e-05s     C    13376       8   theano.sandbox.cuda.blas.GpuGemm
   0.7%    98.2%       0.332s       1.66e-05s     C    20064      12   theano.sandbox.cuda.basic_ops.GpuContiguous
   0.4%    98.6%       0.201s       6.02e-05s     C     3344       2   theano.sandbox.cuda.basic_ops.GpuFromHost
   0.3%    98.9%       0.144s       7.19e-06s     C    20064      12   theano.sandbox.cuda.basic_ops.GpuSubtensor
   0.3%    99.2%       0.139s       4.16e-05s     C     3344       2   theano.sandbox.cuda.basic_ops.GpuAlloc
   0.2%    99.4%       0.093s       4.26e-06s     C    21736      13   theano.tensor.elemwise.Elemwise
   0.2%    99.6%       0.088s       3.28e-06s     C    26752      16   theano.compile.ops.Shape_i
   0.1%    99.7%       0.064s       3.83e-05s     C     1672       1   theano.sandbox.cuda.basic_ops.HostFromGpu
   0.1%    99.8%       0.055s       3.32e-06s     C    16720      10   theano.tensor.opt.MakeVector
   0.1%    99.9%       0.032s       4.74e-06s     Py    6688       4   theano.compile.ops.Rebroadcast
   0.0%    99.9%       0.015s       4.58e-06s     C     3344       2   theano.tensor.elemwise.DimShuffle
   0.0%    99.9%       0.015s       8.96e-06s     C     1672       1   theano.tensor.basic.Join
   ... (remaining 3 Classes account for   0.06%(0.03s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  26.1%    26.1%      12.823s       9.59e-04s     C     13376        8   GpuCorrMM{valid, (1, 1)}
  17.1%    43.1%       8.408s       1.86e-04s     C     45144       27   GpuDot22
  13.8%    56.9%       6.769s       2.13e-04s     Py    31768       19   GpuReshape{2}
   9.3%    66.2%       4.592s       3.05e-04s     C     15048        9   GpuElemwise{mul,no_inplace}
   3.9%    70.1%       1.906s       5.70e-04s     C     3344        2   GpuIncSubtensor{Inc;int64:int64:}
   3.1%    73.3%       1.549s       1.85e-04s     C     8360        5   GpuDimShuffle{1,0,2}
   2.6%    75.9%       1.301s       7.78e-04s     C     1672        1   GpuCorrMM_gradInputs{valid, (1, 1)}
   2.4%    78.3%       1.160s       8.67e-05s     C     13376        8   GpuGemm{inplace}
   1.6%    79.8%       0.770s       9.21e-05s     C     8360        5   GpuElemwise{Mul}[(0, 0)]
   1.4%    81.2%       0.669s       1.00e-04s     C     6688        4   GpuIncSubtensor{InplaceInc;int64:int64:}
   1.3%    82.5%       0.659s       3.94e-04s     C     1672        1   GpuElemwise{Composite{(i0 + (i1 + i1))},no_inplace}
   1.1%    83.6%       0.550s       1.64e-04s     C     3344        2   GpuCAReduce{add}{0,0,1}
   1.0%    84.7%       0.507s       4.34e-05s     C     11704        7   GpuCAReduce{add}{1,0}
   1.0%    85.7%       0.487s       5.82e-05s     C     8360        5   GpuDimShuffle{1,0}
   0.9%    86.6%       0.456s       5.45e-05s     C     8360        5   GpuElemwise{add,no_inplace}
   0.8%    87.4%       0.414s       4.95e-05s     C     8360        5   GpuDimShuffle{0,1,2}
   0.8%    88.2%       0.396s       5.93e-05s     C     6688        4   GpuCAReduce{add}{1,0,0}
   0.8%    89.0%       0.391s       1.17e-04s     C     3344        2   GpuElemwise{Composite{(i0 + (i1 + (i1 + (i1 + i1))))},no_inplace}
   0.8%    89.8%       0.383s       7.64e-05s     C     5016        3   GpuElemwise{Composite{((i0 * i1 * i2 * i3) / i4)},no_inplace}
   0.8%    90.6%       0.381s       7.60e-05s     C     5016        3   GpuIncSubtensor{Inc;::, ::, int64:int64:}
   ... (remaining 72 Ops account for   9.42%(4.63s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
   4.1%     4.1%       2.019s       1.21e-03s   1672   283      0.3        0.0 GpuCorrMM{valid, (1, 1)}(GpuContiguous.0, GpuContiguous.0)
    input 0: dtype=float32, shape=(1, 1, 10, 280), strides=(0, 0, 280, 1) 
    input 1: dtype=float32, shape=(1, 1, 10, 80), strides=(0, 0, 80, 1) 
    output 0: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
   4.1%     8.2%       2.013s       1.20e-03s   1672   273      0.3        0.0 GpuCorrMM{valid, (1, 1)}(GpuContiguous.0, GpuContiguous.0)
    input 0: dtype=float32, shape=(1, 1, 10, 280), strides=(0, 0, 280, 1) 
    input 1: dtype=float32, shape=(1, 1, 10, 80), strides=(0, 0, 80, 1) 
    output 0: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
   4.1%    12.3%       2.009s       1.20e-03s   1672   277      0.3        0.0 GpuCorrMM{valid, (1, 1)}(GpuContiguous.0, GpuContiguous.0)
    input 0: dtype=float32, shape=(1, 1, 10, 280), strides=(0, 0, 280, 1) 
    input 1: dtype=float32, shape=(1, 1, 10, 80), strides=(0, 0, 80, 1) 
    output 0: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
   4.1%    16.4%       2.006s       1.20e-03s   1672   271      0.3        0.0 GpuCorrMM{valid, (1, 1)}(GpuContiguous.0, GpuContiguous.0)
    input 0: dtype=float32, shape=(1, 1, 10, 280), strides=(0, 0, 280, 1) 
    input 1: dtype=float32, shape=(1, 1, 10, 80), strides=(0, 0, 80, 1) 
    output 0: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
   2.6%    19.0%       1.301s       7.78e-04s   1672    68      4.3        0.0 GpuCorrMM_gradInputs{valid, (1, 1)}(GpuContiguous.0, GpuContiguous.0)
    input 0: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
    input 1: dtype=float32, shape=(1, 1, 10, 80), strides=(0, 0, 80, 1) 
    output 0: dtype=float32, shape=(1, 1, 10, 280), strides=(0, 0, 280, 1) 
   2.5%    21.5%       1.238s       7.40e-04s   1672   216                     GpuReshape{2}(GpuDimShuffle{1,0,2}.0, MakeVector.0)
    input 0: dtype=float32, shape=(10, 80, 250), strides=(250, 2500, 1) 
    input 1: dtype=int64, shape=(2,), strides=c 
    output 0: dtype=float32, shape=(800, 250), strides=(250, 1) 
   2.5%    24.0%       1.233s       7.38e-04s   1672   205                     GpuReshape{2}(GpuDimShuffle{1,0,2}.0, MakeVector.0)
    input 0: dtype=float32, shape=(10, 80, 250), strides=(250, 2500, 1) 
    input 1: dtype=int64, shape=(2,), strides=c 
    output 0: dtype=float32, shape=(800, 250), strides=(250, 1) 
   2.5%    26.5%       1.228s       7.35e-04s   1672   209                     GpuReshape{2}(GpuDimShuffle{1,0,2}.0, MakeVector.0)
    input 0: dtype=float32, shape=(10, 80, 250), strides=(250, 2500, 1) 
    input 1: dtype=int64, shape=(2,), strides=c 
    output 0: dtype=float32, shape=(800, 250), strides=(250, 1) 
   2.5%    29.0%       1.223s       7.31e-04s   1672   212                     GpuReshape{2}(GpuDimShuffle{1,0,2}.0, MakeVector.0)
    input 0: dtype=float32, shape=(10, 80, 250), strides=(250, 2500, 1) 
    input 1: dtype=int64, shape=(2,), strides=c 
    output 0: dtype=float32, shape=(800, 250), strides=(250, 1) 
   2.5%    31.5%       1.214s       7.26e-04s   1672   278      0.3        0.0 GpuCorrMM{valid, (1, 1)}(GpuContiguous.0, GpuContiguous.0)
    input 0: dtype=float32, shape=(1, 1, 10, 280), strides=(0, 0, 280, 1) 
    input 1: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
    output 0: dtype=float32, shape=(1, 1, 10, 80), strides=(0, 0, 80, 1) 
   2.4%    33.9%       1.197s       7.16e-04s   1672   284      0.3        0.0 GpuCorrMM{valid, (1, 1)}(GpuContiguous.0, GpuContiguous.0)
    input 0: dtype=float32, shape=(1, 1, 10, 280), strides=(0, 0, 280, 1) 
    input 1: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
    output 0: dtype=float32, shape=(1, 1, 10, 80), strides=(0, 0, 80, 1) 
   2.4%    36.3%       1.191s       7.12e-04s   1672   274      0.3        0.0 GpuCorrMM{valid, (1, 1)}(GpuContiguous.0, GpuContiguous.0)
    input 0: dtype=float32, shape=(1, 1, 10, 280), strides=(0, 0, 280, 1) 
    input 1: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
    output 0: dtype=float32, shape=(1, 1, 10, 80), strides=(0, 0, 80, 1) 
   2.4%    38.7%       1.175s       7.03e-04s   1672   272      0.3        0.0 GpuCorrMM{valid, (1, 1)}(GpuContiguous.0, GpuContiguous.0)
    input 0: dtype=float32, shape=(1, 1, 10, 280), strides=(0, 0, 280, 1) 
    input 1: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
    output 0: dtype=float32, shape=(1, 1, 10, 80), strides=(0, 0, 80, 1) 
   2.3%    41.0%       1.112s       6.65e-04s   1672   147                     GpuElemwise{mul,no_inplace}(GpuElemwise{Add}[(0, 0)].0, GpuElemwise{TrueDiv}[(0, 0)].0)
    input 0: dtype=float32, shape=(1, 10, 500), strides=(0, 500, 1) 
    input 1: dtype=float32, shape=(80, 10, 1), strides=(10, 1, 0) 
    output 0: dtype=float32, shape=(80, 10, 500), strides=(5000, 500, 1) 
   2.2%    43.2%       1.101s       6.59e-04s   1672   155                     GpuIncSubtensor{Inc;int64:int64:}(GpuIncSubtensor{InplaceInc;int64:int64:}.0, GpuElemwise{mul,no_inplace}.0, ScalarFromTensor.0, ScalarFromTensor.0)
    input 0: dtype=float32, shape=(831, 10, 500), strides=(5000, 500, 1) 
    input 1: dtype=float32, shape=(80, 10, 500), strides=(5000, 500, 1) 
    input 2: dtype=int64, shape=8, strides=c 
    input 3: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(831, 10, 500), strides=(5000, 500, 1) 
   2.2%    45.4%       1.087s       6.50e-04s   1672   146                     GpuElemwise{mul,no_inplace}(GpuDimShuffle{x,0,1}.0, GpuElemwise{TrueDiv}[(0, 0)].0)
    input 0: dtype=float32, shape=(1, 10, 500), strides=(0, 500, 1) 
    input 1: dtype=float32, shape=(80, 10, 1), strides=(10, 1, 0) 
    output 0: dtype=float32, shape=(80, 10, 500), strides=(5000, 500, 1) 
   2.2%    47.6%       1.080s       6.46e-04s   1672   116                     GpuElemwise{mul,no_inplace}(GpuElemwise{Add}[(0, 0)].0, GpuSubtensor{int64:int64:}.0)
    input 0: dtype=float32, shape=(1, 10, 500), strides=(0, 500, 1) 
    input 1: dtype=float32, shape=(80, 10, 500), strides=(10000, 500, 1) 
    output 0: dtype=float32, shape=(80, 10, 500), strides=(5000, 500, 1) 
   2.2%    49.8%       1.065s       6.37e-04s   1672    54                     GpuElemwise{mul,no_inplace}(GpuDimShuffle{x,0,1}.0, GpuSubtensor{int64:int64:}.0)
    input 0: dtype=float32, shape=(1, 10, 500), strides=(0, 500, 1) 
    input 1: dtype=float32, shape=(80, 10, 500), strides=(10000, 500, 1) 
    output 0: dtype=float32, shape=(80, 10, 500), strides=(5000, 500, 1) 
   1.7%    51.5%       0.828s       4.95e-04s   1672   120                     GpuReshape{2}(GpuDimShuffle{0,1,2}.0, MakeVector.0)
    input 0: dtype=float32, shape=(80, 10, 250), strides=(250, 20000, 1) 
    input 1: dtype=int64, shape=(2,), strides=c 
    output 0: dtype=float32, shape=(800, 250), strides=(250, 1) 
   1.6%    53.1%       0.805s       4.82e-04s   1672   231                     GpuIncSubtensor{Inc;int64:int64:}(GpuIncSubtensor{InplaceInc;int64:int64:}.0, GpuElemwise{Mul}[(0, 0)].0, ScalarFromTensor.0, ScalarFromTensor.0)
    input 0: dtype=float32, shape=(831, 10, 250), strides=(2500, 250, 1) 
    input 1: dtype=float32, shape=(80, 10, 250), strides=(2500, 250, 1) 
    input 2: dtype=int64, shape=8, strides=c 
    input 3: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(831, 10, 250), strides=(2500, 250, 1) 
   ... (remaining 278 Apply instances account for 46.91%(23.08s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 60058KB (60058KB)
    CPU: 4KB (4KB)
    GPU: 60053KB (60053KB)
---
    Max if linker=cvm(default): 45372KB (43894KB)
    CPU: 4KB (4KB)
    GPU: 45368KB (43890KB)
---
    Memory saved if views are used: 19276KB (19276KB)
    Memory saved if inplace ops are used: 46071KB (46071KB)
    Memory saved if gc is enabled: 14686KB (16164KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

      16620000B  [(831, 10, 500)] i GpuIncSubtensor{InplaceInc;int64:int64:}(GpuElemwise{Composite{(i0 + (i1 + i1))},no_inplace}.0, GpuElemwise{mul,no_inplace}.0, ScalarFromTensor.0, ScalarFromTensor.0)
      16620000B  [(831, 10, 500)] c GpuIncSubtensor{Inc;int64:int64:}(GpuIncSubtensor{InplaceInc;int64:int64:}.0, GpuElemwise{mul,no_inplace}.0, ScalarFromTensor.0, ScalarFromTensor.0)
      16620000B  [(831, 10, 500)] c GpuElemwise{Composite{(i0 + (i1 + i1))},no_inplace}(<CudaNdarrayType(float32, 3D)>, <CudaNdarrayType(float32, 3D)>)
       8310000B  [(831, 10, 250)] c GpuElemwise{Composite{(i0 + (i1 + (i1 + (i1 + i1))))},no_inplace}(<CudaNdarrayType(float32, 3D)>, <CudaNdarrayType(float32, 3D)>)
       8310000B  [(831, 10, 250)] c GpuIncSubtensor{Inc;int64:int64:}(GpuIncSubtensor{InplaceInc;int64:int64:}.0, GpuElemwise{Mul}[(0, 0)].0, ScalarFromTensor.0, ScalarFromTensor.0)
       8310000B  [(831, 10, 250)] i GpuIncSubtensor{InplaceInc;int64:int64:}(GpuElemwise{Composite{(i0 + (i1 + (i1 + (i1 + i1))))},no_inplace}.0, GpuElemwise{Mul}[(0, 0)].0, ScalarFromTensor.0, ScalarFromTensor.0)
       8310000B  [(831, 10, 250)] i GpuIncSubtensor{InplaceInc;int64:int64:}(GpuIncSubtensor{InplaceInc;int64:int64:}.0, GpuElemwise{Mul}[(0, 0)].0, ScalarFromTensor.0, ScalarFromTensor.0)
       8310000B  [(831, 10, 250)] i GpuIncSubtensor{InplaceInc;int64:int64:}(GpuIncSubtensor{InplaceInc;int64:int64:}.0, GpuElemwise{Mul}[(0, 0)].0, ScalarFromTensor.0, ScalarFromTensor.0)
       1600000B  [(80, 10, 500)] c GpuElemwise{mul,no_inplace}(GpuElemwise{Add}[(0, 0)].0, GpuSubtensor{int64:int64:}.0)
       1600000B  [(80, 10, 500)] c GpuElemwise{mul,no_inplace}(GpuDimShuffle{x,0,1}.0, GpuSubtensor{int64:int64:}.0)
       1600000B  [(80, 10, 500)] c GpuElemwise{mul,no_inplace}(GpuElemwise{Add}[(0, 0)].0, GpuElemwise{TrueDiv}[(0, 0)].0)
       1600000B  [(80, 10, 500)] c GpuElemwise{mul,no_inplace}(GpuDimShuffle{x,0,1}.0, GpuElemwise{TrueDiv}[(0, 0)].0)
       1600000B  [(80, 10, 500)] v GpuSubtensor{int64:int64:}(conv_att_take_glimpses_attended_replace[cuda], ScalarFromTensor.0, ScalarFromTensor.0)
        800000B  [(80, 10, 250)] v GpuReshape{3}(GpuDot22.0, MakeVector.0)
        800000B  [(800, 250)] c GpuDot22(GpuReshape{2}.0, <CudaNdarrayType(float32, matrix)>)
        800000B  [(80, 10, 250)] v GpuReshape{3}(GpuDot22.0, MakeVector.0)
        800000B  [(80, 10, 250)] i GpuElemwise{Mul}[(0, 0)](GpuDimShuffle{0,1,2}.0, GpuElemwise{Composite{(i0 - sqr(i1))}}[(0, 1)].0)
        800000B  [(80, 10, 250)] v GpuDimShuffle{0,1,2}(GpuReshape{3}.0)
        800000B  [(80, 10, 250)] v GpuDimShuffle{1,0,2}(GpuReshape{3}.0)
        800000B  [(80, 10, 250)] i GpuElemwise{Composite{(i0 - sqr(i1))}}[(0, 1)](CudaNdarrayConstant{[[[ 1.]]]}, GpuElemwise{Composite{tanh(((i0 + i1) + i2))}}[(0, 2)].0)
   ... (remaining 278 Apply account for 23403900B/128413900B ((18.23%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan )
==================
  Message: None
  Time in 10 calls of the op (for a total of 12654 steps) 3.284631e+01s

  Total time spent in calling the VM 2.820536e+01s (85.871%)
  Total overhead (computing slices..) 4.640951e+00s (14.129%)

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  43.7%    43.7%       7.333s       9.66e-05s     C    75924       6   theano.sandbox.cuda.blas.GpuGemm
  38.5%    82.2%       6.456s       2.32e-05s     C   278388      22   theano.sandbox.cuda.basic_ops.GpuElemwise
   6.2%    88.3%       1.034s       2.04e-05s     C    50616       4   theano.sandbox.cuda.basic_ops.GpuIncSubtensor
   5.9%    94.2%       0.982s       3.88e-05s     C    25308       2   theano.sandbox.cuda.blas.GpuDot22
   3.0%    97.2%       0.505s       1.99e-05s     C    25308       2   theano.sandbox.cuda.basic_ops.GpuAlloc
   1.4%    98.6%       0.230s       4.55e-06s     C    50616       4   theano.sandbox.cuda.basic_ops.GpuSubtensor
   0.9%    99.5%       0.154s       3.05e-06s     C    50616       4   theano.compile.ops.Shape_i
   0.5%   100.0%       0.086s       3.39e-06s     C    25308       2   theano.sandbox.cuda.basic_ops.GpuDimShuffle
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  34.3%    34.3%       5.753s       1.14e-04s     C     50616        4   GpuGemm{no_inplace}
   9.4%    43.7%       1.580s       6.24e-05s     C     25308        2   GpuGemm{inplace}
   9.0%    52.7%       1.510s       1.99e-05s     C     75924        6   GpuElemwise{mul,no_inplace}
   8.8%    61.5%       1.475s       5.83e-05s     C     25308        2   GpuElemwise{Composite{((i0 * (i1 - i2)) + (i3 * i4) + i5 + i6)},no_inplace}
   5.9%    67.3%       0.982s       3.88e-05s     C     25308        2   GpuDot22
   3.2%    70.6%       0.541s       2.14e-05s     C     25308        2   GpuIncSubtensor{InplaceInc;::, int64::}
   3.1%    73.7%       0.528s       2.09e-05s     C     25308        2   GpuElemwise{ScalarSigmoid}[(0, 0)]
   3.1%    76.8%       0.520s       2.05e-05s     C     25308        2   GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}
   3.1%    79.9%       0.514s       2.03e-05s     C     25308        2   GpuElemwise{Tanh}[(0, 0)]
   3.0%    82.9%       0.505s       1.99e-05s     C     25308        2   GpuAlloc{memset_0=True}
   2.9%    85.8%       0.493s       1.95e-05s     C     25308        2   GpuIncSubtensor{InplaceInc;::, :int64:}
   2.9%    88.7%       0.489s       1.93e-05s     C     25308        2   GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}
   2.9%    91.6%       0.484s       1.91e-05s     C     25308        2   GpuElemwise{sub,no_inplace}
   2.8%    94.4%       0.470s       1.86e-05s     C     25308        2   GpuElemwise{Composite{((i0 * i1) + (-(i0 * i2)))}}[(0, 1)]
   2.8%    97.2%       0.466s       1.84e-05s     C     25308        2   GpuElemwise{Mul}[(0, 0)]
   0.7%    97.9%       0.116s       4.58e-06s     C     25308        2   GpuSubtensor{::, :int64:}
   0.7%    98.6%       0.114s       4.52e-06s     C     25308        2   GpuSubtensor{::, int64::}
   0.5%    99.1%       0.086s       3.39e-06s     C     25308        2   GpuDimShuffle{1,0}
   0.5%    99.6%       0.080s       3.16e-06s     C     25308        2   Shape_i{1}
   0.4%   100.0%       0.074s       2.94e-06s     C     25308        2   Shape_i{0}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
   9.0%     9.0%       1.515s       1.20e-04s   12654     2                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
   8.8%    17.8%       1.471s       1.16e-04s   12654     7                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
   8.3%    26.1%       1.387s       1.10e-04s   12654    23                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   8.2%    34.3%       1.380s       1.09e-04s   12654    21                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace1[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   4.7%    39.0%       0.790s       6.24e-05s   12654    42                     GpuGemm{inplace}(GpuElemwise{mul,no_inplace}.0, TensorConstant{1.0}, GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}.0, state_to_gates_copy.T_replace1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 500), strides=(500, 1) 
    input 3: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   4.7%    43.7%       0.790s       6.24e-05s   12654    43                     GpuGemm{inplace}(GpuElemwise{mul,no_inplace}.0, TensorConstant{1.0}, GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}.0, state_to_gates_copy.T_replace0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 500), strides=(500, 1) 
    input 3: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   4.5%    48.2%       0.757s       5.98e-05s   12654    45                     GpuElemwise{Composite{((i0 * (i1 - i2)) + (i3 * i4) + i5 + i6)},no_inplace}(GpuElemwise{mul,no_inplace}.0, CudaNdarrayConstant{[[ 1.]]}, GpuSubtensor{::, :int64:}.0, gatedrecurrent_apply_states0[cuda], GpuElemwise{sub,no_inplace}.0, gatedrecurrent_apply_states0[cuda], GpuGemm{inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 1: dtype=float32, shape=(1, 1), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=(500, 1) 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(10, 1), strides=(1, 0) 
    input 5: dtype=float32, shape=(10, 250), strides=c 
    input 6: dtype=float32, shape=(10, 250), strides=(250, 1) 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   4.3%    52.5%       0.718s       5.68e-05s   12654    44                     GpuElemwise{Composite{((i0 * (i1 - i2)) + (i3 * i4) + i5 + i6)},no_inplace}(GpuElemwise{mul,no_inplace}.0, CudaNdarrayConstant{[[ 1.]]}, GpuSubtensor{::, :int64:}.0, gatedrecurrent_apply_states1[cuda], GpuElemwise{sub,no_inplace}.0, gatedrecurrent_apply_states1[cuda], GpuGemm{inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 1: dtype=float32, shape=(1, 1), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=(500, 1) 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(10, 1), strides=(1, 0) 
    input 5: dtype=float32, shape=(10, 250), strides=c 
    input 6: dtype=float32, shape=(10, 250), strides=(250, 1) 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   2.9%    55.4%       0.492s       3.89e-05s   12654    30                     GpuDot22(GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}.0, state_to_state_copy.T_replace1[cuda])
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 1: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   2.9%    58.3%       0.490s       3.88e-05s   12654    31                     GpuDot22(GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}.0, state_to_state_copy.T_replace0[cuda])
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 1: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   1.6%    60.0%       0.272s       2.15e-05s   12654    36                     GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
   1.6%    61.6%       0.269s       2.12e-05s   12654    37                     GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
   1.6%    63.2%       0.266s       2.11e-05s   12654    11                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
   1.6%    64.7%       0.263s       2.07e-05s   12654    26                     GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}(GpuElemwise{mul,no_inplace}.0, GpuSubtensor{::, :int64:}.0, CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{Tanh}[(0, 0)].0)
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 1: dtype=float32, shape=(10, 250), strides=(500, 1) 
    input 2: dtype=float32, shape=(1, 1), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=(250, 1) 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   1.6%    66.3%       0.262s       2.07e-05s   12654    13                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
   1.5%    67.8%       0.259s       2.04e-05s   12654    10                     GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[ 0.]]}, Shape_i{0}.0, Shape_i{1}.0)
    input 0: dtype=float32, shape=(1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=(500, 1) 
   1.5%    69.4%       0.257s       2.03e-05s   12654    24                     GpuElemwise{Tanh}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   1.5%    70.9%       0.257s       2.03e-05s   12654    27                     GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}(GpuElemwise{mul,no_inplace}.0, GpuSubtensor{::, :int64:}.0, CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{Tanh}[(0, 0)].0)
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    input 1: dtype=float32, shape=(10, 250), strides=(500, 1) 
    input 2: dtype=float32, shape=(1, 1), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=(250, 1) 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   1.5%    72.4%       0.256s       2.03e-05s   12654    25                     GpuElemwise{Tanh}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   1.5%    73.9%       0.256s       2.02e-05s   12654     4                     GpuElemwise{mul,no_inplace}(gatedrecurrent_apply_states1[cuda], <CudaNdarrayType(float32, col)>)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=(250, 1) 
   ... (remaining 26 Apply instances account for 26.06%(4.37s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 254KB (254KB)
    CPU: 0KB (0KB)
    GPU: 254KB (254KB)
---
    Max if linker=cvm(default): 176KB (195KB)
    CPU: 0KB (0KB)
    GPU: 176KB (195KB)
---
    Memory saved if views are used: 59KB (59KB)
    Memory saved if inplace ops are used: 195KB (195KB)
    Memory saved if gc is enabled: 78KB (58KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, :int64:}(GpuIncSubtensor{InplaceInc;::, int64::}.0, GpuElemwise{Composite{((i0 * i1) + (-(i0 * i2)))}}[(0, 1)].0, Constant{250})
         20000B  [(10, 500)] c GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}(GpuIncSubtensor{InplaceInc;::, :int64:}.0, GpuElemwise{ScalarSigmoid}[(0, 0)].0, CudaNdarrayConstant{[[ 1.]]})
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
         20000B  [(10, 500)] c GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}(GpuIncSubtensor{InplaceInc;::, :int64:}.0, GpuElemwise{ScalarSigmoid}[(0, 0)].0, CudaNdarrayConstant{[[ 1.]]})
         20000B  [(10, 500)] c GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[ 0.]]}, Shape_i{0}.0, Shape_i{1}.0)
         20000B  [(10, 500)] c GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[ 0.]]}, Shape_i{0}.0, Shape_i{1}.0)
         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, :int64:}(GpuIncSubtensor{InplaceInc;::, int64::}.0, GpuElemwise{Composite{((i0 * i1) + (-(i0 * i2)))}}[(0, 1)].0, Constant{250})
         10000B  [(10, 250)] c GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(gatedrecurrent_apply_states1[cuda], <CudaNdarrayType(float32, col)>)
         10000B  [(10, 250)] i GpuGemm{inplace}(GpuElemwise{mul,no_inplace}.0, TensorConstant{1.0}, GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}.0, state_to_gates_copy.T_replace0[cuda], TensorConstant{1.0})
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(GpuDot22.0, GpuSubtensor{::, int64::}.0)
         10000B  [(10, 250)] v GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] v GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(250, 10)] v GpuDimShuffle{1,0}(GpuElemwise{mul,no_inplace}.0)
         10000B  [(10, 250)] i GpuGemm{inplace}(GpuElemwise{mul,no_inplace}.0, TensorConstant{1.0}, GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}.0, state_to_gates_copy.T_replace1[cuda], TensorConstant{1.0})
   ... (remaining 26 Apply account for 200112B/520112B ((38.47%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan )
==================
  Message: None
  Time in 10 calls of the op (for a total of 12649 steps) 3.266755e+01s

  Total time spent in calling the VM 2.805633e+01s (85.884%)
  Total overhead (computing slices..) 4.611219e+00s (14.116%)

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  43.7%    43.7%       7.315s       9.64e-05s     C    75894       6   theano.sandbox.cuda.blas.GpuGemm
  38.5%    82.3%       6.444s       2.32e-05s     C   278278      22   theano.sandbox.cuda.basic_ops.GpuElemwise
   6.1%    88.4%       1.019s       2.01e-05s     C    50596       4   theano.sandbox.cuda.basic_ops.GpuIncSubtensor
   5.8%    94.2%       0.976s       3.86e-05s     C    25298       2   theano.sandbox.cuda.blas.GpuDot22
   3.0%    97.2%       0.501s       1.98e-05s     C    25298       2   theano.sandbox.cuda.basic_ops.GpuAlloc
   1.4%    98.6%       0.233s       4.60e-06s     C    50596       4   theano.sandbox.cuda.basic_ops.GpuSubtensor
   0.9%    99.5%       0.151s       2.98e-06s     C    50596       4   theano.compile.ops.Shape_i
   0.5%   100.0%       0.087s       3.42e-06s     C    25298       2   theano.sandbox.cuda.basic_ops.GpuDimShuffle
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  34.4%    34.4%       5.747s       1.14e-04s     C     50596        4   GpuGemm{no_inplace}
   9.4%    43.7%       1.568s       6.20e-05s     C     25298        2   GpuGemm{inplace}
   9.1%    52.8%       1.519s       6.00e-05s     C     25298        2   GpuElemwise{Composite{((i0 * (i1 - i2)) + (i3 * i4) + i5 + i6)},no_inplace}
   8.9%    61.7%       1.489s       1.96e-05s     C     75894        6   GpuElemwise{mul,no_inplace}
   5.8%    67.6%       0.976s       3.86e-05s     C     25298        2   GpuDot22
   3.2%    70.8%       0.536s       2.12e-05s     C     25298        2   GpuIncSubtensor{InplaceInc;::, int64::}
   3.1%    73.9%       0.525s       2.08e-05s     C     25298        2   GpuElemwise{ScalarSigmoid}[(0, 0)]
   3.1%    77.0%       0.514s       2.03e-05s     C     25298        2   GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}
   3.1%    80.0%       0.510s       2.02e-05s     C     25298        2   GpuElemwise{Tanh}[(0, 0)]
   3.0%    83.0%       0.501s       1.98e-05s     C     25298        2   GpuAlloc{memset_0=True}
   2.9%    85.9%       0.485s       1.92e-05s     C     25298        2   GpuElemwise{sub,no_inplace}
   2.9%    88.8%       0.483s       1.91e-05s     C     25298        2   GpuIncSubtensor{InplaceInc;::, :int64:}
   2.9%    91.7%       0.478s       1.89e-05s     C     25298        2   GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}
   2.8%    94.4%       0.466s       1.84e-05s     C     25298        2   GpuElemwise{Composite{((i0 * i1) + (-(i0 * i2)))}}[(0, 1)]
   2.7%    97.2%       0.459s       1.81e-05s     C     25298        2   GpuElemwise{Mul}[(0, 0)]
   0.7%    97.9%       0.118s       4.65e-06s     C     25298        2   GpuSubtensor{::, :int64:}
   0.7%    98.6%       0.115s       4.55e-06s     C     25298        2   GpuSubtensor{::, int64::}
   0.5%    99.1%       0.087s       3.42e-06s     C     25298        2   GpuDimShuffle{1,0}
   0.5%    99.6%       0.078s       3.07e-06s     C     25298        2   Shape_i{1}
   0.4%   100.0%       0.073s       2.89e-06s     C     25298        2   Shape_i{0}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
   9.0%     9.0%       1.512s       1.20e-04s   12649     2                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   8.8%    17.8%       1.467s       1.16e-04s   12649     7                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   8.3%    26.1%       1.389s       1.10e-04s   12649    23                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   8.2%    34.4%       1.379s       1.09e-04s   12649    21                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace1[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   4.7%    39.1%       0.787s       6.22e-05s   12649    43                     GpuGemm{inplace}(GpuElemwise{mul,no_inplace}.0, TensorConstant{1.0}, GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}.0, state_to_gates_copy.T_replace0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 500), strides=c 
    input 3: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   4.7%    43.7%       0.782s       6.18e-05s   12649    44                     GpuElemwise{Composite{((i0 * (i1 - i2)) + (i3 * i4) + i5 + i6)},no_inplace}(GpuElemwise{mul,no_inplace}.0, CudaNdarrayConstant{[[ 1.]]}, GpuSubtensor{::, :int64:}.0, gatedrecurrent_apply_states1[cuda], GpuElemwise{sub,no_inplace}.0, gatedrecurrent_apply_states1[cuda], GpuGemm{inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(1, 1), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(10, 1), strides=c 
    input 5: dtype=float32, shape=(10, 250), strides=c 
    input 6: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   4.7%    48.4%       0.782s       6.18e-05s   12649    42                     GpuGemm{inplace}(GpuElemwise{mul,no_inplace}.0, TensorConstant{1.0}, GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}.0, state_to_gates_copy.T_replace1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 500), strides=c 
    input 3: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   4.4%    52.8%       0.737s       5.83e-05s   12649    45                     GpuElemwise{Composite{((i0 * (i1 - i2)) + (i3 * i4) + i5 + i6)},no_inplace}(GpuElemwise{mul,no_inplace}.0, CudaNdarrayConstant{[[ 1.]]}, GpuSubtensor{::, :int64:}.0, gatedrecurrent_apply_states0[cuda], GpuElemwise{sub,no_inplace}.0, gatedrecurrent_apply_states0[cuda], GpuGemm{inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(1, 1), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(10, 1), strides=c 
    input 5: dtype=float32, shape=(10, 250), strides=c 
    input 6: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   2.9%    55.7%       0.489s       3.86e-05s   12649    30                     GpuDot22(GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}.0, state_to_state_copy.T_replace1[cuda])
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   2.9%    58.7%       0.487s       3.85e-05s   12649    31                     GpuDot22(GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}.0, state_to_state_copy.T_replace0[cuda])
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   1.6%    60.3%       0.271s       2.14e-05s   12649    36                     GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   1.6%    61.9%       0.265s       2.10e-05s   12649    11                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   1.6%    63.4%       0.265s       2.09e-05s   12649    37                     GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   1.6%    65.0%       0.260s       2.06e-05s   12649    13                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   1.5%    66.5%       0.258s       2.04e-05s   12649    26                     GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}(GpuElemwise{mul,no_inplace}.0, GpuSubtensor{::, :int64:}.0, CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{Tanh}[(0, 0)].0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=float32, shape=(1, 1), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   1.5%    68.1%       0.256s       2.03e-05s   12649    27                     GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}(GpuElemwise{mul,no_inplace}.0, GpuSubtensor{::, :int64:}.0, CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{Tanh}[(0, 0)].0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=float32, shape=(1, 1), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   1.5%    69.6%       0.256s       2.03e-05s   12649    24                     GpuElemwise{Tanh}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   1.5%    71.1%       0.255s       2.02e-05s   12649    10                     GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[ 0.]]}, Shape_i{0}.0, Shape_i{1}.0)
    input 0: dtype=float32, shape=(1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   1.5%    72.6%       0.254s       2.01e-05s   12649    25                     GpuElemwise{Tanh}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   1.5%    74.2%       0.253s       2.00e-05s   12649     4                     GpuElemwise{mul,no_inplace}(gatedrecurrent_apply_states1[cuda], <CudaNdarrayType(float32, col)>)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   ... (remaining 26 Apply instances account for 25.84%(4.32s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 254KB (254KB)
    CPU: 0KB (0KB)
    GPU: 254KB (254KB)
---
    Max if linker=cvm(default): 176KB (195KB)
    CPU: 0KB (0KB)
    GPU: 176KB (195KB)
---
    Memory saved if views are used: 59KB (59KB)
    Memory saved if inplace ops are used: 195KB (195KB)
    Memory saved if gc is enabled: 78KB (58KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
         20000B  [(10, 500)] c GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}(GpuIncSubtensor{InplaceInc;::, :int64:}.0, GpuElemwise{ScalarSigmoid}[(0, 0)].0, CudaNdarrayConstant{[[ 1.]]})
         20000B  [(10, 500)] c GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[ 0.]]}, Shape_i{0}.0, Shape_i{1}.0)
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, :int64:}(GpuIncSubtensor{InplaceInc;::, int64::}.0, GpuElemwise{Composite{((i0 * i1) + (-(i0 * i2)))}}[(0, 1)].0, Constant{250})
         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
         20000B  [(10, 500)] c GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}(GpuIncSubtensor{InplaceInc;::, :int64:}.0, GpuElemwise{ScalarSigmoid}[(0, 0)].0, CudaNdarrayConstant{[[ 1.]]})
         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, :int64:}(GpuIncSubtensor{InplaceInc;::, int64::}.0, GpuElemwise{Composite{((i0 * i1) + (-(i0 * i2)))}}[(0, 1)].0, Constant{250})
         20000B  [(10, 500)] c GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[ 0.]]}, Shape_i{0}.0, Shape_i{1}.0)
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(GpuDot22.0, GpuSubtensor{::, int64::}.0)
         10000B  [(10, 250)] c GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
         10000B  [(10, 250)] i GpuElemwise{Mul}[(0, 0)](GpuDot22.0, gatedrecurrent_apply_states_replace0[cuda])
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(gatedrecurrent_apply_states0[cuda], <CudaNdarrayType(float32, col)>)
         10000B  [(10, 250)] c GpuElemwise{Composite{((i0 * (i1 - i2)) + (i3 * i4) + i5 + i6)},no_inplace}(GpuElemwise{mul,no_inplace}.0, CudaNdarrayConstant{[[ 1.]]}, GpuSubtensor{::, :int64:}.0, gatedrecurrent_apply_states1[cuda], GpuElemwise{sub,no_inplace}.0, gatedrecurrent_apply_states1[cuda], GpuGemm{inplace}.0)
         10000B  [(10, 250)] c GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}(GpuElemwise{mul,no_inplace}.0, GpuSubtensor{::, :int64:}.0, CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{Tanh}[(0, 0)].0)
         10000B  [(10, 250)] c GpuElemwise{Composite{((i0 * (i1 - i2)) + (i3 * i4) + i5 + i6)},no_inplace}(GpuElemwise{mul,no_inplace}.0, CudaNdarrayConstant{[[ 1.]]}, GpuSubtensor{::, :int64:}.0, gatedrecurrent_apply_states0[cuda], GpuElemwise{sub,no_inplace}.0, gatedrecurrent_apply_states0[cuda], GpuGemm{inplace}.0)
         10000B  [(10, 250)] v GpuSubtensor{::, :int64:}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
   ... (remaining 26 Apply account for 200112B/520112B ((38.47%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan )
==================
  Message: None
  Time in 10 calls of the op (for a total of 12649 steps) 3.218276e+01s

  Total time spent in calling the VM 2.764015e+01s (85.885%)
  Total overhead (computing slices..) 4.542603e+00s (14.115%)

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  44.1%    44.1%       7.291s       9.61e-05s     C    75894       6   theano.sandbox.cuda.blas.GpuGemm
  38.1%    82.2%       6.306s       2.27e-05s     C   278278      22   theano.sandbox.cuda.basic_ops.GpuElemwise
   6.1%    88.3%       1.008s       1.99e-05s     C    50596       4   theano.sandbox.cuda.basic_ops.GpuIncSubtensor
   5.9%    94.2%       0.971s       3.84e-05s     C    25298       2   theano.sandbox.cuda.blas.GpuDot22
   3.0%    97.2%       0.500s       1.98e-05s     C    25298       2   theano.sandbox.cuda.basic_ops.GpuAlloc
   1.4%    98.6%       0.229s       4.52e-06s     C    50596       4   theano.sandbox.cuda.basic_ops.GpuSubtensor
   0.9%    99.5%       0.151s       2.98e-06s     C    50596       4   theano.compile.ops.Shape_i
   0.5%   100.0%       0.086s       3.38e-06s     C    25298       2   theano.sandbox.cuda.basic_ops.GpuDimShuffle
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  34.6%    34.6%       5.726s       1.13e-04s     C     50596        4   GpuGemm{no_inplace}
   9.5%    44.1%       1.565s       6.19e-05s     C     25298        2   GpuGemm{inplace}
   9.0%    53.0%       1.484s       1.95e-05s     C     75894        6   GpuElemwise{mul,no_inplace}
   8.5%    61.6%       1.410s       5.57e-05s     C     25298        2   GpuElemwise{Composite{((i0 * (i1 - i2)) + (i3 * i4) + i5 + i6)},no_inplace}
   5.9%    67.4%       0.971s       3.84e-05s     C     25298        2   GpuDot22
   3.2%    70.7%       0.532s       2.10e-05s     C     25298        2   GpuIncSubtensor{InplaceInc;::, int64::}
   3.2%    73.8%       0.522s       2.06e-05s     C     25298        2   GpuElemwise{ScalarSigmoid}[(0, 0)]
   3.1%    76.9%       0.512s       2.02e-05s     C     25298        2   GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}
   3.1%    80.0%       0.506s       2.00e-05s     C     25298        2   GpuElemwise{Tanh}[(0, 0)]
   3.0%    83.0%       0.500s       1.98e-05s     C     25298        2   GpuAlloc{memset_0=True}
   2.9%    85.9%       0.477s       1.89e-05s     C     25298        2   GpuElemwise{sub,no_inplace}
   2.9%    88.7%       0.477s       1.88e-05s     C     25298        2   GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}
   2.9%    91.6%       0.476s       1.88e-05s     C     25298        2   GpuIncSubtensor{InplaceInc;::, :int64:}
   2.8%    94.4%       0.463s       1.83e-05s     C     25298        2   GpuElemwise{Composite{((i0 * i1) + (-(i0 * i2)))}}[(0, 1)]
   2.8%    97.2%       0.457s       1.81e-05s     C     25298        2   GpuElemwise{Mul}[(0, 0)]
   0.7%    97.9%       0.115s       4.55e-06s     C     25298        2   GpuSubtensor{::, :int64:}
   0.7%    98.6%       0.114s       4.49e-06s     C     25298        2   GpuSubtensor{::, int64::}
   0.5%    99.1%       0.086s       3.38e-06s     C     25298        2   GpuDimShuffle{1,0}
   0.5%    99.6%       0.077s       3.04e-06s     C     25298        2   Shape_i{1}
   0.4%   100.0%       0.074s       2.91e-06s     C     25298        2   Shape_i{0}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
   9.1%     9.1%       1.506s       1.19e-04s   12649     2                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   8.9%    18.0%       1.467s       1.16e-04s   12649     7                     GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 500), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   8.3%    26.3%       1.381s       1.09e-04s   12649    23                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   8.3%    34.6%       1.372s       1.08e-04s   12649    21                     GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace1[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(250, 250), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   4.7%    39.4%       0.784s       6.20e-05s   12649    43                     GpuGemm{inplace}(GpuElemwise{mul,no_inplace}.0, TensorConstant{1.0}, GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}.0, state_to_gates_copy.T_replace0[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 500), strides=c 
    input 3: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   4.7%    44.1%       0.781s       6.17e-05s   12649    42                     GpuGemm{inplace}(GpuElemwise{mul,no_inplace}.0, TensorConstant{1.0}, GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}.0, state_to_gates_copy.T_replace1[cuda], TensorConstant{1.0})
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(10, 500), strides=c 
    input 3: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   4.4%    48.4%       0.721s       5.70e-05s   12649    44                     GpuElemwise{Composite{((i0 * (i1 - i2)) + (i3 * i4) + i5 + i6)},no_inplace}(GpuElemwise{mul,no_inplace}.0, CudaNdarrayConstant{[[ 1.]]}, GpuSubtensor{::, :int64:}.0, gatedrecurrent_apply_states1[cuda], GpuElemwise{sub,no_inplace}.0, gatedrecurrent_apply_states1[cuda], GpuGemm{inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(1, 1), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(10, 1), strides=c 
    input 5: dtype=float32, shape=(10, 250), strides=c 
    input 6: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   4.2%    52.6%       0.689s       5.45e-05s   12649    45                     GpuElemwise{Composite{((i0 * (i1 - i2)) + (i3 * i4) + i5 + i6)},no_inplace}(GpuElemwise{mul,no_inplace}.0, CudaNdarrayConstant{[[ 1.]]}, GpuSubtensor{::, :int64:}.0, gatedrecurrent_apply_states0[cuda], GpuElemwise{sub,no_inplace}.0, gatedrecurrent_apply_states0[cuda], GpuGemm{inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(1, 1), strides=c 
    input 2: dtype=float32, shape=(10, 250), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    input 4: dtype=float32, shape=(10, 1), strides=c 
    input 5: dtype=float32, shape=(10, 250), strides=c 
    input 6: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   2.9%    55.5%       0.486s       3.84e-05s   12649    30                     GpuDot22(GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}.0, state_to_state_copy.T_replace1[cuda])
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   2.9%    58.5%       0.485s       3.84e-05s   12649    31                     GpuDot22(GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}.0, state_to_state_copy.T_replace0[cuda])
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   1.6%    60.1%       0.269s       2.12e-05s   12649    36                     GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   1.6%    61.7%       0.265s       2.10e-05s   12649    11                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   1.6%    63.3%       0.263s       2.08e-05s   12649    37                     GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
    input 0: dtype=float32, shape=(10, 500), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   1.6%    64.9%       0.259s       2.04e-05s   12649    10                     GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[ 0.]]}, Shape_i{0}.0, Shape_i{1}.0)
    input 0: dtype=float32, shape=(1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   1.6%    66.4%       0.258s       2.04e-05s   12649    26                     GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}(GpuElemwise{mul,no_inplace}.0, GpuSubtensor{::, :int64:}.0, CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{Tanh}[(0, 0)].0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=float32, shape=(1, 1), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   1.6%    68.0%       0.257s       2.03e-05s   12649    13                     GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 500), strides=c 
    output 0: dtype=float32, shape=(10, 500), strides=c 
   1.5%    69.5%       0.254s       2.01e-05s   12649    25                     GpuElemwise{Tanh}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   1.5%    71.0%       0.254s       2.00e-05s   12649    27                     GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}(GpuElemwise{mul,no_inplace}.0, GpuSubtensor{::, :int64:}.0, CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{Tanh}[(0, 0)].0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 250), strides=c 
    input 2: dtype=float32, shape=(1, 1), strides=c 
    input 3: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   1.5%    72.6%       0.252s       1.99e-05s   12649     4                     GpuElemwise{mul,no_inplace}(gatedrecurrent_apply_states1[cuda], <CudaNdarrayType(float32, col)>)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    input 1: dtype=float32, shape=(10, 1), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   1.5%    74.1%       0.252s       1.99e-05s   12649    24                     GpuElemwise{Tanh}[(0, 0)](GpuGemm{no_inplace}.0)
    input 0: dtype=float32, shape=(10, 250), strides=c 
    output 0: dtype=float32, shape=(10, 250), strides=c 
   ... (remaining 26 Apply instances account for 25.92%(4.29s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 254KB (254KB)
    CPU: 0KB (0KB)
    GPU: 254KB (254KB)
---
    Max if linker=cvm(default): 176KB (195KB)
    CPU: 0KB (0KB)
    GPU: 176KB (195KB)
---
    Memory saved if views are used: 59KB (59KB)
    Memory saved if inplace ops are used: 195KB (195KB)
    Memory saved if gc is enabled: 78KB (58KB)
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, :int64:}(GpuIncSubtensor{InplaceInc;::, int64::}.0, GpuElemwise{Composite{((i0 * i1) + (-(i0 * i2)))}}[(0, 1)].0, Constant{250})
         20000B  [(10, 500)] c GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}(GpuIncSubtensor{InplaceInc;::, :int64:}.0, GpuElemwise{ScalarSigmoid}[(0, 0)].0, CudaNdarrayConstant{[[ 1.]]})
         20000B  [(10, 500)] i GpuElemwise{ScalarSigmoid}[(0, 0)](GpuGemm{no_inplace}.0)
         20000B  [(10, 500)] c GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[ 0.]]}, Shape_i{0}.0, Shape_i{1}.0)
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace1[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace1[cuda], state_to_gates_copy1[cuda], TensorConstant{1.0})
         20000B  [(10, 500)] c GpuGemm{no_inplace}(gatedrecurrent_apply_gate_inputs_replace0[cuda], TensorConstant{1.0}, gatedrecurrent_apply_states_replace0[cuda], state_to_gates_copy0[cuda], TensorConstant{1.0})
         20000B  [(10, 500)] c GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[ 0.]]}, Shape_i{0}.0, Shape_i{1}.0)
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, :int64:}(GpuIncSubtensor{InplaceInc;::, int64::}.0, GpuElemwise{Composite{((i0 * i1) + (-(i0 * i2)))}}[(0, 1)].0, Constant{250})
         20000B  [(10, 500)] i GpuIncSubtensor{InplaceInc;::, int64::}(GpuAlloc{memset_0=True}.0, GpuElemwise{Mul}[(0, 0)].0, Constant{250})
         20000B  [(10, 500)] c GpuElemwise{Composite{((i0 * i1) * (i2 - i1))},no_inplace}(GpuIncSubtensor{InplaceInc;::, :int64:}.0, GpuElemwise{ScalarSigmoid}[(0, 0)].0, CudaNdarrayConstant{[[ 1.]]})
         10000B  [(10, 250)] i GpuElemwise{Composite{((i0 * i1) + (-(i0 * i2)))}}[(0, 1)](GpuElemwise{mul,no_inplace}.0, GpuElemwise{Tanh}[(0, 0)].0, gatedrecurrent_apply_states_replace1[cuda])
         10000B  [(10, 250)] v GpuSubtensor{::, int64::}(GpuElemwise{ScalarSigmoid}[(0, 0)].0, Constant{250})
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(gatedrecurrent_apply_states_replace1[cuda], GpuSubtensor{::, int64::}.0)
         10000B  [(10, 250)] c GpuElemwise{mul,no_inplace}(GpuDot22.0, GpuSubtensor{::, int64::}.0)
         10000B  [(250, 10)] v GpuDimShuffle{1,0}(GpuElemwise{mul,no_inplace}.0)
         10000B  [(10, 250)] c GpuElemwise{Composite{((i0 * i1) * (i2 - sqr(i3)))},no_inplace}(GpuElemwise{mul,no_inplace}.0, GpuSubtensor{::, :int64:}.0, CudaNdarrayConstant{[[ 1.]]}, GpuElemwise{Tanh}[(0, 0)].0)
         10000B  [(10, 250)] i GpuElemwise{Mul}[(0, 0)](GpuDot22.0, gatedrecurrent_apply_states_replace1[cuda])
         10000B  [(10, 250)] c GpuGemm{no_inplace}(gatedrecurrent_apply_inputs_replace0[cuda], TensorConstant{1.0}, GpuElemwise{mul,no_inplace}.0, state_to_state_copy0[cuda], TensorConstant{1.0})
   ... (remaining 26 Apply account for 200112B/520112B ((38.47%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

Function profiling
==================
  Message: Sum of all(14) printed profiles at exit excluding Scan op profile.
  Time in 34 calls to Function.__call__: 2.505645e+02s
  Time in Function.fn.__call__: 2.504457e+02s (99.953%)
  Time in thunks: 2.137713e+02s (85.316%)
  Total compile time: 4.228056e+02s
    Number of Apply nodes: 19
    Theano Optimizer time: 3.525797e+02s
       Theano validate time: 6.287514e+01s
    Theano Linker time (includes C, CUDA code generation/compiling): 2.681211e+01s
       Import time 1.505122e-01s

Time in all call to theano.grad() 5.968459e+00s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  96.6%    96.6%     206.482s       2.58e+00s     Py      80       8   theano.scan_module.scan_op.Scan
   1.0%    97.6%       2.183s       3.90e-03s     C      560      56   theano.sandbox.cuda.blas.GpuDot22
   0.5%    98.1%       0.986s       4.27e-04s     C     2310     231   theano.sandbox.cuda.basic_ops.GpuCAReduce
   0.3%    98.4%       0.656s       4.10e-03s     C      160      16   theano.sandbox.cuda.blas.GpuGemm
   0.3%    98.6%       0.540s       5.38e-05s     C    10028    1010   theano.sandbox.cuda.basic_ops.GpuElemwise
   0.2%    98.8%       0.436s       5.13e-04s     C      850      85   theano.sandbox.cuda.basic_ops.GpuFromHost
   0.2%    99.0%       0.370s       2.54e-05s     C    14589    1521   theano.tensor.elemwise.Elemwise
   0.2%    99.2%       0.352s       1.92e-04s     Py    1830     183   theano.sandbox.cuda.basic_ops.GpuFlatten
   0.2%    99.3%       0.323s       4.68e-04s     C      690      69   theano.sandbox.cuda.basic_ops.GpuIncSubtensor
   0.1%    99.5%       0.307s       6.26e-05s     C     4908     561   theano.sandbox.cuda.basic_ops.HostFromGpu
   0.1%    99.6%       0.215s       2.87e-04s     C      750      75   theano.sandbox.cuda.basic_ops.GpuAlloc
   0.1%    99.7%       0.175s       1.75e-02s     C       10       1   theano.sandbox.cuda.basic_ops.GpuAdvancedIncSubtensor1
   0.1%    99.7%       0.140s       6.37e-04s     C      220      74   theano.tensor.basic.Alloc
   0.1%    99.8%       0.108s       2.70e-03s     Py      40       4   theano.sandbox.cuda.basic_ops.GpuSplit
   0.0%    99.8%       0.087s       1.45e-03s     C       60       6   theano.sandbox.cuda.basic_ops.GpuJoin
   0.0%    99.8%       0.067s       4.84e-05s     C     1393     271   theano.compile.ops.DeepCopyOp
   0.0%    99.9%       0.061s       4.59e-05s     C     1320     132   theano.tensor.elemwise.Sum
   0.0%    99.9%       0.058s       5.85e-03s     C       10       1   theano.tensor.basic.Join
   0.0%    99.9%       0.056s       3.07e-05s     Py    1840     184   theano.sandbox.cuda.basic_ops.GpuReshape
   0.0%    99.9%       0.038s       1.52e-05s     Py    2490     166   theano.ifelse.IfElse
   ... (remaining 19 Classes account for   0.06%(0.13s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  45.7%    45.7%      97.775s       3.26e+00s     Py      30        3   forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}
  29.1%    74.8%      62.122s       6.21e+00s     Py      10        1   forall_inplace,gpu,grad_of_att_trans_do_apply_scan}
  16.9%    91.7%      36.035s       1.20e+00s     Py      30        3   forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}
   4.9%    96.6%      10.551s       1.06e+00s     Py      10        1   forall_inplace,gpu,att_trans_do_apply_scan}
   1.0%    97.6%       2.183s       3.90e-03s     C      560       56   GpuDot22
   0.3%    97.9%       0.656s       4.10e-03s     C      160       16   GpuGemm{inplace}
   0.2%    98.1%       0.436s       5.13e-04s     C      850       85   GpuFromHost
   0.2%    98.3%       0.370s       2.18e-03s     C      170       17   GpuCAReduce{add}{1,1,0}
   0.2%    98.5%       0.341s       2.28e-03s     Py     150       15   GpuFlatten{2}
   0.1%    98.6%       0.307s       6.26e-05s     C     4908      561   HostFromGpu
   0.1%    98.7%       0.263s       3.42e-04s     C      770       77   GpuCAReduce{pre=sqr,red=add}{1}
   0.1%    98.8%       0.226s       1.88e-04s     C     1200      120   GpuCAReduce{pre=sqr,red=add}{1,1}
   0.1%    98.9%       0.202s       3.06e-04s     C      660       66   GpuAlloc{memset_0=True}
   0.1%    99.0%       0.175s       1.75e-02s     C       10        1   GpuAdvancedIncSubtensor1{inplace,inc}
   0.1%    99.1%       0.147s       2.27e-04s     C      650       65   Elemwise{isinf,no_inplace}
   0.1%    99.1%       0.140s       6.37e-04s     C      220       74   Alloc
   0.1%    99.2%       0.133s       2.04e-04s     C      650       65   Elemwise{isnan,no_inplace}
   0.1%    99.3%       0.114s       5.69e-03s     C       20        2   GpuCAReduce{add}{1,1,1}
   0.0%    99.3%       0.093s       4.44e-04s     C      210       21   GpuIncSubtensor{InplaceInc;int64::}
   0.0%    99.3%       0.087s       1.45e-03s     C       60        6   GpuJoin
   ... (remaining 229 Ops account for   0.66%(1.41s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  29.1%    29.1%      62.122s       6.21e+00s     10   2504                     forall_inplace,gpu,grad_of_att_trans_do_apply_scan}(Shape_i{0}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, Subtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,2,1}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSub
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(183, 10, 831), strides=(8310, 831, 1) 
    input 2: dtype=float32, shape=(183, 10, 500), strides=(5000, 500, 1) 
    input 3: dtype=float32, shape=(183, 10), strides=(10, 1) 
    input 4: dtype=float32, shape=(183, 10, 500), strides=(-5000, 500, 1) 
    input 5: dtype=float32, shape=(183, 10, 250), strides=(-2500, 250, 1) 
    input 6: dtype=float32, shape=(183, 10, 250), strides=(-2500, 250, 1) 
    input 7: dtype=int64, shape=(183, 10), strides=(-80, 8) 
    input 8: dtype=float32, shape=(183, 10, 831), strides=(-8310, 831, 1) 
    input 9: dtype=float32, shape=(183, 10, 500), strides=(-5000, 500, 1) 
    input 10: dtype=float32, shape=(183, 250, 10), strides=(-2500, 1, 250) 
    input 11: dtype=float32, shape=(183, 10, 1), strides=(-10, 1, 0) 
    input 12: dtype=float32, shape=(184, 10, 250), strides=(-2500, 250, 1) 
    input 13: dtype=float32, shape=(184, 10, 500), strides=(-5000, 500, 1) 
    input 14: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    input 15: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    input 16: dtype=float32, shape=(184, 10), strides=(10, 1) 
    input 17: dtype=float32, shape=(2, 250, 250), strides=(62500, 250, 1) 
    input 18: dtype=float32, shape=(2, 1, 201), strides=(201, 0, 1) 
    input 19: dtype=float32, shape=(2, 1, 250), strides=(250, 0, 1) 
    input 20: dtype=float32, shape=(2, 250, 1), strides=(250, 1, 0) 
    input 21: dtype=float32, shape=(2, 831, 10, 500), strides=(4155000, 5000, 500, 1) 
    input 22: dtype=float32, shape=(2, 831, 10, 250), strides=(2077500, 2500, 250, 1) 
    input 23: dtype=int64, shape=(), strides=c 
    input 24: dtype=int64, shape=(), strides=c 
    input 25: dtype=int64, shape=(), strides=c 
    input 26: dtype=float32, shape=(250, 500), strides=c 
    input 27: dtype=float32, shape=(250, 250), strides=c 
    input 28: dtype=float32, shape=(500, 500), strides=c 
    input 29: dtype=float32, shape=(250, 250), strides=c 
    input 30: dtype=float32, shape=(500, 250), strides=c 
    input 31: dtype=float32, shape=(500, 500), strides=(1, 500) 
    input 32: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 33: dtype=float32, shape=(250, 500), strides=(1, 250) 
    input 34: dtype=float32, shape=(250, 250), strides=(1, 250) 
    input 35: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
    input 36: dtype=float32, shape=(250, 250), strides=(1, 250) 
    input 37: dtype=int64, shape=(2,), strides=c 
    input 38: dtype=int64, shape=(2,), strides=c 
    input 39: dtype=float32, shape=(831, 10, 500), strides=(10000, 500, 1) 
    input 40: dtype=int64, shape=(1,), strides=c 
    input 41: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, -1) 
    input 42: dtype=float32, shape=(831, 10, 250), strides=(2500, 250, 1) 
    input 43: dtype=int64, shape=(1,), strides=c 
    input 44: dtype=float32, shape=(831, 10), strides=(20, 1) 
    input 45: dtype=float32, shape=(831, 10, 500), strides=(5000, 500, 1) 
    input 46: dtype=float32, shape=(831, 10, 250), strides=(2500, 250, 1) 
    input 47: dtype=int64, shape=(), strides=c 
    input 48: dtype=int64, shape=(), strides=c 
    input 49: dtype=float32, shape=(1, 250), strides=(0, 1) 
    input 50: dtype=float32, shape=(250, 1), strides=(1, 0) 
    input 51: dtype=float32, shape=(250, 1), strides=(1, 0) 
    input 52: dtype=float32, shape=(1, 250), strides=(0, 1) 
    output 0: dtype=float32, shape=(184, 10, 250), strides=(-2500, 250, 1) 
    output 1: dtype=float32, shape=(184, 10, 500), strides=(-5000, 500, 1) 
    output 2: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    output 3: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    output 4: dtype=float32, shape=(184, 10), strides=(10, 1) 
    output 5: dtype=float32, shape=(2, 250, 250), strides=(62500, 250, 1) 
    output 6: dtype=float32, shape=(2, 1, 201), strides=(201, 0, 1) 
    output 7: dtype=float32, shape=(2, 1, 250), strides=(250, 0, 1) 
    output 8: dtype=float32, shape=(2, 250, 1), strides=(250, 1, 0) 
    output 9: dtype=float32, shape=(2, 831, 10, 500), strides=(4155000, 5000, 500, 1) 
    output 10: dtype=float32, shape=(2, 831, 10, 250), strides=(2077500, 2500, 250, 1) 
    output 11: dtype=float32, shape=(183, 10, 250), strides=(2500, 250, 1) 
    output 12: dtype=float32, shape=(183, 10, 500), strides=(5000, 500, 1) 
    output 13: dtype=float32, shape=(183, 250, 10), strides=(2500, 10, 1) 
  15.4%    44.4%      32.875s       3.29e+00s     10   2612                     forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 4: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 5: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 6: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 7: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 9: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 10: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 11: dtype=int64, shape=(), strides=c 
    input 12: dtype=int64, shape=(), strides=c 
    input 13: dtype=int64, shape=(), strides=c 
    input 14: dtype=int64, shape=(), strides=c 
    input 15: dtype=int64, shape=(), strides=c 
    input 16: dtype=int64, shape=(), strides=c 
    input 17: dtype=float32, shape=(250, 500), strides=c 
    input 18: dtype=float32, shape=(250, 250), strides=c 
    input 19: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 20: dtype=float32, shape=(250, 250), strides=(1, 250) 
    input 21: dtype=float32, shape=(250, 500), strides=c 
    input 22: dtype=float32, shape=(250, 250), strides=c 
    input 23: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 24: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 3: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 4: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
    output 5: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 6: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 7: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
  15.3%    59.7%      32.689s       3.27e+00s     10   2744                     forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 4: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 5: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 6: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 7: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 9: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 10: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 11: dtype=int64, shape=(), strides=c 
    input 12: dtype=int64, shape=(), strides=c 
    input 13: dtype=int64, shape=(), strides=c 
    input 14: dtype=int64, shape=(), strides=c 
    input 15: dtype=int64, shape=(), strides=c 
    input 16: dtype=int64, shape=(), strides=c 
    input 17: dtype=float32, shape=(250, 500), strides=c 
    input 18: dtype=float32, shape=(250, 250), strides=c 
    input 19: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 20: dtype=float32, shape=(250, 250), strides=(1, 250) 
    input 21: dtype=float32, shape=(250, 500), strides=c 
    input 22: dtype=float32, shape=(250, 250), strides=c 
    input 23: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 24: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 3: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 4: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
    output 5: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 6: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 7: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
  15.1%    74.8%      32.211s       3.22e+00s     10   2875                     forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 4: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 5: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 6: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 7: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 9: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 10: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    input 11: dtype=int64, shape=(), strides=c 
    input 12: dtype=int64, shape=(), strides=c 
    input 13: dtype=int64, shape=(), strides=c 
    input 14: dtype=int64, shape=(), strides=c 
    input 15: dtype=int64, shape=(), strides=c 
    input 16: dtype=int64, shape=(), strides=c 
    input 17: dtype=float32, shape=(250, 500), strides=c 
    input 18: dtype=float32, shape=(250, 250), strides=c 
    input 19: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 20: dtype=float32, shape=(250, 250), strides=(1, 250) 
    input 21: dtype=float32, shape=(250, 500), strides=c 
    input 22: dtype=float32, shape=(250, 250), strides=c 
    input 23: dtype=float32, shape=(500, 250), strides=(1, 500) 
    input 24: dtype=float32, shape=(250, 250), strides=(1, 250) 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(-2500, 250, 1) 
    output 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 3: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 4: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
    output 5: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    output 6: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    output 7: dtype=float32, shape=(1662, 250, 10), strides=(2500, 10, 1) 
   5.6%    80.4%      12.054s       1.21e+00s     10   2124                     forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 4: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 5: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 6: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 7: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 9: dtype=float32, shape=(250, 500), strides=c 
    input 10: dtype=float32, shape=(250, 250), strides=c 
    input 11: dtype=float32, shape=(250, 500), strides=c 
    input 12: dtype=float32, shape=(250, 250), strides=c 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
   5.6%    86.1%      12.039s       1.20e+00s     10   1803                     forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 4: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 5: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 6: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 7: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 9: dtype=float32, shape=(250, 500), strides=c 
    input 10: dtype=float32, shape=(250, 250), strides=c 
    input 11: dtype=float32, shape=(250, 500), strides=c 
    input 12: dtype=float32, shape=(250, 250), strides=c 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
   5.6%    91.7%      11.941s       1.19e+00s     10   2307                     forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(1662, 10, 500), strides=(5000, 500, 1) 
    input 2: dtype=float32, shape=(1662, 10, 250), strides=(2500, 250, 1) 
    input 3: dtype=float32, shape=(1662, 10, 1), strides=(10, 1, 0) 
    input 4: dtype=float32, shape=(1662, 10, 500), strides=(-5000, 500, 1) 
    input 5: dtype=float32, shape=(1662, 10, 250), strides=(-2500, 250, 1) 
    input 6: dtype=float32, shape=(1662, 10, 1), strides=(-10, 1, 0) 
    input 7: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 8: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    input 9: dtype=float32, shape=(250, 500), strides=c 
    input 10: dtype=float32, shape=(250, 250), strides=c 
    input 11: dtype=float32, shape=(250, 500), strides=c 
    input 12: dtype=float32, shape=(250, 250), strides=c 
    output 0: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
    output 1: dtype=float32, shape=(1663, 10, 250), strides=(2500, 250, 1) 
   4.9%    96.6%      10.551s       1.06e+00s     10   2352                     forall_inplace,gpu,att_trans_do_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuAlloc{memset_0=True}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, Alloc.0, state_to_gates, W, W, state_to_state, W, GpuDimShuffle{0,x,x,1}.0, GpuSubtensor{::int64}.0, MakeVector.0, GpuElemwise{add,no_inplace}.0, MakeVector.0, GpuSubtensor{::int
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(183, 10, 500), strides=(5000, 500, 1) 
    input 2: dtype=float32, shape=(183, 10, 250), strides=(2500, 250, 1) 
    input 3: dtype=float32, shape=(183, 10, 1), strides=(10, 1, 0) 
    input 4: dtype=float32, shape=(184, 10, 250), strides=(2500, 250, 1) 
    input 5: dtype=float32, shape=(183, 10, 500), strides=(5000, 500, 1) 
    input 6: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    input 7: dtype=int64, shape=(184, 10), strides=c 
    input 8: dtype=float32, shape=(250, 500), strides=c 
    input 9: dtype=float32, shape=(250, 250), strides=c 
    input 10: dtype=float32, shape=(500, 500), strides=c 
    input 11: dtype=float32, shape=(250, 250), strides=c 
    input 12: dtype=float32, shape=(500, 250), strides=c 
    input 13: dtype=float32, shape=(1, 1, 1, 201), strides=(0, 0, 0, 1) 
    input 14: dtype=float32, shape=(831, 10, 500), strides=(10000, 500, 1) 
    input 15: dtype=int64, shape=(1,), strides=c 
    input 16: dtype=float32, shape=(831, 10, 250), strides=(2500, 250, 1) 
    input 17: dtype=int64, shape=(1,), strides=c 
    input 18: dtype=float32, shape=(831, 10), strides=(20, 1) 
    input 19: dtype=int64, shape=(), strides=c 
    input 20: dtype=int64, shape=(), strides=c 
    input 21: dtype=float32, shape=(1, 250), strides=(0, 1) 
    input 22: dtype=float32, shape=(250, 1), strides=(1, 0) 
    output 0: dtype=float32, shape=(184, 10, 250), strides=(2500, 250, 1) 
    output 1: dtype=float32, shape=(183, 10, 500), strides=(5000, 500, 1) 
    output 2: dtype=float32, shape=(184, 10, 831), strides=(8310, 831, 1) 
    output 3: dtype=int64, shape=(184, 10), strides=c 
   0.1%    96.7%       0.175s       1.75e-02s     10   2452                     GpuAdvancedIncSubtensor1{inplace,inc}(GpuAlloc{memset_0=True}.0, GpuReshape{1}.0, Elemwise{Composite{((i0 * i1) + i2)}}[(0, 1)].0)
    input 0: dtype=float32, shape=(58560,), strides=(1,) 
    input 1: dtype=float32, shape=(1830,), strides=(1,) 
    input 2: dtype=int64, shape=(1830,), strides=c 
    output 0: dtype=float32, shape=(58560,), strides=(1,) 
   0.0%    96.7%       0.101s       1.01e-02s     10   1442                     GpuFromHost(Alloc.0)
    input 0: dtype=float32, shape=(8310000,), strides=c 
    output 0: dtype=float32, shape=(8310000,), strides=(1,) 
   0.0%    96.8%       0.095s       9.52e-03s     10   1807                     GpuFromHost(Alloc.0)
    input 0: dtype=float32, shape=(8310000,), strides=c 
    output 0: dtype=float32, shape=(8310000,), strides=(1,) 
   0.0%    96.8%       0.082s       8.18e-03s     10   1977                     GpuDot22(GpuReshape{2}.0, GpuReshape{2}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    96.8%       0.081s       8.07e-03s     10   2173                     GpuDot22(GpuReshape{2}.0, GpuReshape{2}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    96.9%       0.081s       8.06e-03s     10   2175                     GpuDot22(GpuReshape{2}.0, GpuReshape{2}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    96.9%       0.081s       8.05e-03s     10   2802                     GpuDot22(GpuReshape{2}.0, GpuDimShuffle{1,0}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(1, 500) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    97.0%       0.080s       8.05e-03s     10   2682                     GpuDot22(GpuReshape{2}.0, GpuDimShuffle{1,0}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(1, 500) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    97.0%       0.080s       8.04e-03s     10   1979                     GpuDot22(GpuReshape{2}.0, GpuReshape{2}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    97.0%       0.080s       8.02e-03s     10   2671                     GpuDot22(GpuReshape{2}.0, GpuDimShuffle{1,0}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(1, 500) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   0.0%    97.1%       0.080s       8.01e-03s     10   2672                     GpuDot22(GpuDimShuffle{1,0}.0, GpuReshape{2}.0)
    input 0: dtype=float32, shape=(500, 16620), strides=(1, 500) 
    input 1: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    output 0: dtype=float32, shape=(500, 500), strides=(500, 1) 
   0.0%    97.1%       0.080s       8.01e-03s     10   2813                     GpuDot22(GpuReshape{2}.0, GpuDimShuffle{1,0}.0)
    input 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
    input 1: dtype=float32, shape=(500, 500), strides=(1, 500) 
    output 0: dtype=float32, shape=(16620, 500), strides=(500, 1) 
   ... (remaining 5893 Apply instances account for 2.90%(6.19s) of the runtime)

Memory Profile (the max between all functions in that profile)
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 3036900KB (3036900KB)
    CPU: 162926KB (162926KB)
    GPU: 2873974KB (2873974KB)
---
    Max if linker=cvm(default): 910339KB (1331587KB)
    CPU: 32462KB (38479KB)
    GPU: 877877KB (1293108KB)
---
    Memory saved if views are used: 4291558KB (4291557KB)
    Memory saved if inplace ops are used: 1605717KB (1605717KB)
    Memory saved if gc is enabled: 2126561KB (1705312KB)
---

    This list is based on all functions in the profile
    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

     166220000B  [(1663, 10, 250), (1663, 10, 250), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10)] i i c c c c c c forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0)
     166220000B  [(1663, 10, 250), (1663, 10, 250), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10)] i i c c c c c c forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0)
     166220000B  [(1663, 10, 250), (1663, 10, 250), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10), (1662, 10, 250), (1662, 10, 500), (1662, 250, 10)] i i c c c c c c forall_inplace,gpu,grad_of_gatedrecurrent_apply_scan&grad_of_gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, state_to_gates, state_to_state, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0)
      75445288B  [(184, 10, 250), (184, 10, 500), (184, 10, 831), (184, 10, 831), (184, 10), (2, 250, 250), (2, 1, 201), (2, 1, 250), (2, 250, 1), (2, 831, 10, 500), (2, 831, 10, 250), (183, 10, 250), (183, 10, 500), (183, 250, 10)] i i i i i i i i i i i c c c forall_inplace,gpu,grad_of_att_trans_do_apply_scan}(Shape_i{0}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, Subtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,2,1}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{::int64}.0, GpuSubtensor{::int64}.0, GpuFromHost.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, Shape_i{0}.0, Shape_i{0}.0, Shape_i{0}.0, state_to_gates, W, W, state_to_state, W, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{0,x,x,1}.0, GpuDimShuffle{1,0}.0, MakeVector.0, MakeVector.0, GpuSubtensor{::int64}.0, MakeVector.0, GpuSubtensor{::, ::, ::int64, ::int64}.0, GpuElemwise{add,no_inplace}.0, MakeVector.0, GpuSubtensor{::int64}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, max_attended_length, Elemwise{Composite{(((i0 + i1) - Switch(LT(i2, i1), i2, i1)) // i3)}}.0, GpuReshape{2}.0, GpuReshape{2}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0)
      33260000B  [(1663, 10, 250), (1663, 10, 250)] i i forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
      33260000B  [(1663, 10, 250), (1663, 10, 250)] i i forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
      33260000B  [(1663, 10, 250), (1663, 10, 250)] i i forall_inplace,gpu,gatedrecurrent_apply_scan&gatedrecurrent_apply_scan}(Shape_i{0}.0, GpuSubtensor{int64:int64:int8}.0, GpuSubtensor{int64:int64:int8}.0, GpuDimShuffle{0,1,x}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuDimShuffle{0,1,x}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, state_to_gates, state_to_state, state_to_gates, state_to_state)
      33240000B  [(16620, 500)] v GpuReshape{2}(GpuIncSubtensor{InplaceInc;int64::}.0, MakeVector.0)
      33240000B  [(1662, 10, 500)] i GpuElemwise{Composite{((i0 + i1) + (i2 + i3))}}[(0, 0)](GpuSubtensor{:int64:}.0, GpuSubtensor{:int64:}.0, GpuSubtensor{:int64:}.0, GpuSubtensor{:int64:}.0)
      33240000B  [(1662, 10, 500)] v GpuReshape{3}(GpuDot22.0, MakeVector.0)
      33240000B  [(1662, 10, 500)] v GpuSubtensor{:int64:}(GpuDimShuffle{0,1,2}.0, ScalarFromTensor.0)
      33240000B  [(1662, 10, 500)] v GpuDimShuffle{0,1,2}(GpuIncSubtensor{InplaceSet;:int64:}.0)
      33240000B  [(1662, 10, 500)] i GpuIncSubtensor{InplaceInc;int64::}(GpuAlloc{memset_0=True}.0, GpuIncSubtensor{Inc;::int64}.0, Constant{0})
      33240000B  [(1662, 10, 500)] i GpuIncSubtensor{InplaceInc;:int64:}(GpuAlloc{memset_0=True}.0, GpuSubtensor{::int64}.0, ScalarFromTensor.0)
      33240000B  [(1662, 10, 500)] v GpuReshape{3}(GpuDot22.0, MakeVector.0)
      33240000B  [(1662, 10, 500)] v GpuReshape{3}(GpuDot22.0, MakeVector.0)
      33240000B  [(16620, 500)] c GpuDot22(GpuReshape{2}.0, GpuReshape{2}.0)
      33240000B  [(1662, 10, 500)] c GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[[ 0.]]]}, Elemwise{sub,no_inplace}.0, Shape_i{0}.0, Elemwise{add,no_inplace}.0)
      33240000B  [(16620, 500)] v GpuReshape{2}(GpuIncSubtensor{InplaceInc;int64::}.0, MakeVector.0)
      33240000B  [(1662, 10, 500)] i GpuIncSubtensor{InplaceInc;::int64}(GpuAlloc{memset_0=True}.0, GpuElemwise{Composite{((i0 + i1) + (i2 + i3))}}[(0, 0)].0, Constant{1})
   ... (remaining 5893 Apply account for 8042595801B/9148601089B ((87.91%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

_/recognizer/encoder/bidir2/forward/gatedrecurrent.state_to_state_stats: [  1.00383533e-01   5.00813147e-01   3.89649937e-04   9.16719472e-04]
	 average_/recognizer/generator/att_trans/conv_att/conv1d.filters_stats: [  1.03385845e-01   1.71608098e-03   1.57559413e-05   9.63074742e-03]
	 average_/recognizer/generator/att_trans/conv_att/energy_comp/linear.W_stats: [  9.46863318e-02   3.71041956e+00   5.28481944e-04   1.77161704e-04]
	 average_/recognizer/generator/att_trans/conv_att/handler.W_stats: [  9.43099508e-02   1.76320323e-03   1.55522843e-05   9.64520522e-03]
	 average_/recognizer/generator/att_trans/conv_att/preprocess.W_stats: [ 0.09970464  0.18743728  0.00034742  0.00225261]
	 average_/recognizer/generator/att_trans/conv_att/preprocess.b_stats: [ 0.00057009  0.16119971  0.00033696  0.00251228]
	 average_/recognizer/generator/att_trans/conv_att/state_trans/transform_states.W_stats: [ 0.09993146  0.11504627  0.0002861   0.00296832]
	 average_/recognizer/generator/att_trans/distribute/fork_gate_inputs.W_stats: [ 0.10004991  0.1153786   0.00024764  0.00252861]
	 average_/recognizer/generator/att_trans/distribute/fork_inputs.W_stats: [  1.00389519e-01   7.75855152e-01   3.88536312e-04   5.95256218e-04]
	 average_/recognizer/generator/att_trans/transition.initial_state_stats: [ 0.00113674  0.23661354  0.00038632  0.001939  ]
	 average_/recognizer/generator/att_trans/transition.state_to_gates_stats: [ 0.09996259  0.14924286  0.00026746  0.00207988]
	 average_/recognizer/generator/att_trans/transition.state_to_state_stats: [  9.98305710e-02   5.96250620e-01   3.63500056e-04   7.17162938e-04]
	 average_/recognizer/generator/fork/fork_gate_inputs.W_stats: [  9.95401097e-02   7.25810892e-03   5.55114445e-05   8.74636658e-03]
	 average_/recognizer/generator/fork/fork_gate_inputs.b_stats: [ 0.00056188  0.21685773  0.00030946  0.00168483]
	 average_/recognizer/generator/fork/fork_inputs.W_stats: [ 0.10026672  0.04583523  0.00019746  0.00500861]
	 average_/recognizer/generator/fork/fork_inputs.b_stats: [  6.25522554e-04   1.47812988e+00   4.09666885e-04   3.31335337e-04]
	 average_/recognizer/generator/readout/lookupfeedback/lookuptable.W_stats: [ 0.09911473  0.12405798  0.00027519  0.00254402]
	 average_/recognizer/generator/readout/merge/transform_states.W_stats: [  9.99569016e-02   1.28329349e+00   3.81013227e-04   3.27678837e-04]
	 average_/recognizer/generator/readout/merge/transform_weighted_averages.W_stats: [  1.00204990e-01   9.72616962e-01   3.69553009e-04   4.23945035e-04]
	 average_/recognizer/generator/readout/post_merge/bias.b_stats: [  5.84864115e-04   1.84800115e+00   3.98165826e-04   2.41200220e-04]
	 average_/recognizer/generator/readout/post_merge/mlp/linear_0.W_stats: [  9.98084557e-02   5.76659061e+00   3.48503424e-04   6.87044663e-05]
	 average_/recognizer/generator/readout/post_merge/mlp/linear_0.b_stats: [  1.13995327e-03   5.27414539e+00   4.97567658e-04   1.07450998e-04]
	 average_batch_size: 10.0
	 average_gradient_norm_threshold: 98.2516937256
	 average_mask_density: 0.63605260849
	 average_max_energy: 5.09787559509
	 average_max_num_phonemes: 167.199996948
	 average_mean_attended: 0.547704577446
	 average_mean_bottom_output: 0.267571926117
	 average_min_energy: -5.72096586227
	 average_sequence_log_likelihood: 431.364547729
	 average_total_gradient_norm: 1191.6706543
	 average_total_step_norm: 0.653477430344
	 average_weights_entropy_per_label: -4.70119428635
	 average_weights_penalty_per_recording: 5.21301722527
	 gradient_norm_threshold: 98.2516937256
	 max_attended_length: 831.0
	 max_attended_mask_length: 831.0
	 max_recording_length: 1662.0
	 saved_to: ('tmp.zip',)
	 sequence_log_likelihood: 533.571411133
	 time_read_data_this_batch: 0.0275950431824
	 time_read_data_total: 0.233206987381
	 time_train_this_batch: 31.1435320377
	 time_train_total: 250.570747614
	 total_gradient_norm: 1083.12548828
	 total_step_norm: 0.629614830017
	 training_finish_requested: True
	 training_finished: True

